import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib import font_manager
import joblib
import xgboost as xgb
import tensorflow as tf
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.ensemble import RandomForestClassifier
import shap
# æ³¨æ„ï¼šç§»é™¤äº† CNN-LSTM ç›¸å…³çš„ layers å¯¼å…¥ï¼Œå› ä¸º MLP-DA ä¸éœ€è¦å®ƒä»¬
# from tensorflow.keras.layers import Input, Conv1D, BatchNormalization, ReLU, MaxPooling1D, LSTM, \
#     GlobalAveragePooling1D, Dense, Dropout, multiply, Activation
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, Dropout, Lambda # å¯¼å…¥ MLP-DA éœ€è¦çš„å±‚
from tensorflow.keras.utils import to_categorical
# --- æ–°å¢ï¼šå¯¼å…¥ GradReverse å±‚å®šä¹‰ ---
@tf.custom_gradient
def grad_reverse(x, lambda_val=1.0):
    """æ¢¯åº¦åè½¬å‡½æ•°"""
    y = tf.identity(x)
    def custom_grad(dy):
        return -dy * lambda_val, None
    return y, custom_grad

class GradReverse(tf.keras.layers.Layer):
    """æ¢¯åº¦åè½¬å±‚ Keras å°è£…"""
    def __init__(self, lambda_val=1.0, **kwargs):
        super(GradReverse, self).__init__(**kwargs)
        self.lambda_val = lambda_val
    def call(self, x):
        return grad_reverse(x, self.lambda_val)
    def get_config(self):
        config = super(GradReverse, self).get_config()
        config.update({'lambda_val': self.lambda_val})
        return config
# --- æ–°å¢ç»“æŸ ---

# ==================== æ·»åŠ  MLP-DA æ¨¡å‹å®šä¹‰å‡½æ•° ====================
# --- æ–°å¢ï¼šå®šä¹‰ä¸è®­ç»ƒæ—¶å®Œå…¨ä¸€è‡´çš„ MLP-DA æ¨¡å‹æ¶æ„ ---
def create_mlp_da_model(input_dim, num_classes, lambda_grl=1.0):
    """
    åˆ›å»ºç”¨äºæºåŸŸè®­ç»ƒçš„ç®€åŒ– MLP æ¨¡å‹ï¼ŒåŒ…å«é¢†åŸŸè‡ªé€‚åº”ç»„ä»¶ã€‚
    """
    # 1. è¾“å…¥å±‚
    inputs = Input(shape=(input_dim,))

    # 2. ç‰¹å¾æå–å™¨ (MLP)
    shared = Dense(128, activation='relu', name='feature_extractor_1')(inputs)
    shared = Dropout(0.5)(shared)
    shared = Dense(64, activation='relu', name='feature_extractor_2')(shared)
    shared = Dropout(0.5)(shared)
    features_before_grl = Dense(32, activation='relu', name='feature_extractor_3')(shared)

    # --- æ–°å¢ï¼šé¢†åŸŸè‡ªé€‚åº”åˆ†æ”¯ ---
    # 3a. æ¢¯åº¦åè½¬å±‚ (GRL)
    grl = GradReverse(lambda_val=lambda_grl)(features_before_grl)

    # 3b. é¢†åŸŸåˆ¤åˆ«å™¨ (Domain Discriminator)
    d_net = Dense(32, activation='relu')(grl)
    d_net = Dropout(0.5)(d_net)
    domain_output = Dense(1, activation='sigmoid', name='domain_output')(d_net)
    # --- æ–°å¢ç»“æŸ ---

    # 4. ä¸»ä»»åŠ¡åˆ†ç±»å¤´
    c_net = Dense(64, activation='relu')(features_before_grl)
    c_net = Dropout(0.5)(c_net)
    class_output = Dense(num_classes, activation='softmax', name='class_output')(c_net)

    # 5. æ„å»ºæ¨¡å‹
    model = Model(inputs=inputs, outputs=[class_output, domain_output])
    # æ³¨æ„ï¼šå¯è§†åŒ–æ—¶ä¸éœ€è¦ç¼–è¯‘æ¨¡å‹
    return model
# --- æ–°å¢ç»“æŸ ---
# ==================== æ·»åŠ æ¨¡å‹å®šä¹‰å‡½æ•°ç»“æŸ ====================


# è®¾ç½®ä¸­æ–‡å­—ä½“
def set_chinese_font():
    """è®¾ç½®ä¸­æ–‡å­—ä½“"""
    plt.rcParams['font.sans-serif'] = ['Microsoft YaHei']
    plt.rcParams['axes.unicode_minus'] = False
    print("âœ… å·²è®¾ç½®ä¸­æ–‡å­—ä½“ã€‚")


# åˆ›å»ºè¾“å‡ºç›®å½•
def create_output_dir():
    """åˆ›å»ºå¯è§†åŒ–è¾“å‡ºç›®å½•"""
    output_dir = os.path.join('..', 'data', 'processed', 'task2_visualizations')
    os.makedirs(output_dir, exist_ok=True)
    return output_dir


# 1. SHAPç‰¹å¾é‡è¦æ€§å›¾ï¼ˆTop 10ç‰¹å¾ï¼‰- XGBoost (ä¿®å¤ç‰ˆ)
def plot_shap_feature_importance(X, model, feature_names, output_dir):
    """ç»˜åˆ¶SHAPç‰¹å¾é‡è¦æ€§å›¾"""
    print("  - æ­£åœ¨ç”ŸæˆSHAPç‰¹å¾é‡è¦æ€§å›¾...")

    try:
        # ç¡®ä¿Xæ˜¯numpyæ•°ç»„ä¸”å½¢çŠ¶æ­£ç¡®
        if isinstance(X, pd.DataFrame):
            X_array = X.values
        else:
            X_array = np.asarray(X)

        # åˆ›å»ºSHAPè§£é‡Šå™¨
        explainer = shap.TreeExplainer(model)

        # ä¸ºäº†æé«˜æ•ˆç‡ï¼Œåªä½¿ç”¨éƒ¨åˆ†æ•°æ®è®¡ç®—SHAPå€¼
        sample_size = min(500, X_array.shape[0])  # å‡å°‘æ ·æœ¬æ•°é¿å…å†…å­˜é—®é¢˜
        sample_indices = np.random.choice(X_array.shape[0], sample_size, replace=False)
        X_sample = X_array[sample_indices]

        # ç¡®ä¿ç‰¹å¾åç§°æ˜¯åˆ—è¡¨
        if isinstance(feature_names, pd.Index):
            feature_names_list = feature_names.tolist()
        else:
            feature_names_list = list(feature_names)

        # ç¡®ä¿X_sampleæ˜¯äºŒç»´æ•°ç»„ä¸”åˆ—æ•°ä¸ç‰¹å¾åç§°åŒ¹é…
        if X_sample.ndim == 1:
            X_sample = X_sample.reshape(-1, 1)

        # å¦‚æœç‰¹å¾æ•°é‡ä¸åŒ¹é…ï¼Œæˆªå–æˆ–è¡¥é½
        if X_sample.shape[1] != len(feature_names_list):
            min_features = min(X_sample.shape[1], len(feature_names_list))
            X_sample = X_sample[:, :min_features]
            feature_names_list = feature_names_list[:min_features]
            print(f"  - âš ï¸ ç‰¹å¾æ•°é‡ä¸åŒ¹é…ï¼Œå·²è°ƒæ•´ä¸º {min_features} ä¸ªç‰¹å¾")

        # åˆ›å»ºDataFrameç¡®ä¿åˆ—åæ­£ç¡®
        X_sample_df = pd.DataFrame(X_sample, columns=feature_names_list)

        # è®¡ç®—SHAPå€¼
        shap_values = explainer.shap_values(X_sample_df)

        # å¦‚æœæ˜¯å¤šåˆ†ç±»ï¼Œshap_valuesæ˜¯ä¸€ä¸ªåˆ—è¡¨
        if isinstance(shap_values, list):
            # å¯¹äºå¤šåˆ†ç±»ï¼Œæˆ‘ä»¬è®¡ç®—æ¯ä¸ªç±»åˆ«çš„å¹³å‡ç»å¯¹SHAPå€¼
            shap_importance = np.mean([np.abs(sv).mean(0) for sv in shap_values], axis=0)
        else:
            shap_importance = np.abs(shap_values).mean(0)

        # ç¡®ä¿shap_importanceæ˜¯ä¸€ç»´æ•°ç»„ä¸”é•¿åº¦ä¸ç‰¹å¾åç§°åŒ¹é…
        if hasattr(shap_importance, 'ndim') and shap_importance.ndim > 1:
            shap_importance = shap_importance.flatten()

        if len(shap_importance) != len(feature_names_list):
            min_len = min(len(shap_importance), len(feature_names_list))
            shap_importance = shap_importance[:min_len]
            feature_names_list = feature_names_list[:min_len]

        # åˆ›å»ºç‰¹å¾é‡è¦æ€§DataFrame
        feature_importance_df = pd.DataFrame({
            'feature': feature_names_list,
            'importance': np.abs(shap_importance)  # ç¡®ä¿æ˜¯ç»å¯¹å€¼
        }).sort_values('importance', ascending=False)

        # ç»˜åˆ¶Top 10ç‰¹å¾é‡è¦æ€§
        plt.figure(figsize=(12, 8))
        top_features = feature_importance_df.head(10)
        sns.barplot(data=top_features, x='importance', y='feature', palette='viridis')
        plt.title('Top 10 SHAPç‰¹å¾é‡è¦æ€§ (XGBoost)', fontsize=16, weight='bold')
        plt.xlabel('å¹³å‡SHAPå€¼', fontsize=12)
        plt.ylabel('ç‰¹å¾åç§°', fontsize=12)
        plt.tight_layout()

        save_path = os.path.join(output_dir, '12_1_shap_feature_importance_xgboost.png')
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
        print(f"  - âœ… SHAPç‰¹å¾é‡è¦æ€§å›¾å·²ä¿å­˜è‡³: {save_path}")

    except Exception as e:
        print(f"  - âš ï¸ ç”ŸæˆSHAPç‰¹å¾é‡è¦æ€§å›¾æ—¶å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()


# 2. ç±»åˆ«åˆ†å¸ƒé¥¼å›¾ (ä½œä¸ºé—®é¢˜èƒŒæ™¯è¯´æ˜)
def plot_class_distribution(y_true, class_names, output_dir):
    """ç»˜åˆ¶ç±»åˆ«åˆ†å¸ƒé¥¼å›¾"""
    print("  - æ­£åœ¨ç”Ÿæˆç±»åˆ«åˆ†å¸ƒé¥¼å›¾...")

    try:
        # ç»Ÿè®¡å„ç±»åˆ«æ•°é‡
        unique, counts = np.unique(y_true, return_counts=True)
        class_counts = dict(zip(unique, counts))
        class_labels = [class_names[i] for i in unique]
        class_values = [class_counts[i] for i in unique]

        # ç»˜åˆ¶é¥¼å›¾
        plt.figure(figsize=(10, 8))
        colors = ['gold', 'yellowgreen', 'lightcoral', 'lightskyblue', 'orange']
        plt.pie(class_values, labels=class_labels, colors=colors, autopct='%1.1f%%', startangle=140)
        plt.title('æºåŸŸæ•°æ®é›†ç±»åˆ«åˆ†å¸ƒ\n(ç”¨äºæ­ç¤ºæ•°æ®ä¸å¹³è¡¡æŒ‘æˆ˜)', fontsize=16, weight='bold')
        plt.axis('equal')

        save_path = os.path.join(output_dir, '12_4_class_distribution.png')
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
        print(f"  - âœ… ç±»åˆ«åˆ†å¸ƒé¥¼å›¾å·²ä¿å­˜è‡³: {save_path}")

    except Exception as e:
        print(f"  - âš ï¸ ç”Ÿæˆç±»åˆ«åˆ†å¸ƒé¥¼å›¾æ—¶å‡ºé”™: {e}")


# 3. æ··æ·†çŸ©é˜µçƒ­åŠ›å›¾ï¼ˆç»¼åˆï¼‰
def plot_confusion_matrix_heatmap(y_true, y_pred_models, class_names, output_dir):
    """ç»˜åˆ¶ç»¼åˆæ··æ·†çŸ©é˜µçƒ­åŠ›å›¾"""
    print("  - æ­£åœ¨ç”Ÿæˆç»¼åˆæ··æ·†çŸ©é˜µçƒ­åŠ›å›¾...")

    try:
        # é€‰æ‹©æœ€ä½³æ¨¡å‹ï¼ˆè¿™é‡Œé€‰æ‹©ç¬¬ä¸€ä¸ªæ¨¡å‹ï¼‰è¿›è¡Œè¯¦ç»†åˆ†æ
        best_model_name = list(y_pred_models.keys())[0]
        y_pred_best = y_pred_models[best_model_name]

        # è®¡ç®—æ··æ·†çŸ©é˜µ
        cm = confusion_matrix(y_true, y_pred_best)

        # ç»˜åˆ¶çƒ­åŠ›å›¾
        plt.figure(figsize=(10, 8))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                    xticklabels=class_names, yticklabels=class_names)
        plt.title(f'{best_model_name} æ¨¡å‹æ··æ·†çŸ©é˜µ', fontsize=16, weight='bold')
        plt.xlabel('é¢„æµ‹æ ‡ç­¾')
        plt.ylabel('çœŸå®æ ‡ç­¾')

        save_path = os.path.join(output_dir, '12_5_confusion_matrix_heatmap.png')
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
        print(f"  - âœ… ç»¼åˆæ··æ·†çŸ©é˜µçƒ­åŠ›å›¾å·²ä¿å­˜è‡³: {save_path}")

    except Exception as e:
        print(f"  - âš ï¸ ç”Ÿæˆç»¼åˆæ··æ·†çŸ©é˜µçƒ­åŠ›å›¾æ—¶å‡ºé”™: {e}")


# 4. ä¸šåŠ¡ä»·å€¼å¯¼å‘è¯„ä»·å›¾ï¼ˆå®‰å…¨æ€§-ç»æµæ€§æƒè¡¡ï¼‰
def plot_safety_economy_tradeoff(models_reports, output_dir):
    """ç»˜åˆ¶å®‰å…¨æ€§å’Œç»æµæ€§æƒè¡¡å›¾"""
    print("  - æ­£åœ¨ç”Ÿæˆå®‰å…¨æ€§-ç»æµæ€§æƒè¡¡å›¾...")

    try:
        models_names = list(models_reports.keys())
        safety_scores = []  # IRç±»å¬å›ç‡ï¼ˆå®‰å…¨æ€§ï¼‰
        economy_scores = []  # Bç±»ç²¾ç¡®ç‡ï¼ˆç»æµæ€§ï¼‰

        # --- æ–°å¢è°ƒè¯•ä¿¡æ¯ ---
        print(f"  - Debug: å‡†å¤‡ç»˜åˆ¶ {len(models_names)} ä¸ªæ¨¡å‹: {models_names}")
        # --- æ–°å¢è°ƒè¯•ä¿¡æ¯ç»“æŸ ---

        for model_name, report in models_reports.items():
            ir_recall = report.get('IR', {}).get('recall', 0)
            b_precision = report.get('B', {}).get('precision', 0)
            safety_scores.append(ir_recall)
            economy_scores.append(b_precision)
            # --- æ–°å¢è°ƒè¯•ä¿¡æ¯ ---
            print(f"    - Debug: {model_name} -> IR Recall: {ir_recall:.4f}, B Precision: {b_precision:.4f}")
            # --- æ–°å¢è°ƒè¯•ä¿¡æ¯ç»“æŸ ---

        plt.figure(figsize=(10, 8))
        scatter = plt.scatter(economy_scores, safety_scores, s=150, alpha=0.7, c=range(len(models_names)),
                              cmap='viridis')

        # æ·»åŠ æ¨¡å‹åç§°æ ‡ç­¾
        for i, model_name in enumerate(models_names):
            plt.annotate(model_name, (economy_scores[i], safety_scores[i]),
                         xytext=(5, 5), textcoords='offset points', fontsize=10, weight='bold')

        plt.xlabel('ç»æµæ€§æŒ‡æ ‡ (Bç±»ç²¾ç¡®ç‡)\nâ†‘ é¿å…è¯¯åˆ¤ï¼Œå‡å°‘ä¸å¿…è¦åœæœº', fontsize=12)
        plt.ylabel('å®‰å…¨æ€§æŒ‡æ ‡ (IRç±»å¬å›ç‡)\nâ†‘ é¿å…æ¼æ£€ï¼Œä¿éšœè®¾å¤‡å®‰å…¨', fontsize=12)
        plt.title('æ¨¡å‹å®‰å…¨æ€§-ç»æµæ€§æƒè¡¡åˆ†æ\n(å·¦ä¸Šè§’ä¸ºç†æƒ³åŒºåŸŸ)', fontsize=14, weight='bold')
        plt.grid(True, alpha=0.3)

        # æ·»åŠ è±¡é™å‚è€ƒçº¿ (x=0.98, y=0.98)ï¼Œä»£è¡¨ç†æƒ³ç›®æ ‡
        plt.axhline(y=0.98, color='r', linestyle='--', alpha=0.5, linewidth=1)
        plt.axvline(x=0.98, color='r', linestyle='--', alpha=0.5, linewidth=1)

        # å°†åæ ‡è½´èŒƒå›´è°ƒæ•´åˆ°æ›´ç²¾ç»†çš„åŒºé—´ï¼Œä»¥çªå‡ºå¾®å°å·®å¼‚
        plt.xlim(0.85, 1.01)
        plt.ylim(0.96, 1.01)

        save_path = os.path.join(output_dir, '12_6_safety_economy_tradeoff.png')
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
        print(f"  - âœ… å®‰å…¨æ€§-ç»æµæ€§æƒè¡¡å›¾å·²ä¿å­˜è‡³: {save_path}")

    except Exception as e:
        print(f"  - âš ï¸ ç”Ÿæˆå®‰å…¨æ€§-ç»æµæ€§æƒè¡¡å›¾æ—¶å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()



# 5. å…³é”®é£é™©æŒ‡æ ‡é›·è¾¾å›¾
def plot_key_risk_indicators(models_reports, output_dir):
    """ç»˜åˆ¶å…³é”®é£é™©æŒ‡æ ‡é›·è¾¾å›¾"""
    print("  - æ­£åœ¨ç”Ÿæˆå…³é”®é£é™©æŒ‡æ ‡é›·è¾¾å›¾...")

    try:
        models_names = list(models_reports.keys())

        # æå–å…³é”®æŒ‡æ ‡
        indicators = ['IRå¬å›ç‡', 'Bç²¾ç¡®ç‡', 'Nå¬å›ç‡', 'ORç²¾ç¡®ç‡', 'æ€»ä½“å‡†ç¡®ç‡']
        data = {}

        for model_name, report in models_reports.items():
            data[model_name] = [
                report.get('IR', {}).get('recall', 0),
                report.get('B', {}).get('precision', 0),
                report.get('N', {}).get('recall', 0),
                report.get('OR', {}).get('precision', 0),
                report.get('accuracy', 0)
            ]

        # ç»˜åˆ¶é›·è¾¾å›¾
        labels = np.array(indicators)
        num_vars = len(labels)
        angles = np.linspace(0, 2 * np.pi, num_vars, endpoint=False).tolist()
        angles += angles[:1]  # é—­åˆå›¾å½¢

        fig, ax = plt.subplots(figsize=(12, 10), subplot_kw=dict(projection='polar'))

        colors = ['red', 'blue', 'green', 'orange', 'purple']
        for i, (model_name, values) in enumerate(data.items()):
            values += values[:1]  # é—­åˆå›¾å½¢
            ax.plot(angles, values, 'o-', linewidth=2, label=model_name, color=colors[i % len(colors)])
            ax.fill(angles, values, alpha=0.25, color=colors[i % len(colors)])

        ax.set_xticks(angles[:-1])
        ax.set_xticklabels(labels, fontsize=11)
        ax.set_ylim(0, 1)
        ax.set_title('å…³é”®é£é™©æŒ‡æ ‡é›·è¾¾å›¾\n(è¶Šé è¿‘è¾¹ç¼˜è¶Šå¥½)', size=16, weight='bold', pad=30)
        ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))
        ax.grid(True, alpha=0.3)

        save_path = os.path.join(output_dir, '12_7_key_risk_indicators.png')
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
        print(f"  - âœ… å…³é”®é£é™©æŒ‡æ ‡é›·è¾¾å›¾å·²ä¿å­˜è‡³: {save_path}")

    except Exception as e:
        print(f"  - âš ï¸ ç”Ÿæˆå…³é”®é£é™©æŒ‡æ ‡é›·è¾¾å›¾æ—¶å‡ºé”™: {e}")


# 6. ç»¼åˆæ€§èƒ½è¯„åˆ†å¡
def plot_comprehensive_scorecard(models_reports, output_dir):
    """ç»˜åˆ¶ç»¼åˆæ€§èƒ½è¯„åˆ†å¡"""
    print("  - æ­£åœ¨ç”Ÿæˆç»¼åˆæ€§èƒ½è¯„åˆ†å¡...")

    try:
        models_names = list(models_reports.keys())

        # å®šä¹‰è¯„åˆ†ç»´åº¦å’Œæƒé‡
        scoring_criteria = {
            'accuracy': 0.3,  # æ€»ä½“å‡†ç¡®ç‡ 30%
            'IR_recall': 0.25,  # IRå¬å›ç‡ 25%
            'B_precision': 0.2,  # Bç²¾ç¡®ç‡ 20%
            'N_recall': 0.15,  # Nå¬å›ç‡ 15%
            'macro_f1': 0.1  # å®å¹³å‡F1 10%
        }

        # è®¡ç®—å„æ¨¡å‹ç»¼åˆå¾—åˆ†
        scores = {}
        for model_name, report in models_reports.items():
            score = 0
            score += report.get('accuracy', 0) * scoring_criteria['accuracy']
            score += report.get('IR', {}).get('recall', 0) * scoring_criteria['IR_recall']
            score += report.get('B', {}).get('precision', 0) * scoring_criteria['B_precision']
            score += report.get('N', {}).get('recall', 0) * scoring_criteria['N_recall']
            # ä½¿ç”¨ 'macro avg' æˆ–ç›´æ¥ä»æŠ¥å‘Šä¸­è·å–å®F1
            macro_f1 = report.get('macro avg', {}).get('f1-score', 0)
            if macro_f1 == 0:
                # å¦‚æœ 'macro avg' é”®ä¸å­˜åœ¨ï¼Œå°è¯•ä» 'weighted avg' æˆ–å…¶ä»–æ–¹å¼è·å–ï¼Œæˆ–è€…ç”¨0ä»£æ›¿
                macro_f1 = 0
            score += macro_f1 * scoring_criteria['macro_f1']
            scores[model_name] = score * 100  # è½¬æ¢ä¸ºç™¾åˆ†åˆ¶

        # ç»˜åˆ¶è¯„åˆ†å¡
        plt.figure(figsize=(16, 6))

        # å·¦ä¾§ï¼šç»¼åˆå¾—åˆ†æŸ±çŠ¶å›¾
        models_list = list(scores.keys())
        scores_list = list(scores.values())

        bars = plt.bar(models_list, scores_list, color=['skyblue', 'lightgreen', 'lightcoral'], alpha=0.8)
        plt.ylabel('ç»¼åˆå¾—åˆ† (æ»¡åˆ†100)', fontsize=12)
        plt.title('æ¨¡å‹ç»¼åˆæ€§èƒ½è¯„åˆ†å¡\n(åŠ æƒå¤šç»´åº¦è¯„ä¼°)', fontsize=14, weight='bold')
        plt.grid(True, alpha=0.3, axis='y')

        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, score in zip(bars, scores_list):
            plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 1,
                     f'{score:.1f}', ha='center', va='bottom', fontsize=12, weight='bold')

        plt.ylim(0, 100)
        plt.xticks(rotation=45)

        # å³ä¾§ï¼šæœ€ä½³æ¨¡å‹è¯¦ç»†æ€§èƒ½é›·è¾¾å›¾
        # æ‰¾åˆ°ç»¼åˆå¾—åˆ†æœ€é«˜çš„æ¨¡å‹
        best_model = max(scores, key=scores.get)
        best_report = models_reports[best_model]

        # å®šä¹‰ä¸€ä¸ªå‡½æ•°æ¥å®‰å…¨åœ°è·å–æ•°å€¼
        def get_safe_value(report, key, default=0):
            try:
                if isinstance(report, dict):
                    return report.get(key, default)
                else:
                    return default
            except:
                return default

        # ä½¿ç”¨å®‰å…¨å‡½æ•°è·å–é›·è¾¾å›¾æ•°æ®
        radar_values = [
            get_safe_value(best_report, 'accuracy'),
            get_safe_value(best_report, 'IR', {}).get('recall', 0),
            get_safe_value(best_report, 'B', {}).get('precision', 0),
            get_safe_value(best_report, 'N', {}).get('recall', 0),
            get_safe_value(best_report, 'macro avg', {}).get('f1-score', 0)
        ]

        # ç¡®ä¿æ•°ç»„é•¿åº¦æ­£ç¡®
        if len(radar_values) != 5:
            print(f"âš ï¸ è­¦å‘Š: {best_model} çš„é›·è¾¾å›¾æ•°æ®é•¿åº¦å¼‚å¸¸ï¼Œå·²å¡«å……é»˜è®¤å€¼")
            radar_values = [0] * 5  # æˆ–è€…æ ¹æ®å®é™…æƒ…å†µå¤„ç†

        # ç»˜åˆ¶é›·è¾¾å›¾
        ax2 = plt.subplot(1, 2, 2, projection='polar')
        angles = np.linspace(0, 2 * np.pi, 5, endpoint=False).tolist()
        angles += angles[:1]
        radar_values += radar_values[:1]

        ax2.plot(angles, radar_values, 'o-', linewidth=2, label=best_model, color='red')
        ax2.fill(angles, radar_values, alpha=0.25, color='red')
        ax2.set_xticks(angles[:-1])
        ax2.set_xticklabels(['accuracy', 'IR_recall', 'B_precision', 'N_recall', 'macro_f1'], fontsize=10)
        ax2.set_ylim(0, 1)
        ax2.set_title(f'{best_model} è¯¦ç»†æ€§èƒ½é›·è¾¾å›¾', size=14, weight='bold')
        ax2.legend()
        ax2.grid(True, alpha=0.3)

        plt.tight_layout()
        save_path = os.path.join(output_dir, '12_10_comprehensive_scorecard.png')
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
        print(f"  - âœ… ç»¼åˆæ€§èƒ½è¯„åˆ†å¡å·²ä¿å­˜è‡³: {save_path}")

    except Exception as e:
        print(f"  - âš ï¸ ç”Ÿæˆç»¼åˆæ€§èƒ½è¯„åˆ†å¡æ—¶å‡ºé”™: {e}")


# 7. ç»†åŒ–æ··æ·†çŸ©é˜µï¼ˆçªå‡º IR/Nï¼‰
def plot_detailed_confusion_matrix(y_true, y_pred_models, class_names, output_dir):
    """ç»˜åˆ¶ç»†åŒ–çš„æ··æ·†çŸ©é˜µï¼Œçªå‡ºæ˜¾ç¤º IR å’Œ N ç±»"""
    print("  - æ­£åœ¨ç”Ÿæˆç»†åŒ–æ··æ·†çŸ©é˜µå›¾...")

    try:
        # å‡è®¾ IR æ˜¯ 'IR'ï¼ŒN æ˜¯ 'N'
        ir_index = list(class_names).index('IR')
        n_index = list(class_names).index('N')
        target_indices = [ir_index, n_index]
        target_labels = ['IR', 'N']

        for model_name, y_pred in y_pred_models.items():
            # è®¡ç®—å®Œæ•´æ··æ·†çŸ©é˜µ
            cm_full = confusion_matrix(y_true, y_pred, labels=range(len(class_names)))

            # æå– IR å’Œ N çš„å­çŸ©é˜µ
            cm_sub = cm_full[np.ix_(target_indices, target_indices)]

            # ç»˜åˆ¶ IR/N å­çŸ©é˜µ
            plt.figure(figsize=(6, 5))
            sns.heatmap(cm_sub, annot=True, fmt='d', cmap='Reds',  # ä½¿ç”¨çº¢è‰²ç³»çªå‡º
                        xticklabels=target_labels, yticklabels=target_labels,
                        cbar_kws={"shrink": .8})
            plt.title(f'{model_name} æ¨¡å‹ IR/N ç±»æ··æ·†çŸ©é˜µ', fontsize=14, weight='bold')
            plt.xlabel('é¢„æµ‹æ ‡ç­¾')
            plt.ylabel('çœŸå®æ ‡ç­¾')
            plt.tight_layout()

            save_path = os.path.join(output_dir, f'13_detailed_cm_{model_name}_IR_N.png')
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            plt.close()
            print(f"  - âœ… {model_name} ç»†åŒ–æ··æ·†çŸ©é˜µ (IR/N) å·²ä¿å­˜è‡³: {save_path}")

    except Exception as e:
        print(f"  - âš ï¸ ç”Ÿæˆç»†åŒ–æ··æ·†çŸ©é˜µå›¾æ—¶å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()


# 8. å…³é”®ç±»æ€§èƒ½ç¨³å®šæ€§å›¾ï¼ˆæ–¹æ¡ˆBï¼šç®€åŒ–ç‰ˆï¼‰
def plot_key_class_performance_summary(models_reports, output_dir):
    """ç»˜åˆ¶å…³é”®ç±»æ€§èƒ½æ±‡æ€»å›¾ï¼ˆç®€åŒ–ç‰ˆï¼Œéè·¨æŠ˜ç¨³å®šæ€§ï¼‰"""
    print("  - æ­£åœ¨ç”Ÿæˆå…³é”®ç±»æ€§èƒ½æ±‡æ€»å›¾...")
    try:
        models_names = list(models_reports.keys())
        ir_recalls = [models_reports[m].get('IR', {}).get('recall', 0) for m in models_names]
        n_recalls = [models_reports[m].get('N', {}).get('recall', 0) for m in models_names]
        n_f1s = [models_reports[m].get('N', {}).get('f1-score', 0) for m in models_names]

        x = np.arange(len(models_names))  # æ ‡ç­¾ä½ç½®
        width = 0.25  # æŸ±çŠ¶å›¾çš„å®½åº¦

        fig, ax = plt.subplots(figsize=(10, 6))
        rects1 = ax.bar(x - width, ir_recalls, width, label='IR å¬å›ç‡', color='skyblue')
        rects2 = ax.bar(x, n_recalls, width, label='N å¬å›ç‡', color='lightcoral')
        rects3 = ax.bar(x + width, n_f1s, width, label='N F1åˆ†æ•°', color='lightgreen')

        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        def autolabel(rects):
            for rect in rects:
                height = rect.get_height()
                ax.annotate(f'{height:.2f}',
                            xy=(rect.get_x() + rect.get_width() / 2, height),
                            xytext=(0, 3),  # 3 points vertical offset
                            textcoords="offset points",
                            ha='center', va='bottom', fontsize=9)

        autolabel(rects1)
        autolabel(rects2)
        autolabel(rects3)

        ax.set_ylabel('åˆ†æ•°')
        ax.set_title('å„æ¨¡å‹å…³é”®ç±» (IR, N) æ€§èƒ½æ±‡æ€»')
        ax.set_xticks(x)
        ax.set_xticklabels(models_names)
        ax.legend()
        ax.grid(True, alpha=0.3, axis='y')
        plt.tight_layout()

        save_path = os.path.join(output_dir, '14_key_class_performance_summary.png')
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
        print(f"  - âœ… å…³é”®ç±»æ€§èƒ½æ±‡æ€»å›¾å·²ä¿å­˜è‡³: {save_path}")

    except Exception as e:
        print(f"  - âš ï¸ ç”Ÿæˆå…³é”®ç±»æ€§èƒ½æ±‡æ€»å›¾æ—¶å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()


# ä¸»ç¨‹åº
if __name__ == "__main__":
    set_chinese_font()
    output_dir = create_output_dir()

    print("ğŸš€ å¼€å§‹ç”Ÿæˆä»»åŠ¡äºŒé¢å¤–å¯è§†åŒ–å›¾è¡¨...")

    # åŠ è½½æ•°æ®
    PROCESSED_DIR = os.path.join('..', 'data', 'processed')
    FEATURES_PATH = os.path.join(PROCESSED_DIR, 'source_features_selected.csv')

    try:
        # åŠ è½½ç‰¹å¾æ•°æ®
        df_features = pd.read_csv(FEATURES_PATH)
        X_raw = df_features.drop(columns=['label', 'rpm', 'filename'])
        y_str = df_features['label']
        le = LabelEncoder()
        y = le.fit_transform(y_str)

        print(f"æˆåŠŸåŠ è½½æ•°æ®: {len(X_raw)} ä¸ªæ ·æœ¬")

        # åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
        XGB_MODEL_PATH = os.path.join(PROCESSED_DIR, 'task2_outputs_final', 'final_xgb_model.joblib')
        RF_MODEL_PATH = os.path.join(PROCESSED_DIR, 'task2_outputs_final', 'final_rf_model.joblib')
        # --- ä¿®æ”¹ç‚¹1: æ›´æ­£å˜é‡åï¼ŒæŒ‡å‘ MLP-DA æ¨¡å‹æƒé‡ ---
        MLP_DA_WEIGHTS_PATH = os.path.join(PROCESSED_DIR, 'task2_outputs_final', 'final_mlp_da_model.weights.h5') # <--- ä¿®æ”¹è¿™é‡Œ
        SCALER_PATH = os.path.join(PROCESSED_DIR, 'task2_outputs_final', 'final_scaler.joblib')

        models = {}
        if os.path.exists(XGB_MODEL_PATH):
            models['XGBoost'] = joblib.load(XGB_MODEL_PATH)
            print("æˆåŠŸåŠ è½½XGBoostæ¨¡å‹")
        else:
            print("æœªæ‰¾åˆ°XGBoostæ¨¡å‹")

        if os.path.exists(RF_MODEL_PATH):
            models['RandomForest'] = joblib.load(RF_MODEL_PATH)
            print("æˆåŠŸåŠ è½½éšæœºæ£®æ—æ¨¡å‹")
        else:
            print("æœªæ‰¾åˆ°éšæœºæ£®æ—æ¨¡å‹")

        # --- ä¿®æ”¹ç‚¹2: ç§»é™¤æ—§çš„ CNN-LSTM åŠ è½½é€»è¾‘ï¼Œæ·»åŠ æ–°çš„ MLP-DA åŠ è½½æƒé‡é€»è¾‘ ---
        # if os.path.exists(CNN_LSTM_MODEL_PATH):
        #     models['CNN-LSTM'] = tf.keras.models.load_model(CNN_LSTM_MODEL_PATH)
        #     print("æˆåŠŸåŠ è½½CNN-LSTMæ¨¡å‹")
        # else:
        #     print("æœªæ‰¾åˆ°CNN-LSTMæ¨¡å‹")

        # --- æ–°å¢ï¼šåŠ è½½ MLP-DA æ¨¡å‹æƒé‡ ---
        if os.path.exists(MLP_DA_WEIGHTS_PATH): # <--- ä¿®æ”¹è¿™é‡Œ
            # 1. è·å–æ¨¡å‹è¾“å…¥ç»´åº¦å’Œç±»åˆ«æ•°
            input_dim = X_raw.shape[1]  # (ç‰¹å¾æ•°)
            num_classes = len(np.unique(y))    # ç±»åˆ«æ•°

            # 2. é‡æ–°åˆ›å»ºæ¨¡å‹æ¶æ„ (ä½¿ç”¨ä¸Šé¢å®šä¹‰çš„å‡½æ•°)
            reconstructed_mlp_da_model = create_mlp_da_model(input_dim, num_classes) # <--- ä¿®æ”¹è¿™é‡Œ

            # 3. åŠ è½½æƒé‡åˆ°é‡å»ºçš„æ¨¡å‹ä¸­
            reconstructed_mlp_da_model.load_weights(MLP_DA_WEIGHTS_PATH) # <--- ä¿®æ”¹è¿™é‡Œ
            models['MLP-DA'] = reconstructed_mlp_da_model # <--- ä¿®æ”¹æ¨¡å‹å­—å…¸ä¸­çš„é”®å
            print("æˆåŠŸåŠ è½½MLP-DAæ¨¡å‹ (é€šè¿‡æƒé‡)") # <--- ä¿®æ”¹è¿™é‡Œ
        else:
            print("æœªæ‰¾åˆ°MLP-DAæ¨¡å‹æƒé‡æ–‡ä»¶") # <--- ä¿®æ”¹è¿™é‡Œ
        # --- æ–°å¢ç»“æŸ ---

        # 1. SHAPç‰¹å¾é‡è¦æ€§å›¾ (XGBoost) - ä¿®å¤ç‰ˆ
        if 'XGBoost' in models:
            plot_shap_feature_importance(X_raw, models['XGBoost'], X_raw.columns, output_dir)

        # 2. æ¨¡å‹æ€§èƒ½è¯„ä¼°
        if os.path.exists(SCALER_PATH):
            scaler = joblib.load(SCALER_PATH)
            X_scaled = scaler.transform(X_raw)

            # è·å–é¢„æµ‹ç»“æœå’ŒçœŸå®æ ‡ç­¾
            y_true = y

            # è®¡ç®—å„æ¨¡å‹å‡†ç¡®ç‡å’Œåˆ†ç±»æŠ¥å‘Š
            accuracies = {}
            reports = {}
            y_pred_models = {}  # ä¿å­˜å„æ¨¡å‹é¢„æµ‹ç»“æœ

            for model_name, model in models.items():
                try:
                    if model_name == 'XGBoost':
                        y_pred = model.predict(X_scaled)
                    elif model_name == 'RandomForest':
                        y_pred = model.predict(X_scaled)
                    # --- ä¿®æ”¹ç‚¹3: ä¿®æ”¹ MLP-DA æ¨¡å‹çš„é¢„æµ‹é€»è¾‘ ---
                    # elif model_name == 'CNN-LSTM':
                    #     # æ³¨æ„ï¼šCNN-LSTMæ¨¡å‹çš„è¾“å…¥éœ€è¦æ˜¯3Då¼ é‡
                    #     X_scaled_cnn = np.expand_dims(X_scaled, axis=2)
                    #     y_pred_prob = model.predict(X_scaled_cnn)
                    #     y_pred = np.argmax(y_pred_prob, axis=1)
                    elif model_name == 'MLP-DA': # <--- ä¿®æ”¹è¿™é‡Œ
                        # MLP-DA æ¨¡å‹çš„è¾“å…¥æ˜¯2Då¼ é‡ (samples, features)
                        # è¾“å‡ºæ˜¯ [class_output_probabilities, domain_output_probabilities]
                        y_pred_prob, _ = model.predict(X_scaled) # <--- ä¿®æ”¹è¿™é‡Œ
                        y_pred = np.argmax(y_pred_prob, axis=1)   # <--- ä¿®æ”¹è¿™é‡Œ
                    else:
                        continue

                    accuracy = accuracy_score(y_true, y_pred)
                    report = classification_report(y_true, y_pred, target_names=le.classes_, output_dict=True)

                    accuracies[model_name] = accuracy
                    reports[model_name] = report
                    y_pred_models[model_name] = y_pred  # ä¿å­˜é¢„æµ‹ç»“æœ

                    print(f"  - {model_name} å‡†ç¡®ç‡: {accuracy:.4f}") # <--- è¿™é‡Œä¼šæ‰“å° MLP-DA çš„å‡†ç¡®ç‡
                except Exception as e:
                    print(f"  - âš ï¸ è®¡ç®— {model_name} æ€§èƒ½æ—¶å‡ºé”™: {e}")

            # ç”Ÿæˆæ ¸å¿ƒè¯Šæ–­ç»“æœè¯„ä»·å¯è§†åŒ–å›¾è¡¨
            if reports:
                # ç”Ÿæˆæ‰€æœ‰è¯„ä»·å›¾è¡¨
                plot_safety_economy_tradeoff(reports, output_dir)  # å®‰å…¨æ€§-ç»æµæ€§æƒè¡¡
                plot_key_risk_indicators(reports, output_dir)  # å…³é”®é£é™©æŒ‡æ ‡é›·è¾¾å›¾
                plot_comprehensive_scorecard(reports, output_dir)  # ç»¼åˆæ€§èƒ½è¯„åˆ†å¡
                # --- æ–°å¢è°ƒç”¨ ---
                plot_detailed_confusion_matrix(y_true, y_pred_models, le.classes_, output_dir)
                plot_key_class_performance_summary(reports, output_dir)

            # 3. ç±»åˆ«åˆ†å¸ƒé¥¼å›¾ (ä½œä¸ºèƒŒæ™¯ä¿¡æ¯)
            plot_class_distribution(y_true, le.classes_, output_dir)

            # 4. ç»¼åˆæ··æ·†çŸ©é˜µçƒ­åŠ›å›¾
            if y_pred_models:
                plot_confusion_matrix_heatmap(y_true, y_pred_models, le.classes_, output_dir)

        print(f"\nğŸ‰ ä»»åŠ¡äºŒé¢å¤–å¯è§†åŒ–å›¾è¡¨å·²å…¨éƒ¨ç”Ÿæˆå¹¶ä¿å­˜è‡³: {os.path.abspath(output_dir)}")

    except FileNotFoundError as e:
        print(f"â€¼ï¸ é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ‰€éœ€çš„æ•°æ®æ–‡ä»¶ {e.filename}")
        print("è¯·ç¡®ä¿å·²å®Œæ•´è¿è¡Œå‰é¢çš„æ•°æ®å¤„ç†è„šæœ¬")
    except Exception as e:
        print(f"â€¼ï¸ å‘ç”Ÿé”™è¯¯: {e}")
        import traceback

        traceback.print_exc()
