# 12_task2_additional_visualizations.py
import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib import font_manager
import joblib
import xgboost as xgb
import tensorflow as tf
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.ensemble import RandomForestClassifier
import shap
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Conv1D, BatchNormalization, ReLU, MaxPooling1D, LSTM, \
    GlobalAveragePooling1D, Dense, Dropout, multiply, Activation
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping
import lightgbm as lgb
from sklearn import svm

# ==================== æ·»åŠ æ¨¡å‹å®šä¹‰å‡½æ•° ====================
def categorical_focal_loss(gamma=2., alpha=0.25):
    """
    Focal Loss for addressing class imbalance in categorical classification
    """
    def categorical_focal_loss_fixed(y_true, y_pred):
        epsilon = tf.keras.backend.epsilon()
        y_pred = tf.keras.backend.clip(y_pred, epsilon, 1. - epsilon)
        cross_entropy = -y_true * tf.keras.backend.log(y_pred)
        weight = alpha * y_true * tf.keras.backend.pow((1 - y_pred), gamma)
        loss = weight * cross_entropy
        loss = tf.keras.backend.sum(loss, axis=1)
        return loss
    return categorical_focal_loss_fixed


def create_cnn_lstm_model(input_shape, num_classes):
    """åˆ›å»ºç®€å•çš„CNN+LSTMæ¨¡å‹"""
    inputs = Input(shape=input_shape)

    # CNNç‰¹å¾æå–å±‚
    x = Conv1D(filters=64, kernel_size=3, padding='same')(inputs)
    x = BatchNormalization()(x)
    x = ReLU()(x)
    x = MaxPooling1D(pool_size=2, padding='same')(x)

    x = Conv1D(filters=128, kernel_size=3, padding='same')(x)
    x = BatchNormalization()(x)
    x = ReLU()(x)
    x = MaxPooling1D(pool_size=2, padding='same')(x)

    # LSTMæ—¶åºå»ºæ¨¡å±‚
    x = LSTM(64, return_sequences=True)(x)
    x = LSTM(32, return_sequences=False)(x)

    # å…¨è¿æ¥åˆ†ç±»å±‚
    x = Dense(128, activation='relu')(x)
    x = Dropout(0.5)(x)
    outputs = Dense(num_classes, activation='softmax')(x)

    model = Model(inputs=inputs, outputs=outputs)

    # æ³¨æ„ï¼šåœ¨åŠ è½½æƒé‡æ—¶ï¼Œé€šå¸¸ä¸éœ€è¦ç¼–è¯‘æ¨¡å‹ã€‚
    # ä½†å¦‚æœåç»­éœ€è¦ï¼ˆä¾‹å¦‚å¾®è°ƒï¼‰ï¼Œå¯ä»¥å–æ¶ˆä¸‹é¢çš„æ³¨é‡Šå¹¶ç¡®ä¿ categorical_focal_loss å¯ç”¨ã€‚
    # model.compile(
    #     optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
    #     loss=categorical_focal_loss(gamma=2., alpha=0.5),
    #     metrics=['accuracy']
    # )
    return model
# ==================== æ·»åŠ æ¨¡å‹å®šä¹‰å‡½æ•°ç»“æŸ ====================


# è®¾ç½®ä¸­æ–‡å­—ä½“
def set_chinese_font():
    """è®¾ç½®ä¸­æ–‡å­—ä½“"""
    plt.rcParams['font.sans-serif'] = ['Microsoft YaHei']
    plt.rcParams['axes.unicode_minus'] = False
    print("âœ… å·²è®¾ç½®ä¸­æ–‡å­—ä½“ã€‚")


# åˆ›å»ºè¾“å‡ºç›®å½•
def create_output_dir():
    """åˆ›å»ºå¯è§†åŒ–è¾“å‡ºç›®å½•"""
    output_dir = os.path.join('..', 'data', 'processed', 'task2_visualizations')
    os.makedirs(output_dir, exist_ok=True)
    return output_dir


# 1. SHAPç‰¹å¾é‡è¦æ€§å›¾ï¼ˆTop 10ç‰¹å¾ï¼‰- XGBoost (ä¿®å¤ç‰ˆ)
def plot_shap_feature_importance(X, model, feature_names, output_dir):
    """ç»˜åˆ¶SHAPç‰¹å¾é‡è¦æ€§å›¾"""
    print("  - æ­£åœ¨ç”ŸæˆSHAPç‰¹å¾é‡è¦æ€§å›¾...")

    try:
        # ç¡®ä¿Xæ˜¯numpyæ•°ç»„ä¸”å½¢çŠ¶æ­£ç¡®
        if isinstance(X, pd.DataFrame):
            X_array = X.values
        else:
            X_array = np.asarray(X)

        # åˆ›å»ºSHAPè§£é‡Šå™¨
        explainer = shap.TreeExplainer(model)

        # ä¸ºäº†æé«˜æ•ˆç‡ï¼Œåªä½¿ç”¨éƒ¨åˆ†æ•°æ®è®¡ç®—SHAPå€¼
        sample_size = min(500, X_array.shape[0])  # å‡å°‘æ ·æœ¬æ•°é¿å…å†…å­˜é—®é¢˜
        sample_indices = np.random.choice(X_array.shape[0], sample_size, replace=False)
        X_sample = X_array[sample_indices]

        # ç¡®ä¿ç‰¹å¾åç§°æ˜¯åˆ—è¡¨
        if isinstance(feature_names, pd.Index):
            feature_names_list = feature_names.tolist()
        else:
            feature_names_list = list(feature_names)

        # ç¡®ä¿X_sampleæ˜¯äºŒç»´æ•°ç»„ä¸”åˆ—æ•°ä¸ç‰¹å¾åç§°åŒ¹é…
        if X_sample.ndim == 1:
            X_sample = X_sample.reshape(-1, 1)

        # å¦‚æœç‰¹å¾æ•°é‡ä¸åŒ¹é…ï¼Œæˆªå–æˆ–è¡¥é½
        if X_sample.shape[1] != len(feature_names_list):
            min_features = min(X_sample.shape[1], len(feature_names_list))
            X_sample = X_sample[:, :min_features]
            feature_names_list = feature_names_list[:min_features]
            print(f"  - âš ï¸ ç‰¹å¾æ•°é‡ä¸åŒ¹é…ï¼Œå·²è°ƒæ•´ä¸º {min_features} ä¸ªç‰¹å¾")

        # åˆ›å»ºDataFrameç¡®ä¿åˆ—åæ­£ç¡®
        X_sample_df = pd.DataFrame(X_sample, columns=feature_names_list)

        # è®¡ç®—SHAPå€¼
        shap_values = explainer.shap_values(X_sample_df)

        # å¦‚æœæ˜¯å¤šåˆ†ç±»ï¼Œshap_valuesæ˜¯ä¸€ä¸ªåˆ—è¡¨
        if isinstance(shap_values, list):
            # å¯¹äºå¤šåˆ†ç±»ï¼Œæˆ‘ä»¬è®¡ç®—æ¯ä¸ªç±»åˆ«çš„å¹³å‡ç»å¯¹SHAPå€¼
            shap_importance = np.mean([np.abs(sv).mean(0) for sv in shap_values], axis=0)
        else:
            shap_importance = np.abs(shap_values).mean(0)

        # ç¡®ä¿shap_importanceæ˜¯ä¸€ç»´æ•°ç»„ä¸”é•¿åº¦ä¸ç‰¹å¾åç§°åŒ¹é…
        if hasattr(shap_importance, 'ndim') and shap_importance.ndim > 1:
            shap_importance = shap_importance.flatten()

        if len(shap_importance) != len(feature_names_list):
            min_len = min(len(shap_importance), len(feature_names_list))
            shap_importance = shap_importance[:min_len]
            feature_names_list = feature_names_list[:min_len]

        # åˆ›å»ºç‰¹å¾é‡è¦æ€§DataFrame
        feature_importance_df = pd.DataFrame({
            'feature': feature_names_list,
            'importance': np.abs(shap_importance)  # ç¡®ä¿æ˜¯ç»å¯¹å€¼
        }).sort_values('importance', ascending=False)

        # ç»˜åˆ¶Top 10ç‰¹å¾é‡è¦æ€§
        plt.figure(figsize=(12, 8))
        top_features = feature_importance_df.head(10)
        sns.barplot(data=top_features, x='importance', y='feature', palette='viridis')
        plt.title('Top 10 SHAPç‰¹å¾é‡è¦æ€§ (XGBoost)', fontsize=16, weight='bold')
        plt.xlabel('å¹³å‡SHAPå€¼', fontsize=12)
        plt.ylabel('ç‰¹å¾åç§°', fontsize=12)
        plt.tight_layout()

        save_path = os.path.join(output_dir, '12_1_shap_feature_importance_xgboost.png')
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
        print(f"  - âœ… SHAPç‰¹å¾é‡è¦æ€§å›¾å·²ä¿å­˜è‡³: {save_path}")

    except Exception as e:
        print(f"  - âš ï¸ ç”ŸæˆSHAPç‰¹å¾é‡è¦æ€§å›¾æ—¶å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()


# 2. ç±»åˆ«åˆ†å¸ƒé¥¼å›¾ (ä½œä¸ºé—®é¢˜èƒŒæ™¯è¯´æ˜)
def plot_class_distribution(y_true, class_names, output_dir):
    """ç»˜åˆ¶ç±»åˆ«åˆ†å¸ƒé¥¼å›¾"""
    print("  - æ­£åœ¨ç”Ÿæˆç±»åˆ«åˆ†å¸ƒé¥¼å›¾...")

    try:
        # ç»Ÿè®¡å„ç±»åˆ«æ•°é‡
        unique, counts = np.unique(y_true, return_counts=True)
        class_counts = dict(zip(unique, counts))
        class_labels = [class_names[i] for i in unique]
        class_values = [class_counts[i] for i in unique]

        # ç»˜åˆ¶é¥¼å›¾
        plt.figure(figsize=(10, 8))
        colors = ['gold', 'yellowgreen', 'lightcoral', 'lightskyblue', 'orange']
        plt.pie(class_values, labels=class_labels, colors=colors, autopct='%1.1f%%', startangle=140)
        plt.title('æºåŸŸæ•°æ®é›†ç±»åˆ«åˆ†å¸ƒ\n(ç”¨äºæ­ç¤ºæ•°æ®ä¸å¹³è¡¡æŒ‘æˆ˜)', fontsize=16, weight='bold')
        plt.axis('equal')

        save_path = os.path.join(output_dir, '12_4_class_distribution.png')
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
        print(f"  - âœ… ç±»åˆ«åˆ†å¸ƒé¥¼å›¾å·²ä¿å­˜è‡³: {save_path}")

    except Exception as e:
        print(f"  - âš ï¸ ç”Ÿæˆç±»åˆ«åˆ†å¸ƒé¥¼å›¾æ—¶å‡ºé”™: {e}")


# 3. æ··æ·†çŸ©é˜µçƒ­åŠ›å›¾ï¼ˆç»¼åˆï¼‰
def plot_confusion_matrix_heatmap(y_true, y_pred_models, class_names, output_dir):
    """ç»˜åˆ¶ç»¼åˆæ··æ·†çŸ©é˜µçƒ­åŠ›å›¾"""
    print("  - æ­£åœ¨ç”Ÿæˆç»¼åˆæ··æ·†çŸ©é˜µçƒ­åŠ›å›¾...")

    try:
        # é€‰æ‹©æœ€ä½³æ¨¡å‹ï¼ˆè¿™é‡Œé€‰æ‹©ç¬¬ä¸€ä¸ªæ¨¡å‹ï¼‰è¿›è¡Œè¯¦ç»†åˆ†æ
        best_model_name = list(y_pred_models.keys())[0]
        y_pred_best = y_pred_models[best_model_name]

        # è®¡ç®—æ··æ·†çŸ©é˜µ
        cm = confusion_matrix(y_true, y_pred_best)

        # ç»˜åˆ¶çƒ­åŠ›å›¾
        plt.figure(figsize=(10, 8))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                    xticklabels=class_names, yticklabels=class_names)
        plt.title(f'{best_model_name} æ¨¡å‹æ··æ·†çŸ©é˜µ', fontsize=16, weight='bold')
        plt.xlabel('é¢„æµ‹æ ‡ç­¾')
        plt.ylabel('çœŸå®æ ‡ç­¾')

        save_path = os.path.join(output_dir, '12_5_confusion_matrix_heatmap.png')
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
        print(f"  - âœ… ç»¼åˆæ··æ·†çŸ©é˜µçƒ­åŠ›å›¾å·²ä¿å­˜è‡³: {save_path}")

    except Exception as e:
        print(f"  - âš ï¸ ç”Ÿæˆç»¼åˆæ··æ·†çŸ©é˜µçƒ­åŠ›å›¾æ—¶å‡ºé”™: {e}")


# 4. ä¸šåŠ¡ä»·å€¼å¯¼å‘è¯„ä»·å›¾ï¼ˆå®‰å…¨æ€§-ç»æµæ€§æƒè¡¡ï¼‰
def plot_safety_economy_tradeoff(models_reports, output_dir):
    """ç»˜åˆ¶å®‰å…¨æ€§å’Œç»æµæ€§æƒè¡¡å›¾"""
    print("  - æ­£åœ¨ç”Ÿæˆå®‰å…¨æ€§-ç»æµæ€§æƒè¡¡å›¾...")

    try:
        models_names = list(models_reports.keys())
        safety_scores = []  # IRç±»å¬å›ç‡ï¼ˆå®‰å…¨æ€§ï¼‰
        economy_scores = []  # Bç±»ç²¾ç¡®ç‡ï¼ˆç»æµæ€§ï¼‰

        for model_name, report in models_reports.items():
            ir_recall = report.get('IR', {}).get('recall', 0)
            b_precision = report.get('B', {}).get('precision', 0)
            safety_scores.append(ir_recall)
            economy_scores.append(b_precision)

        plt.figure(figsize=(10, 8))
        scatter = plt.scatter(economy_scores, safety_scores, s=150, alpha=0.7, c=range(len(models_names)),
                              cmap='viridis')

        # æ·»åŠ æ¨¡å‹åç§°æ ‡ç­¾
        for i, model_name in enumerate(models_names):
            plt.annotate(model_name, (economy_scores[i], safety_scores[i]),
                         xytext=(5, 5), textcoords='offset points', fontsize=10, weight='bold')

        plt.xlabel('ç»æµæ€§æŒ‡æ ‡ (Bç±»ç²¾ç¡®ç‡)\nâ†‘ é¿å…è¯¯åˆ¤ï¼Œå‡å°‘ä¸å¿…è¦åœæœº', fontsize=12)
        plt.ylabel('å®‰å…¨æ€§æŒ‡æ ‡ (IRç±»å¬å›ç‡)\nâ†‘ é¿å…æ¼æ£€ï¼Œä¿éšœè®¾å¤‡å®‰å…¨', fontsize=12)
        plt.title('æ¨¡å‹å®‰å…¨æ€§-ç»æµæ€§æƒè¡¡åˆ†æ\n(å·¦ä¸Šè§’ä¸ºç†æƒ³åŒºåŸŸ)', fontsize=14, weight='bold')
        plt.grid(True, alpha=0.3)

        # æ·»åŠ è±¡é™å‚è€ƒçº¿ (x=0.98, y=0.98)ï¼Œä»£è¡¨ç†æƒ³ç›®æ ‡
        plt.axhline(y=0.98, color='r', linestyle='--', alpha=0.5, linewidth=1)
        plt.axvline(x=0.98, color='r', linestyle='--', alpha=0.5, linewidth=1)

        # å°†åæ ‡è½´èŒƒå›´è°ƒæ•´åˆ°æ›´ç²¾ç»†çš„åŒºé—´ï¼Œä»¥çªå‡ºå¾®å°å·®å¼‚
        plt.xlim(0.96, 1.01)
        plt.ylim(0.96, 1.01)

        save_path = os.path.join(output_dir, '12_6_safety_economy_tradeoff.png')
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
        print(f"  - âœ… å®‰å…¨æ€§-ç»æµæ€§æƒè¡¡å›¾å·²ä¿å­˜è‡³: {save_path}")

    except Exception as e:
        print(f"  - âš ï¸ ç”Ÿæˆå®‰å…¨æ€§-ç»æµæ€§æƒè¡¡å›¾æ—¶å‡ºé”™: {e}")


# 5. å…³é”®é£é™©æŒ‡æ ‡é›·è¾¾å›¾
def plot_key_risk_indicators(models_reports, output_dir):
    """ç»˜åˆ¶å…³é”®é£é™©æŒ‡æ ‡é›·è¾¾å›¾"""
    print("  - æ­£åœ¨ç”Ÿæˆå…³é”®é£é™©æŒ‡æ ‡é›·è¾¾å›¾...")

    try:
        models_names = list(models_reports.keys())

        # æå–å…³é”®æŒ‡æ ‡
        indicators = ['IRå¬å›ç‡', 'Bç²¾ç¡®ç‡', 'Nå¬å›ç‡', 'ORç²¾ç¡®ç‡', 'æ€»ä½“å‡†ç¡®ç‡']
        data = {}

        for model_name, report in models_reports.items():
            data[model_name] = [
                report.get('IR', {}).get('recall', 0),
                report.get('B', {}).get('precision', 0),
                report.get('N', {}).get('recall', 0),
                report.get('OR', {}).get('precision', 0),
                report.get('accuracy', 0)
            ]

        # ç»˜åˆ¶é›·è¾¾å›¾
        labels = np.array(indicators)
        num_vars = len(labels)
        angles = np.linspace(0, 2 * np.pi, num_vars, endpoint=False).tolist()
        angles += angles[:1]  # é—­åˆå›¾å½¢

        fig, ax = plt.subplots(figsize=(12, 10), subplot_kw=dict(projection='polar'))

        colors = ['red', 'blue', 'green', 'orange', 'purple']
        for i, (model_name, values) in enumerate(data.items()):
            values += values[:1]  # é—­åˆå›¾å½¢
            ax.plot(angles, values, 'o-', linewidth=2, label=model_name, color=colors[i % len(colors)])
            ax.fill(angles, values, alpha=0.25, color=colors[i % len(colors)])

        ax.set_xticks(angles[:-1])
        ax.set_xticklabels(labels, fontsize=11)
        ax.set_ylim(0, 1)
        ax.set_title('å…³é”®é£é™©æŒ‡æ ‡é›·è¾¾å›¾\n(è¶Šé è¿‘è¾¹ç¼˜è¶Šå¥½)', size=16, weight='bold', pad=30)
        ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))
        ax.grid(True, alpha=0.3)

        save_path = os.path.join(output_dir, '12_7_key_risk_indicators.png')
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
        print(f"  - âœ… å…³é”®é£é™©æŒ‡æ ‡é›·è¾¾å›¾å·²ä¿å­˜è‡³: {save_path}")

    except Exception as e:
        print(f"  - âš ï¸ ç”Ÿæˆå…³é”®é£é™©æŒ‡æ ‡é›·è¾¾å›¾æ—¶å‡ºé”™: {e}")


# 6. ç»¼åˆæ€§èƒ½è¯„åˆ†å¡
def plot_comprehensive_scorecard(models_reports, output_dir):
    """ç»˜åˆ¶ç»¼åˆæ€§èƒ½è¯„åˆ†å¡"""
    print("  - æ­£åœ¨ç”Ÿæˆç»¼åˆæ€§èƒ½è¯„åˆ†å¡...")

    try:
        models_names = list(models_reports.keys())

        # å®šä¹‰è¯„åˆ†ç»´åº¦å’Œæƒé‡
        scoring_criteria = {
            'accuracy': 0.3,  # æ€»ä½“å‡†ç¡®ç‡ 30%
            'IR_recall': 0.25,  # IRå¬å›ç‡ 25%
            'B_precision': 0.2,  # Bç²¾ç¡®ç‡ 20%
            'N_recall': 0.15,  # Nå¬å›ç‡ 15%
            'macro_f1': 0.1  # å®å¹³å‡F1 10%
        }

        # è®¡ç®—å„æ¨¡å‹ç»¼åˆå¾—åˆ†
        scores = {}
        for model_name, report in models_reports.items():
            score = 0
            score += report.get('accuracy', 0) * scoring_criteria['accuracy']
            score += report.get('IR', {}).get('recall', 0) * scoring_criteria['IR_recall']
            score += report.get('B', {}).get('precision', 0) * scoring_criteria['B_precision']
            score += report.get('N', {}).get('recall', 0) * scoring_criteria['N_recall']
            # ä½¿ç”¨ 'macro avg' æˆ–ç›´æ¥ä»æŠ¥å‘Šä¸­è·å–å®F1
            macro_f1 = report.get('macro avg', {}).get('f1-score', 0)
            if macro_f1 == 0:
                # å¦‚æœ 'macro avg' é”®ä¸å­˜åœ¨ï¼Œå°è¯•ä» 'weighted avg' æˆ–å…¶ä»–æ–¹å¼è·å–ï¼Œæˆ–è€…ç”¨0ä»£æ›¿
                macro_f1 = 0
            score += macro_f1 * scoring_criteria['macro_f1']
            scores[model_name] = score * 100  # è½¬æ¢ä¸ºç™¾åˆ†åˆ¶

        # ç»˜åˆ¶è¯„åˆ†å¡
        plt.figure(figsize=(16, 6))

        # å·¦ä¾§ï¼šç»¼åˆå¾—åˆ†æŸ±çŠ¶å›¾
        models_list = list(scores.keys())
        scores_list = list(scores.values())

        bars = plt.bar(models_list, scores_list, color=['skyblue', 'lightgreen', 'lightcoral'], alpha=0.8)
        plt.ylabel('ç»¼åˆå¾—åˆ† (æ»¡åˆ†100)', fontsize=12)
        plt.title('æ¨¡å‹ç»¼åˆæ€§èƒ½è¯„åˆ†å¡\n(åŠ æƒå¤šç»´åº¦è¯„ä¼°)', fontsize=14, weight='bold')
        plt.grid(True, alpha=0.3, axis='y')

        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, score in zip(bars, scores_list):
            plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 1,
                     f'{score:.1f}', ha='center', va='bottom', fontsize=12, weight='bold')

        plt.ylim(0, 100)
        plt.xticks(rotation=45)

        # å³ä¾§ï¼šæœ€ä½³æ¨¡å‹è¯¦ç»†æ€§èƒ½é›·è¾¾å›¾
        # æ‰¾åˆ°ç»¼åˆå¾—åˆ†æœ€é«˜çš„æ¨¡å‹
        best_model = max(scores, key=scores.get)
        best_report = models_reports[best_model]

        # å®šä¹‰ä¸€ä¸ªå‡½æ•°æ¥å®‰å…¨åœ°è·å–æ•°å€¼
        def get_safe_value(report, key, default=0):
            try:
                if isinstance(report, dict):
                    return report.get(key, default)
                else:
                    return default
            except:
                return default

        # ä½¿ç”¨å®‰å…¨å‡½æ•°è·å–é›·è¾¾å›¾æ•°æ®
        radar_values = [
            get_safe_value(best_report, 'accuracy'),
            get_safe_value(best_report, 'IR', {}).get('recall', 0),
            get_safe_value(best_report, 'B', {}).get('precision', 0),
            get_safe_value(best_report, 'N', {}).get('recall', 0),
            get_safe_value(best_report, 'macro avg', {}).get('f1-score', 0)
        ]

        # ç¡®ä¿æ•°ç»„é•¿åº¦æ­£ç¡®
        if len(radar_values) != 5:
            print(f"âš ï¸ è­¦å‘Š: {best_model} çš„é›·è¾¾å›¾æ•°æ®é•¿åº¦å¼‚å¸¸ï¼Œå·²å¡«å……é»˜è®¤å€¼")
            radar_values = [0] * 5  # æˆ–è€…æ ¹æ®å®é™…æƒ…å†µå¤„ç†

        # ç»˜åˆ¶é›·è¾¾å›¾
        ax2 = plt.subplot(1, 2, 2, projection='polar')
        angles = np.linspace(0, 2 * np.pi, 5, endpoint=False).tolist()
        angles += angles[:1]
        radar_values += radar_values[:1]

        ax2.plot(angles, radar_values, 'o-', linewidth=2, label=best_model, color='red')
        ax2.fill(angles, radar_values, alpha=0.25, color='red')
        ax2.set_xticks(angles[:-1])
        ax2.set_xticklabels(['accuracy', 'IR_recall', 'B_precision', 'N_recall', 'macro_f1'], fontsize=10)
        ax2.set_ylim(0, 1)
        ax2.set_title(f'{best_model} è¯¦ç»†æ€§èƒ½é›·è¾¾å›¾', size=14, weight='bold')
        ax2.legend()
        ax2.grid(True, alpha=0.3)

        plt.tight_layout()
        save_path = os.path.join(output_dir, '12_10_comprehensive_scorecard.png')
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
        print(f"  - âœ… ç»¼åˆæ€§èƒ½è¯„åˆ†å¡å·²ä¿å­˜è‡³: {save_path}")

    except Exception as e:
        print(f"  - âš ï¸ ç”Ÿæˆç»¼åˆæ€§èƒ½è¯„åˆ†å¡æ—¶å‡ºé”™: {e}")


# 7. ç»†åŒ–æ··æ·†çŸ©é˜µï¼ˆçªå‡º IR/Nï¼‰
def plot_detailed_confusion_matrix(y_true, y_pred_models, class_names, output_dir):
    """ç»˜åˆ¶ç»†åŒ–çš„æ··æ·†çŸ©é˜µï¼Œçªå‡ºæ˜¾ç¤º IR å’Œ N ç±»"""
    print("  - æ­£åœ¨ç”Ÿæˆç»†åŒ–æ··æ·†çŸ©é˜µå›¾...")

    try:
        # å‡è®¾ IR æ˜¯ 'IR'ï¼ŒN æ˜¯ 'N'
        ir_index = list(class_names).index('IR')
        n_index = list(class_names).index('N')
        target_indices = [ir_index, n_index]
        target_labels = ['IR', 'N']

        for model_name, y_pred in y_pred_models.items():
            # è®¡ç®—å®Œæ•´æ··æ·†çŸ©é˜µ
            cm_full = confusion_matrix(y_true, y_pred, labels=range(len(class_names)))

            # æå– IR å’Œ N çš„å­çŸ©é˜µ
            cm_sub = cm_full[np.ix_(target_indices, target_indices)]

            # ç»˜åˆ¶ IR/N å­çŸ©é˜µ
            plt.figure(figsize=(6, 5))
            sns.heatmap(cm_sub, annot=True, fmt='d', cmap='Reds',  # ä½¿ç”¨çº¢è‰²ç³»çªå‡º
                        xticklabels=target_labels, yticklabels=target_labels,
                        cbar_kws={"shrink": .8})
            plt.title(f'{model_name} æ¨¡å‹ IR/N ç±»æ··æ·†çŸ©é˜µ', fontsize=14, weight='bold')
            plt.xlabel('é¢„æµ‹æ ‡ç­¾')
            plt.ylabel('çœŸå®æ ‡ç­¾')
            plt.tight_layout()

            save_path = os.path.join(output_dir, f'13_detailed_cm_{model_name}_IR_N.png')
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            plt.close()
            print(f"  - âœ… {model_name} ç»†åŒ–æ··æ·†çŸ©é˜µ (IR/N) å·²ä¿å­˜è‡³: {save_path}")

    except Exception as e:
        print(f"  - âš ï¸ ç”Ÿæˆç»†åŒ–æ··æ·†çŸ©é˜µå›¾æ—¶å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()


# --- ä¿®æ”¹ï¼šæ›´æ–°å‡½æ•°ç­¾åå’Œå†…éƒ¨é€»è¾‘ ---
# 8. å…³é”®ç±»æ€§èƒ½ç¨³å®šæ€§å›¾ï¼ˆæ–¹æ¡ˆBï¼šç®€åŒ–ç‰ˆï¼‰
# def plot_key_class_performance_summary(models_reports, output_dir): # <--- æ—§ç­¾å
def plot_key_class_performance_summary(y_pred_models_dict, class_names, y_true_input, output_dir): # <--- æ–°ç­¾å
    """ç»˜åˆ¶å…³é”®ç±»æ€§èƒ½æ±‡æ€»å›¾ï¼ˆç®€åŒ–ç‰ˆï¼Œéè·¨æŠ˜ç¨³å®šæ€§ï¼‰"""
    print("  - æ­£åœ¨ç”Ÿæˆå…³é”®ç±»æ€§èƒ½æ±‡æ€»å›¾...")
    try:
        # models_names = list(models_reports.keys()) # <--- æ—§é€»è¾‘
        models_names = list(y_pred_models_dict.keys()) # <--- æ–°é€»è¾‘
        # ir_recalls = [models_reports[m].get('IR', {}).get('recall', 0) for m in models_names] # <--- æ—§é€»è¾‘
        # n_recalls = [models_reports[m].get('N', {}).get('recall', 0) for m in models_names] # <--- æ—§é€»è¾‘
        # n_f1s = [models_reports[m].get('N', {}).get('f1-score', 0) for m in models_names] # <--- æ—§é€»è¾‘

        # --- æ–°å¢ï¼šæ ¹æ® y_pred é‡æ–°è®¡ç®—æŒ‡æ ‡ ---
        ir_recalls = []
        n_recalls = []
        n_f1s = []
        for model_name in models_names:
            y_pred = y_pred_models_dict[model_name]
            # ä½¿ç”¨ sklearn.metrics.classification_report ä¸´æ—¶è®¡ç®—
            tmp_report = classification_report(y_true_input, y_pred, target_names=class_names, output_dict=True)
            ir_recalls.append(tmp_report.get('IR', {}).get('recall', 0))
            n_recalls.append(tmp_report.get('N', {}).get('recall', 0))
            n_f1s.append(tmp_report.get('N', {}).get('f1-score', 0))
        # --- æ–°å¢ç»“æŸ ---

        x = np.arange(len(models_names))  # æ ‡ç­¾ä½ç½®
        width = 0.25  # æŸ±çŠ¶å›¾çš„å®½åº¦

        fig, ax = plt.subplots(figsize=(10, 6))
        rects1 = ax.bar(x - width, ir_recalls, width, label='IR å¬å›ç‡', color='skyblue')
        rects2 = ax.bar(x, n_recalls, width, label='N å¬å›ç‡', color='lightcoral')
        rects3 = ax.bar(x + width, n_f1s, width, label='N F1åˆ†æ•°', color='lightgreen')

        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        def autolabel(rects):
            for rect in rects:
                height = rect.get_height()
                ax.annotate(f'{height:.2f}',
                            xy=(rect.get_x() + rect.get_width() / 2, height),
                            xytext=(0, 3),  # 3 points vertical offset
                            textcoords="offset points",
                            ha='center', va='bottom', fontsize=9)

        autolabel(rects1)
        autolabel(rects2)
        autolabel(rects3)

        ax.set_ylabel('åˆ†æ•°')
        ax.set_title('å„æ¨¡å‹å…³é”®ç±» (IR, N) æ€§èƒ½æ±‡æ€»')
        ax.set_xticks(x)
        ax.set_xticklabels(models_names)
        ax.legend()
        ax.grid(True, alpha=0.3, axis='y')
        plt.tight_layout()

        save_path = os.path.join(output_dir, '14_key_class_performance_summary.png')
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
        print(f"  - âœ… å…³é”®ç±»æ€§èƒ½æ±‡æ€»å›¾å·²ä¿å­˜è‡³: {save_path}")

    except Exception as e:
        print(f"  - âš ï¸ ç”Ÿæˆå…³é”®ç±»æ€§èƒ½æ±‡æ€»å›¾æ—¶å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
# --- ä¿®æ”¹ç»“æŸ ---


# --- ä¿®æ”¹ï¼šç”Ÿæˆæ‰€æœ‰æ¨¡å‹çš„æ··æ·†çŸ©é˜µå’Œç²¾åº¦å¯¹æ¯”å›¾ ---
# def plot_model_comparison_charts(models_reports, output_dir): # <--- æ—§ç­¾å
def plot_model_comparison_charts(y_pred_models_dict, class_names, y_true_input, output_dir): # <--- æ–°ç­¾å
    """ç”Ÿæˆæ‰€æœ‰æ¨¡å‹çš„æ€§èƒ½å¯¹æ¯”å›¾è¡¨"""
    print("  - æ­£åœ¨ç”Ÿæˆæ‰€æœ‰æ¨¡å‹æ€§èƒ½å¯¹æ¯”å›¾è¡¨...")
    try:
        # 1. æ··æ·†çŸ©é˜µå¯¹æ¯” (å­å›¾)
        # å‡è®¾æœ€å¤š6ä¸ªæ¨¡å‹ï¼Œç”¨2x3å¸ƒå±€
        num_models = len(y_pred_models_dict)
        rows = (num_models + 2) // 3 # å‘ä¸Šå–æ•´è®¡ç®—è¡Œæ•°
        cols = 3 if num_models > 1 else 1
        if num_models <= 2:
            rows = 1
            cols = num_models

        fig_cm, axes = plt.subplots(rows, cols, figsize=(6 * cols, 5 * rows))
        # å¤„ç†åªæœ‰ä¸€ä¸ªå­å›¾çš„æƒ…å†µ
        if num_models == 1:
            axes = [axes]
        elif rows == 1 or cols == 1:
            axes = axes.flatten() if hasattr(axes, 'flatten') else [axes]
        else:
            axes = axes.ravel()

        for idx, (model_name, y_pred) in enumerate(y_pred_models_dict.items()): # <--- ä¿®æ”¹ï¼šéå† y_pred_models_dict
            if idx < len(axes):
                # --- ä¿®æ”¹ï¼šä½¿ç”¨ä¼ å…¥çš„ y_true_input å’Œ y_pred ---
                cm = confusion_matrix(y_true_input, y_pred) # <--- ä¿®æ”¹è¿™é‡Œ ---
                sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                            # --- ä¿®æ”¹ï¼šä½¿ç”¨ä¼ å…¥çš„ class_names ---
                            xticklabels=class_names, yticklabels=class_names, # <--- ä¿®æ”¹è¿™é‡Œ ---
                            ax=axes[idx], cbar_kws={"shrink": .8}) # æ·»åŠ é¢œè‰²æ¡ç¼©å°
                axes[idx].set_title(f'{model_name} æ··æ·†çŸ©é˜µ', fontsize=14)
                axes[idx].set_xlabel('Predicted Label')
                axes[idx].set_ylabel('True Label')

        # éšè—å¤šä½™çš„å­å›¾
        for j in range(len(y_pred_models_dict), len(axes)): # <--- ä¿®æ”¹ï¼šä½¿ç”¨ len(y_pred_models_dict) ---
            axes[j].set_visible(False)

        plt.tight_layout()
        save_path_cm_all = os.path.join(output_dir, '12_11_all_models_confusion_matrices.png')
        plt.savefig(save_path_cm_all, dpi=300, bbox_inches='tight')
        plt.close()
        print(f"  - âœ… æ‰€æœ‰æ¨¡å‹æ··æ·†çŸ©é˜µå¯¹æ¯”å›¾å·²ä¿å­˜è‡³: {save_path_cm_all}")

        # 2. ç²¾åº¦/æŒ‡æ ‡å¯¹æ¯” (æ¡å½¢å›¾)
        comparison_data = []
        # --- ä¿®æ”¹ï¼šéå† y_pred_models_dict å¹¶è®¡ç®—æŒ‡æ ‡ ---
        for model_name, y_pred in y_pred_models_dict.items(): # <--- ä¿®æ”¹è¿™é‡Œ ---
            # --- ä¿®æ”¹ï¼šä½¿ç”¨ä¼ å…¥çš„ y_true_input è®¡ç®—å‡†ç¡®ç‡ ---
            accuracy = accuracy_score(y_true_input, y_pred) # <--- ä¿®æ”¹è¿™é‡Œ ---
            # --- ä¿®æ”¹ï¼šä½¿ç”¨ä¼ å…¥çš„ y_true_input å’Œ class_names è®¡ç®—æŠ¥å‘Š ---
            report_dict = classification_report(y_true_input, y_pred, target_names=class_names, output_dict=True) # <--- ä¿®æ”¹è¿™é‡Œ ---
            macro_f1 = report_dict.get('macro avg', {}).get('f1-score', 0)
            weighted_f1 = report_dict.get('weighted avg', {}).get('f1-score', 0)
            ir_recall = report_dict.get('IR', {}).get('recall', 0)
            b_precision = report_dict.get('B', {}).get('precision', 0)
            n_recall = report_dict.get('N', {}).get('recall', 0) # å‡è®¾ N ç±»å­˜åœ¨

            comparison_data.append({
                'Model': model_name,
                'Accuracy': accuracy, # <--- ä½¿ç”¨è®¡ç®—å‡ºçš„å‡†ç¡®ç‡ ---
                'Macro F1': macro_f1,
                'Weighted F1': weighted_f1,
                'IR Recall': ir_recall,
                'B Precision': b_precision,
                'N Recall': n_recall
            })
        # --- ä¿®æ”¹ç»“æŸ ---

        df_comparison = pd.DataFrame(comparison_data)
        # melted_df = df_comparison.melt(id_vars=['Model'], var_name='Metric', value_name='Score')

        fig_metrics, ax = plt.subplots(figsize=(12, 8))

        # ä½¿ç”¨ Seaborn æ›´ç®€æ´åœ°ç»˜åˆ¶
        df_melted = df_comparison.melt(id_vars=['Model'], value_vars=['Accuracy', 'Macro F1', 'Weighted F1', 'IR Recall', 'B Precision', 'N Recall'],
                                     var_name='Metric', value_name='Score')
        sns.barplot(x='Model', y='Score', hue='Metric', data=df_melted, ax=ax)
        ax.set_title('æ¨¡å‹æ€§èƒ½æŒ‡æ ‡å¯¹æ¯” (5æŠ˜äº¤å‰éªŒè¯)', fontsize=16)
        ax.set_ylabel('Score')
        ax.set_xlabel('Model')
        ax.legend(title='Metric')
        plt.xticks(rotation=45)
        plt.tight_layout()

        save_path_metrics = os.path.join(output_dir, '12_12_model_performance_comparison.png')
        plt.savefig(save_path_metrics, dpi=300, bbox_inches='tight')
        plt.close()
        print(f"  - âœ… æ¨¡å‹æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”å›¾å·²ä¿å­˜è‡³: {save_path_metrics}")

    except Exception as e:
        print(f"  - âš ï¸ ç”Ÿæˆæ€§èƒ½å¯¹æ¯”å›¾è¡¨æ—¶å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
# --- ä¿®æ”¹ç»“æŸ ---


# ä¸»ç¨‹åº
if __name__ == "__main__":
    set_chinese_font()
    output_dir = create_output_dir()

    print("ğŸš€ å¼€å§‹ç”Ÿæˆä»»åŠ¡äºŒé¢å¤–å¯è§†åŒ–å›¾è¡¨...")

    # åŠ è½½æ•°æ®
    PROCESSED_DIR = os.path.join('..', 'data', 'processed')
    FEATURES_PATH = os.path.join(PROCESSED_DIR, 'source_features_selected.csv')

    try:
        # åŠ è½½ç‰¹å¾æ•°æ®
        df_features = pd.read_csv(FEATURES_PATH)
        X_raw = df_features.drop(columns=['label', 'rpm', 'filename'])
        y_str = df_features['label']
        le = LabelEncoder()
        y = le.fit_transform(y_str)

        print(f"æˆåŠŸåŠ è½½æ•°æ®: {len(X_raw)} ä¸ªæ ·æœ¬")

        # åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
        XGB_MODEL_PATH = os.path.join(PROCESSED_DIR, 'task2_outputs_final', 'final_xgb_model.joblib')
        RF_MODEL_PATH = os.path.join(PROCESSED_DIR, 'task2_outputs_final', 'final_rf_model.joblib')
        SVM_MODEL_PATH = os.path.join(PROCESSED_DIR, 'task2_outputs_final', 'final_svm_model.joblib') # <--- æ–°å¢ ---
        LGB_MODEL_PATH = os.path.join(PROCESSED_DIR, 'task2_outputs_final', 'final_lgb_model.txt') # <--- æ–°å¢ ---
        # --- ä¿®æ”¹ç‚¹1: æ›´æ­£å˜é‡å ---
        CNN_LSTM_WEIGHTS_PATH = os.path.join(PROCESSED_DIR, 'task2_outputs_final', 'final_cnn_lstm_model.weights.h5') # <--- ä¿®æ”¹è¿™é‡Œ ---
        SCALER_PATH = os.path.join(PROCESSED_DIR, 'task2_outputs_final', 'final_scaler.joblib')

        models = {}
        if os.path.exists(XGB_MODEL_PATH):
            models['XGBoost'] = joblib.load(XGB_MODEL_PATH)
            print("æˆåŠŸåŠ è½½XGBoostæ¨¡å‹")
        else:
            print("æœªæ‰¾åˆ°XGBoostæ¨¡å‹")

        if os.path.exists(RF_MODEL_PATH):
            models['RandomForest'] = joblib.load(RF_MODEL_PATH)
            print("æˆåŠŸåŠ è½½éšæœºæ£®æ—æ¨¡å‹")
        else:
            print("æœªæ‰¾åˆ°éšæœºæ£®æ—æ¨¡å‹")

        # --- æ–°å¢ï¼šåŠ è½½ SVM æ¨¡å‹ ---
        if os.path.exists(SVM_MODEL_PATH):
            models['SVM'] = joblib.load(SVM_MODEL_PATH)
            print("æˆåŠŸåŠ è½½SVMæ¨¡å‹")
        else:
            print("æœªæ‰¾åˆ°SVMæ¨¡å‹")
        # --- æ–°å¢ç»“æŸ ---

        # --- æ–°å¢ï¼šåŠ è½½ LightGBM æ¨¡å‹ ---
        if os.path.exists(LGB_MODEL_PATH):
            # LightGBM Booster å¯¹è±¡éœ€è¦ä½¿ç”¨ lgb.Booster åŠ è½½
            models['LightGBM'] = lgb.Booster(model_file=LGB_MODEL_PATH)
            print("æˆåŠŸåŠ è½½LightGBMæ¨¡å‹")
        else:
            print("æœªæ‰¾åˆ°LightGBMæ¨¡å‹")
        # --- æ–°å¢ç»“æŸ ---

        # --- ä¿®æ”¹ç‚¹2: ç§»é™¤æ—§çš„åŠ è½½é€»è¾‘ï¼Œæ·»åŠ æ–°çš„åŠ è½½æƒé‡é€»è¾‘ ---
        # if os.path.exists(CNN_LSTM_MODEL_PATH):
        #     models['CNN-LSTM'] = tf.keras.models.load_model(CNN_LSTM_MODEL_PATH)  # æ–°å¢åŠ è½½CNN-LSTM
        #     print("æˆåŠŸåŠ è½½CNN-LSTMæ¨¡å‹")
        # else:
        #     print("æœªæ‰¾åˆ°CNN-LSTMæ¨¡å‹")

        # --- æ–°å¢ï¼šåŠ è½½ CNN-LSTM æ¨¡å‹æƒé‡ ---
        if os.path.exists(CNN_LSTM_WEIGHTS_PATH): # <--- ä¿®æ”¹è¿™é‡Œ ---
            # 1. è·å–æ¨¡å‹è¾“å…¥å½¢çŠ¶å’Œç±»åˆ«æ•°
            input_shape = (X_raw.shape[1], 1)  # (ç‰¹å¾æ•°, 1)
            num_classes = len(np.unique(y))    # ç±»åˆ«æ•°

            # 2. é‡æ–°åˆ›å»ºæ¨¡å‹æ¶æ„ (ä½¿ç”¨ä¸Šé¢å®šä¹‰çš„å‡½æ•°)
            reconstructed_cnn_lstm_model = create_cnn_lstm_model(input_shape, num_classes) # <--- ä¿®æ”¹è¿™é‡Œ ---

            # 3. åŠ è½½æƒé‡åˆ°é‡å»ºçš„æ¨¡å‹ä¸­
            reconstructed_cnn_lstm_model.load_weights(CNN_LSTM_WEIGHTS_PATH) # <--- ä¿®æ”¹è¿™é‡Œ ---
            models['CNN-LSTM'] = reconstructed_cnn_lstm_model # <--- ä¿®æ”¹æ¨¡å‹å­—å…¸ä¸­çš„é”®å ---
            print("æˆåŠŸåŠ è½½CNN-LSTMæ¨¡å‹ (é€šè¿‡æƒé‡)") # <--- ä¿®æ”¹è¿™é‡Œ ---
        else:
            print("æœªæ‰¾åˆ°CNN-LSTMæ¨¡å‹æƒé‡æ–‡ä»¶") # <--- ä¿®æ”¹è¿™é‡Œ ---
        # --- æ–°å¢ç»“æŸ ---

        # 1. SHAPç‰¹å¾é‡è¦æ€§å›¾ (XGBoost) - ä¿®å¤ç‰ˆ
        if 'XGBoost' in models:
            plot_shap_feature_importance(X_raw, models['XGBoost'], X_raw.columns, output_dir)

        # 2. æ¨¡å‹æ€§èƒ½è¯„ä¼°
        if os.path.exists(SCALER_PATH):
            scaler = joblib.load(SCALER_PATH)
            X_scaled = scaler.transform(X_raw)

            # è·å–é¢„æµ‹ç»“æœå’ŒçœŸå®æ ‡ç­¾
            y_true = y # <--- å®šä¹‰ y_true ä¾›åç»­ä½¿ç”¨ ---
            global y_true_global # <--- å®šä¹‰å…¨å±€å˜é‡ä¾›å‡½æ•°ä½¿ç”¨ ---
            y_true_global = y_true
            global le_global # <--- å®šä¹‰å…¨å±€å˜é‡ä¾›å‡½æ•°ä½¿ç”¨ ---
            le_global = le

            # è®¡ç®—å„æ¨¡å‹å‡†ç¡®ç‡å’Œåˆ†ç±»æŠ¥å‘Š
            accuracies = {}
            reports = {} # ä¿å­˜ classification_report å­—å…¸
            y_pred_models = {}  # ä¿å­˜å„æ¨¡å‹é¢„æµ‹ç»“æœæ•°ç»„ <--- å…³é”®ä¿®æ”¹ ---

            for model_name, model in models.items():
                try:
                    if model_name == 'XGBoost':
                        y_pred = model.predict(X_scaled)
                    elif model_name == 'RandomForest':
                        y_pred = model.predict(X_scaled)
                    # --- æ–°å¢ï¼šSVM æ¨¡å‹é¢„æµ‹é€»è¾‘ ---
                    elif model_name == 'SVM':
                        y_pred = model.predict(X_scaled)
                    # --- æ–°å¢ç»“æŸ ---
                    # --- æ–°å¢ï¼šLightGBM æ¨¡å‹é¢„æµ‹é€»è¾‘ ---
                    elif model_name == 'LightGBM':
                        # LightGBM Booster éœ€è¦ä½¿ç”¨ predict
                        y_pred_prob = model.predict(X_scaled)
                        y_pred = np.argmax(y_pred_prob, axis=1)
                    # --- æ–°å¢ç»“æŸ ---
                    # --- ä¿®æ”¹ï¼šä¿®æ­£ CNN-LSTM æ¨¡å‹çš„é¢„æµ‹é€»è¾‘ ---
                    # elif model_name == 'CNN-LSTM': # <--- æ—§çš„é”™è¯¯é€»è¾‘ ---
                    #     # CNN-LSTM æ¨¡å‹çš„è¾“å…¥æ˜¯2Då¼ é‡ (samples, features)
                    #     # è¾“å‡ºæ˜¯ [class_output_probabilities, domain_output_probabilities]
                    #     X_scaled_cnn = np.expand_dims(X_scaled, axis=2) # <--- æ·»åŠ è¿™è¡Œ ---
                    #     y_pred_prob, _ = model.predict(X_scaled_cnn) # <--- ä¿®æ”¹è¿™é‡Œ ---
                    #     y_pred = np.argmax(y_pred_prob, axis=1)   # <--- ä¿®æ”¹è¿™é‡Œ ---
                    elif model_name == 'CNN-LSTM':  # <--- æ–°çš„æ­£ç¡®é€»è¾‘ ---
                        # 1. å‡†å¤‡è¾“å…¥æ•°æ® (3D å¼ é‡)
                        X_scaled_cnn = np.expand_dims(X_scaled, axis=2)
                        # 2. æ¨¡å‹é¢„æµ‹ (å¯¹äºå•è¾“å‡ºæ¨¡å‹ï¼Œpredict è¿”å›å•ä¸ªæ•°ç»„)
                        y_pred_prob = model.predict(X_scaled_cnn)  # <--- ä¿®æ”¹è¿™é‡Œ ---
                        # 3. è·å–é¢„æµ‹ç±»åˆ«
                        y_pred = np.argmax(y_pred_prob, axis=1)  # <--- ä¿®æ”¹è¿™é‡Œ ---
                    # --- ä¿®æ”¹ç»“æŸ ---
                    else:
                        continue

                    accuracy = accuracy_score(y_true, y_pred)
                    report = classification_report(y_true, y_pred, target_names=le.classes_, output_dict=True)

                    accuracies[model_name] = accuracy
                    reports[model_name] = report
                    y_pred_models[model_name] = y_pred  # ä¿å­˜é¢„æµ‹ç»“æœæ•°ç»„

                    print(f"  - {model_name} å‡†ç¡®ç‡: {accuracy:.4f}")
                except Exception as e:
                    print(f"  - âš ï¸ è®¡ç®— {model_name} æ€§èƒ½æ—¶å‡ºé”™: {e}")
                    import traceback

                    traceback.print_exc()  # æ‰“å°å®Œæ•´é”™è¯¯å †æ ˆï¼Œä¾¿äºè°ƒè¯•

            # ç”Ÿæˆæ ¸å¿ƒè¯Šæ–­ç»“æœè¯„ä»·å¯è§†åŒ–å›¾è¡¨
            if reports and y_pred_models: # <--- ä¿®æ”¹ï¼šç¡®ä¿ä¸¤ä¸ªå­—å…¸éƒ½ä¸ä¸ºç©º ---
                # ç”Ÿæˆæ‰€æœ‰è¯„ä»·å›¾è¡¨
                plot_safety_economy_tradeoff(reports, output_dir)  # å®‰å…¨æ€§-ç»æµæ€§æƒè¡¡
                plot_key_risk_indicators(reports, output_dir)  # å…³é”®é£é™©æŒ‡æ ‡é›·è¾¾å›¾
                plot_comprehensive_scorecard(reports, output_dir)  # ç»¼åˆæ€§èƒ½è¯„åˆ†å¡
                # --- ä¿®æ”¹è°ƒç”¨ ---
                # plot_detailed_confusion_matrix(y_true, y_pred_models, le.classes_, output_dir) # <--- æ—§è°ƒç”¨ ---
                # plot_key_class_performance_summary(reports, output_dir) # <--- æ—§è°ƒç”¨ ---
                # plot_model_comparison_charts(reports, output_dir) # <--- æ—§è°ƒç”¨ (é”™è¯¯çš„) ---
                # --- æ–°å¢è°ƒç”¨ ---
                plot_detailed_confusion_matrix(y_true, y_pred_models, le.classes_, output_dir) # <--- ä¿æŒä¸å˜ ---
                # --- ä¿®æ”¹ï¼šä¼ é€’æ­£ç¡®çš„å‚æ•° ---
                plot_key_class_performance_summary(y_pred_models, le.classes_, y_true, output_dir) # <--- ä¿®æ”¹è¿™é‡Œ ---
                plot_model_comparison_charts(y_pred_models, le.classes_, y_true, output_dir) # <--- ä¿®æ”¹è¿™é‡Œ ---
                # --- ä¿®æ”¹ç»“æŸ ---

            # 3. ç±»åˆ«åˆ†å¸ƒé¥¼å›¾ (ä½œä¸ºèƒŒæ™¯ä¿¡æ¯)
            plot_class_distribution(y_true, le.classes_, output_dir)

            # 4. ç»¼åˆæ··æ·†çŸ©é˜µçƒ­åŠ›å›¾
            if y_pred_models: # <--- ä¿®æ”¹ï¼šä½¿ç”¨ y_pred_models ---
                plot_confusion_matrix_heatmap(y_true, y_pred_models, le.classes_, output_dir) # <--- ä¿®æ”¹ï¼šä½¿ç”¨ y_pred_models ---

        print(f"\nğŸ‰ ä»»åŠ¡äºŒé¢å¤–å¯è§†åŒ–å›¾è¡¨å·²å…¨éƒ¨ç”Ÿæˆå¹¶ä¿å­˜è‡³: {os.path.abspath(output_dir)}")

    except FileNotFoundError as e:
        print(f"â€¼ï¸ é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ‰€éœ€çš„æ•°æ®æ–‡ä»¶ {e.filename}ã€‚")
        print("è¯·ç¡®ä¿å·²å®Œæ•´è¿è¡Œå‰é¢çš„æ•°æ®å¤„ç†è„šæœ¬")
    except Exception as e:
        print(f"â€¼ï¸ å‘ç”Ÿé”™è¯¯: {e}")
        import traceback

        traceback.print_exc()
