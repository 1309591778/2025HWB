import os
import re
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from matplotlib import font_manager
from matplotlib.ticker import PercentFormatter  # ã€æ–°å¢ã€‘å¯¼å…¥ç”¨äºæ ¼å¼åŒ–ç™¾åˆ†æ¯”çš„å·¥å…·
from sklearn.feature_selection import f_classif
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import LabelEncoder


# ==============================================================================
# 0. å­—ä½“è®¾ç½®å‡½æ•°
# ==============================================================================
def set_chinese_font():
    """
    å¼ºåˆ¶è®¾ç½®ä¸­æ–‡å­—ä½“ä¸º 'Microsoft YaHei'ï¼Œè§£å†³ä¸­æ–‡æ˜¾ç¤ºé—®é¢˜ã€‚
    """
    plt.rcParams['font.sans-serif'] = ['Microsoft YaHei']
    plt.rcParams['axes.unicode_minus'] = False
    print("âœ… å·²è®¾ç½®ä¸­æ–‡å­—ä½“ã€‚")


# ==============================================================================
# 1. ä¸»ç¨‹åº
# ==============================================================================
if __name__ == "__main__":
    set_chinese_font()

    # --- è·¯å¾„å®šä¹‰ ---
    PROCESSED_DIR = os.path.join('..', 'data', 'processed')
    FEATURES_PATH = os.path.join(PROCESSED_DIR, 'source_features.csv')
    OUTPUT_DIR = os.path.join(PROCESSED_DIR, 'task1_outputs')  # ä¸ºä»»åŠ¡ä¸€çš„è¾“å‡ºåˆ›å»ºä¸€ä¸ªä¸“é—¨çš„æ–‡ä»¶å¤¹
    os.makedirs(OUTPUT_DIR, exist_ok=True)

    # --- æ•°æ®åŠ è½½ ---
    try:
        df_features = pd.read_csv(FEATURES_PATH)
    except FileNotFoundError:
        print(f"â€¼ï¸ é”™è¯¯ï¼šæ‰¾ä¸åˆ°ç‰¹å¾æ–‡ä»¶ {FEATURES_PATH}ã€‚è¯·å…ˆè¿è¡Œ03è„šæœ¬ç”Ÿæˆå¢å¼ºç‰ˆç‰¹å¾é›†ã€‚")
        exit()

    if 'filename' not in df_features.columns:
        print("â€¼ï¸ é”™è¯¯ï¼šç‰¹å¾æ–‡ä»¶ä¸­ç¼ºå°‘'filename'åˆ—ã€‚è¯·æŒ‰æŒ‡å¯¼ä¿®æ”¹å¹¶é‡æ–°è¿è¡Œ02å’Œ03è„šæœ¬ã€‚")
        exit()

    X = df_features.drop(columns=['label', 'rpm', 'filename'])
    y_str = df_features['label']
    le = LabelEncoder()
    y = le.fit_transform(y_str)

    # --- åéªŒéªŒè¯ 1: ç›¸å…³æ€§åˆ†æ ---
    print("\nğŸš€ åéªŒéªŒè¯ 1: åˆ†æé«˜åº¦ç›¸å…³çš„ç‰¹å¾å¯¹...")
    corr_matrix = X.corr().abs()
    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))
    highly_correlated = [column for column in upper.columns if any(upper[column] > 0.8)]
    for col in highly_correlated:
        correlated_with = upper.index[upper[col] > 0.8].tolist()
        print(f"  - ç‰¹å¾ '{col}' ä¸ {correlated_with} é«˜åº¦ç›¸å…³ (r>0.8)")

    # --- åéªŒéªŒè¯ 2: æ–¹å·®åˆ†æ (ANOVA) ---
    print("\nğŸš€ åéªŒéªŒè¯ 2: ANOVA Få€¼æ’åº (éªŒè¯ç‰¹å¾åŒºåˆ†åº¦)...")
    f_values, p_values = f_classif(X, y)
    df_anova = pd.DataFrame({'feature': X.columns, 'F_score': f_values}).sort_values(by='F_score', ascending=False)
    print("  - ANOVA Få€¼æ’åå‰15çš„ç‰¹å¾:")
    print(df_anova.head(15))

    # --- åéªŒéªŒè¯ 3: éšæœºæ£®æ—ç‰¹å¾é‡è¦æ€§ ---
    print("\nğŸš€ åéªŒéªŒè¯ 3: éšæœºæ£®æ—ç‰¹å¾é‡è¦æ€§æ’åº...")
    model = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')
    model.fit(X, y)
    importances = model.feature_importances_
    df_importance = pd.DataFrame({'feature': X.columns, 'importance': importances}).sort_values(by='importance',
                                                                                                ascending=False)
    print("  - éšæœºæ£®æ—æ¨¡å‹è®¤ä¸ºæœ€é‡è¦çš„15ä¸ªç‰¹å¾:")
    print(df_importance.head(15))

    # å¯è§†åŒ–ç‰¹å¾é‡è¦æ€§æ¡å½¢å›¾
    plt.figure(figsize=(12, 10))
    sns.barplot(x='importance', y='feature', data=df_importance.head(20))
    plt.title('Top 20 ç‰¹å¾é‡è¦æ€§æ’åº (éšæœºæ£®æ—)', fontsize=16)
    save_path_bar = os.path.join(OUTPUT_DIR, 'task1_feature_importance_bar.png')
    plt.savefig(save_path_bar, dpi=300)
    print(f"\nâœ… ç‰¹å¾é‡è¦æ€§æ’åºå›¾å·²ä¿å­˜è‡³: {os.path.abspath(save_path_bar)}")
    plt.close()

    # --- ã€æ–°å¢ã€‘å¯è§†åŒ–ï¼šç‰¹å¾ç´¯ç§¯é‡è¦æ€§æ›²çº¿ ---
    print("\nğŸš€ æ–°å¢å¯è§†åŒ–ï¼šç”Ÿæˆç‰¹å¾ç´¯ç§¯é‡è¦æ€§æ›²çº¿...")

    # è®¡ç®—ç´¯ç§¯é‡è¦æ€§åˆ†æ•° (å·²ç»æ˜¯æŒ‰é‡è¦æ€§é™åºæ’åˆ—çš„)
    cumulative_importance = np.cumsum(df_importance['importance'])

    # åˆ›å»ºå›¾è¡¨
    plt.figure(figsize=(12, 8))
    plt.plot(range(1, len(cumulative_importance) + 1), cumulative_importance, marker='o', linestyle='--')

    # è®¾ç½®Yè½´ä¸ºç™¾åˆ†æ¯”æ ¼å¼
    plt.gca().yaxis.set_major_formatter(PercentFormatter(1.0))

    # æ ‡è®°95%å’Œ99%çš„é‡è¦æ€§é˜ˆå€¼çº¿
    plt.axhline(y=0.95, color='r', linestyle='--', label='95% ç´¯ç§¯é‡è¦æ€§')

    # æ‰¾åˆ°è¾¾åˆ°95%é˜ˆå€¼éœ€è¦å¤šå°‘ä¸ªç‰¹å¾
    try:
        num_features_95 = np.where(cumulative_importance >= 0.95)[0][0] + 1
        plt.text(num_features_95 + 0.5, 0.93, f'Top {num_features_95} ä¸ªç‰¹å¾', color='r', fontsize=12)
        # ç»˜åˆ¶å‚çº¿ä»¥æ›´æ¸…æ™°åœ°æŒ‡ç¤ºä½ç½®
        plt.axvline(x=num_features_95, color='r', linestyle=':', alpha=0.7)
    except IndexError:
        print("  - æ³¨æ„ï¼šæ‰€æœ‰ç‰¹å¾çš„ç´¯ç§¯é‡è¦æ€§æœªè¾¾åˆ°95%ã€‚")

    plt.title('ç‰¹å¾ç´¯ç§¯é‡è¦æ€§æ›²çº¿', fontsize=18, weight='bold')
    plt.xlabel('æŒ‰é‡è¦æ€§æ’åºçš„ç‰¹å¾æ•°é‡', fontsize=14)
    plt.ylabel('ç´¯ç§¯é‡è¦æ€§', fontsize=14)
    plt.grid(True)
    plt.legend()
    plt.tight_layout()

    save_path_curve = os.path.join(OUTPUT_DIR, 'task1_cumulative_feature_importance_curve.png')
    plt.savefig(save_path_curve, dpi=300)
    print(f"âœ… ç‰¹å¾ç´¯ç§¯é‡è¦æ€§æ›²çº¿å›¾å·²ä¿å­˜è‡³: {os.path.abspath(save_path_curve)}")
    plt.close()

    # --- æœ€ç»ˆç­›é€‰å†³ç­– ---
    print("\nğŸš€ æœ€ç»ˆå†³ç­–ï¼šç­›é€‰ç‰¹å¾...")
    # æˆ‘ä»¬å¯ä»¥æ ¹æ®ç´¯ç§¯é‡è¦æ€§æ›²çº¿çš„ç»“æœæ¥åŠ¨æ€å†³å®šä¿ç•™å¤šå°‘ä¸ªç‰¹å¾
    # ä¾‹å¦‚ï¼Œä¿ç•™è¾¾åˆ°95%é‡è¦æ€§çš„æ‰€æœ‰ç‰¹å¾
    final_features_to_keep = df_importance['feature'].iloc[:num_features_95].tolist()

    # æ£€æŸ¥å¹¶ç§»é™¤é«˜åº¦ç›¸å…³çš„ç‰¹å¾
    # (è¿™æ˜¯ä¸€ä¸ªç®€åŒ–çš„å»å†—ä½™é€»è¾‘ç¤ºä¾‹)
    for col in highly_correlated:
        if col in final_features_to_keep:
            correlated_with = upper.index[upper[col] > 0.8].tolist()
            # å¦‚æœä¸colç›¸å…³çš„ç‰¹å¾ä¹Ÿåœ¨ä¿ç•™åˆ—è¡¨ä¸­ï¼Œä¸”colçš„é‡è¦æ€§æ›´ä½ï¼Œåˆ™è€ƒè™‘ç§»é™¤col
            # æ­¤å¤„é€»è¾‘è¾ƒä¸ºå¤æ‚ï¼Œå¯ä»¥å…ˆæ‰‹åŠ¨å†³ç­–ï¼Œä¾‹å¦‚æˆ‘ä»¬å·²çŸ¥rmså’Œstd_devé«˜åº¦ç›¸å…³
            if 'std_dev' in final_features_to_keep and 'rms' in final_features_to_keep:
                # å‡è®¾æˆ‘ä»¬é€šè¿‡ANOVAæˆ–æ¨¡å‹é‡è¦æ€§ï¼Œå†³å®šä¿ç•™std_dev
                final_features_to_keep.remove('rms')

    print(f"  - âœ… å†³ç­–å®Œæˆï¼šç­›é€‰å‡º {len(final_features_to_keep)} ä¸ªç‰¹å¾ç”¨äºåç»­å»ºæ¨¡ã€‚")
    print(f"  - ç­›é€‰å‡ºçš„ç‰¹å¾åˆ—è¡¨: {final_features_to_keep}")

    # ä¿å­˜æœ€ç»ˆç­›é€‰çš„ç‰¹å¾é›†
    final_columns = ['filename', 'label', 'rpm'] + final_features_to_keep
    df_final_source = df_features[final_columns]

    save_path_source = os.path.join(PROCESSED_DIR, 'source_features_selected.csv')
    df_final_source.to_csv(save_path_source, index=False)
    print(f"  - âœ… ç­›é€‰åçš„æºåŸŸç‰¹å¾é›†å·²ä¿å­˜è‡³: {os.path.abspath(save_path_source)}")

    # ä¿å­˜ç­›é€‰å‡ºçš„ç‰¹å¾åç§°åˆ—è¡¨ï¼Œä¾›06è„šæœ¬ä½¿ç”¨
    save_path_feature_list = os.path.join(PROCESSED_DIR, 'selected_feature_names.txt')
    with open(save_path_feature_list, 'w') as f:
        for feature_name in final_features_to_keep:
            f.write(f"{feature_name}\n")
    print(f"  - âœ… ç­›é€‰å‡ºçš„ç‰¹å¾åç§°åˆ—è¡¨å·²ä¿å­˜è‡³: {os.path.abspath(save_path_feature_list)}")

    print("\nğŸ‰ ä»»åŠ¡ä¸€çš„ç‰¹å¾ç­›é€‰å·¥ä½œå·²å®Œæˆï¼")