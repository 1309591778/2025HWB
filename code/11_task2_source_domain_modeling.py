# 11_task2_source_domain_modeling.py
import os
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from matplotlib import font_manager
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score
from sklearn.ensemble import RandomForestClassifier  # æ·»åŠ éšæœºæ£®æ—
# --- æ–°å¢å¯¼å…¥ ---
import joblib
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Conv1D, BatchNormalization, ReLU, MaxPooling1D, LSTM, \
    GlobalAveragePooling1D, Dense, Dropout, multiply, Activation
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping
# ä¿®æ­£importè¯­å¥
from sklearn.utils.class_weight import compute_class_weight  # æ­£ç¡®çš„å‡½æ•°å
# åœ¨ç°æœ‰å¯¼å…¥åæ·»åŠ 
import xgboost as xgb
# --- æ–°å¢å¯¼å…¥ ---
import lightgbm as lgb
from sklearn import svm
# --- æ–°å¢å¯¼å…¥ç»“æŸ ---
from sklearn.model_selection import StratifiedGroupKFold, train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix


# ==============================================================================
# 0. è¾…åŠ©å‡½æ•°
# ==============================================================================
def set_chinese_font():
    """è®¾ç½®ä¸­æ–‡å­—ä½“"""
    plt.rcParams['font.sans-serif'] = ['Microsoft YaHei']
    plt.rcParams['axes.unicode_minus'] = False
    print("âœ… å·²è®¾ç½®ä¸­æ–‡å­—ä½“ã€‚")


def categorical_focal_loss(gamma=2., alpha=None): # <--- alpha å¯ä»¥æ˜¯åˆ—è¡¨æˆ– None
    """
    Focal Loss for addressing class imbalance in categorical classification.
    Supports per-class alpha weights.
    """
    def categorical_focal_loss_fixed(y_true, y_pred):
        epsilon = tf.keras.backend.epsilon()
        y_pred = tf.keras.backend.clip(y_pred, epsilon, 1. - epsilon)
        cross_entropy = -y_true * tf.keras.backend.log(y_pred)

        # --- ä¿®æ”¹ç‚¹ï¼šæ”¯æŒ alpha ä¸ºåˆ—è¡¨ ---
        if alpha is not None:
            # å‡è®¾ alpha æ˜¯ä¸€ä¸ªåˆ—è¡¨æˆ– numpy arrayï¼Œé•¿åº¦ç­‰äºç±»åˆ«æ•°
            # y_true æ˜¯ one-hot ç¼–ç ï¼Œshape [batch_size, num_classes]
            # alpha éœ€è¦ reshape æˆ [1, num_classes] ä»¥ä¾¿å¹¿æ’­
            alpha_tensor = tf.constant(alpha, dtype=tf.float32)
            alpha_tensor = tf.reshape(alpha_tensor, [1, -1])
            # weight = alpha * y_true * (1 - y_pred)^gamma
            weight = alpha_tensor * y_true * tf.keras.backend.pow((1 - y_pred), gamma)
        else:
            # å¦‚æœ alpha ä¸º Noneï¼Œåˆ™ä¸ä½¿ç”¨æƒé‡
            weight = y_true * tf.keras.backend.pow((1 - y_pred), gamma)
        # --- ä¿®æ”¹ç»“æŸ ---

        loss = weight * cross_entropy
        loss = tf.keras.backend.sum(loss, axis=1)
        return loss
    return categorical_focal_loss_fixed


def create_cnn_lstm_model(input_shape, num_classes):
    """åˆ›å»ºç®€å•çš„CNN+LSTMæ¨¡å‹"""
    inputs = Input(shape=input_shape)

    # CNNç‰¹å¾æå–å±‚
    x = Conv1D(filters=64, kernel_size=3, padding='same')(inputs)
    x = BatchNormalization()(x)
    x = ReLU()(x)
    x = MaxPooling1D(pool_size=2, padding='same')(x)

    x = Conv1D(filters=128, kernel_size=3, padding='same')(x)
    x = BatchNormalization()(x)
    x = ReLU()(x)
    x = MaxPooling1D(pool_size=2, padding='same')(x)

    # LSTMæ—¶åºå»ºæ¨¡å±‚
    x = LSTM(64, return_sequences=True)(x)
    x = LSTM(32, return_sequences=False)(x)

    # å…¨è¿æ¥åˆ†ç±»å±‚
    x = Dense(128, activation='relu')(x)
    x = Dropout(0.5)(x)
    outputs = Dense(num_classes, activation='softmax')(x)

    model = Model(inputs=inputs, outputs=outputs)

    # ä½¿ç”¨Focal Lossæ›¿ä»£æ ‡å‡†äº¤å‰ç†µæŸå¤±
    num_classes = len(le.classes_)  # é€šå¸¸æ˜¯ 4
    # åˆ›å»º alpha åˆ—è¡¨ï¼Œç´¢å¼•å¯¹åº”ç±»åˆ«
    # ä¾‹å¦‚ï¼Œç»™ B ç±» (ç´¢å¼• 0) æ›´é«˜çš„æƒé‡
    alpha_list = [0.25] * num_classes  # é»˜è®¤æ‰€æœ‰ç±»æƒé‡ 0.25
    b_class_index = 0  # ç¡®è®¤ B ç±»ç´¢å¼•
    alpha_list[b_class_index] = 0.75  # ç»™ B ç±»æ›´é«˜çš„æƒé‡ (ä¾‹å¦‚ 0.75)

    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
        loss=categorical_focal_loss(gamma=2., alpha=alpha_list),  # <--- ä½¿ç”¨ alpha åˆ—è¡¨
        metrics=['accuracy']
    )
    return model


def compute_balanced_sample_weights(y, n_class_index=2, n_class_weight=3.0):
    """è®¡ç®—å¹³è¡¡çš„æ ·æœ¬æƒé‡ï¼Œç»™Nç±»æ›´é«˜çš„æƒé‡"""
    # è·å–ç±»åˆ«æ•°é‡
    n_classes = len(np.unique(y))

    # é»˜è®¤æƒé‡ä¸º1.0
    class_weights = {i: 1.0 for i in range(n_classes)}

    # ç»™Nç±»ï¼ˆå‡è®¾ç´¢å¼•ä¸º2ï¼‰æ›´é«˜çš„æƒé‡
    class_weights[n_class_index] = n_class_weight

    # ä¸ºæ¯ä¸ªæ ·æœ¬åˆ†é…æƒé‡
    sample_weights = np.array([class_weights[label] for label in y])
    return sample_weights


# ==============================================================================
# æ–°å¢ï¼šXGBoostæ¨¡å‹ç›¸å…³å‡½æ•°
# ==============================================================================
def create_xgb_model_params():
    """åˆ›å»ºXGBoostæ¨¡å‹å‚æ•°"""
    params = {
        'objective': 'multi:softprob',
        'eval_metric': 'mlogloss',
        'num_class': 4,  # å‡è®¾æœ‰4ä¸ªç±»åˆ«
        'random_state': 42,
        # ä¼˜åŒ–å‚æ•°
        'max_depth': 6,          # é™ä½æ·±åº¦é¿å…è¿‡æ‹Ÿåˆ
        'learning_rate': 0.1,    # æé«˜å­¦ä¹ ç‡
        'subsample': 0.8,
        'colsample_bytree': 0.8,
        'min_child_weight': 3,   # å¢åŠ æœ€å°æƒé‡
        'gamma': 0.2,            # å¢åŠ æœ€å°æŸå¤±å‡å°‘é‡
    }
    return params


def train_xgb_model_cv(X_train, y_train, X_val, y_val, sample_weights_train=None, sample_weights_val=None):
    """è®­ç»ƒXGBoostæ¨¡å‹ï¼ˆäº¤å‰éªŒè¯ç”¨ï¼‰"""
    # è½¬æ¢ä¸º DMatrix æ ¼å¼
    if sample_weights_train is not None and sample_weights_val is not None:
        dtrain = xgb.DMatrix(X_train, label=y_train, weight=sample_weights_train)
        dval = xgb.DMatrix(X_val, label=y_val, weight=sample_weights_val)
    else:
        dtrain = xgb.DMatrix(X_train, label=y_train)
        dval = xgb.DMatrix(X_val, label=y_val)

    # è·å–å‚æ•°
    params = create_xgb_model_params()
    params['num_class'] = len(np.unique(y_train))  # åŠ¨æ€è®¾ç½®ç±»åˆ«æ•°

    # ä½¿ç”¨åŸç”Ÿè®­ç»ƒ API + æ—©åœ
    evals_result = {}
    bst = xgb.train(
        params,
        dtrain,
        num_boost_round=500,     # å‡å°‘è¿­ä»£æ¬¡æ•°
        evals=[(dtrain, 'train'), (dval, 'val')],
        early_stopping_rounds=30, # æ—©åœè½®æ•°
        evals_result=evals_result,
        verbose_eval=False
    )

    return bst


def predict_xgb_model(model, X_test):
    """ä½¿ç”¨XGBoostæ¨¡å‹è¿›è¡Œé¢„æµ‹"""
    dtest = xgb.DMatrix(X_test)
    class_probs = model.predict(dtest)
    y_pred = np.argmax(class_probs, axis=1)
    return y_pred, class_probs

# ==============================================================================
# 1. ä¸»ç¨‹åº
# ==============================================================================
if __name__ == "__main__":
    set_chinese_font()

    RANDOM_STATE = 42
    N_SPLITS = 5
    PROCESSED_DIR = os.path.join('..', 'data', 'processed')
    FEATURES_PATH = os.path.join(PROCESSED_DIR, 'source_features_selected.csv')
    OUTPUT_DIR = os.path.join(PROCESSED_DIR, 'task2_outputs_final')
    os.makedirs(OUTPUT_DIR, exist_ok=True)

    print("ğŸš€ æ­¥éª¤ 1: åŠ è½½æ•°æ®...")
    df_features = pd.read_csv(FEATURES_PATH)
    X_raw = df_features.drop(columns=['label', 'rpm', 'filename'])
    y_str = df_features['label']
    groups = df_features['filename']
    le = LabelEncoder()
    y = le.fit_transform(y_str)
    # --- ã€æ ¸å¿ƒä¿®æ­£ã€‘å®šä¹‰å…¨å±€å¸¸é‡ ---
    NUM_CLASSES = len(le.classes_)

    print(f"\nğŸš€ æ­¥éª¤ 2: å¼€å§‹ {N_SPLITS} æŠ˜åˆ†ç»„åˆ†å±‚äº¤å‰éªŒè¯...")
    sgkf = StratifiedGroupKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)

    # --- ä¿®æ”¹ï¼šåˆå§‹åŒ–æ‰€æœ‰æ¨¡å‹çš„é¢„æµ‹ç»“æœåˆ—è¡¨ ---
    all_y_test = []
    xgb_all_y_pred = []
    rf_all_y_pred = []
    svm_all_y_pred = [] # æ–°å¢
    lgb_all_y_pred = [] # æ–°å¢
    cnn_lstm_all_y_pred = []
    # --- ä¿®æ”¹ç»“æŸ ---

    for fold, (train_idx, test_idx) in enumerate(sgkf.split(X_raw, y, groups)):
        print(f"\n--- ç¬¬ {fold + 1}/{N_SPLITS} æŠ˜ ---")
        X_train_raw, X_test_raw = X_raw.iloc[train_idx], X_raw.iloc[test_idx]
        y_train, y_test = y[train_idx], y[test_idx]

        scaler = StandardScaler()
        X_train = scaler.fit_transform(X_train_raw)
        X_test = scaler.transform(X_test_raw)
        all_y_test.extend(y_test) # æ”¶é›†æµ‹è¯•æ ‡ç­¾

        # ======================================================================
        # XGBoostæ¨¡å‹ä¿æŒä¸å˜ï¼ˆä½ è¦æ±‚çš„ï¼‰
        # ======================================================================
        print("  - æ­£åœ¨è®­ç»ƒ XGBoost (å«æ—©åœè°ƒå‚)...")
        X_train_sub, X_val, y_train_sub, y_val = train_test_split(
            X_train, y_train, test_size=0.2, stratify=y_train, random_state=RANDOM_STATE
        )

        # è®¡ç®—æ ·æœ¬æƒé‡
        sample_weights_train = compute_balanced_sample_weights(y_train_sub)
        sample_weights_val = compute_balanced_sample_weights(y_val)

        # ä½¿ç”¨å°è£…çš„å‡½æ•°è®­ç»ƒXGBoostæ¨¡å‹
        bst = train_xgb_model_cv(
            X_train_sub, y_train_sub, X_val, y_val,
            sample_weights_train, sample_weights_val
        )

        print(f"    æœ€ä½³æ ‘æ•°é‡: {bst.best_iteration}")

        # ä½¿ç”¨å°è£…çš„å‡½æ•°è¿›è¡Œé¢„æµ‹
        y_pred_xgb, class_probs = predict_xgb_model(bst, X_test)
        xgb_all_y_pred.extend(y_pred_xgb)

        # ======================================================================
        # æ–°å¢ï¼šéšæœºæ£®æ—æ¨¡å‹
        # ======================================================================
        print("  - æ­£åœ¨è®­ç»ƒéšæœºæ£®æ—...")
        rf_model = RandomForestClassifier(
            n_estimators=200,
            max_depth=10,
            min_samples_split=5,
            min_samples_leaf=2,
            random_state=RANDOM_STATE,
            class_weight='balanced'
        )
        rf_model.fit(X_train, y_train)
        y_pred_rf = rf_model.predict(X_test)
        rf_all_y_pred.extend(y_pred_rf)

        # ======================================================================
        # æ–°å¢ï¼šSVMæ¨¡å‹
        # ======================================================================
        print("  - æ­£åœ¨è®­ç»ƒ SVM...")
        # SVM éœ€è¦ probability=True æ¥è·å–é¢„æµ‹æ¦‚ç‡
        svm_model = svm.SVC(
            kernel='rbf', # å¸¸ç”¨æ ¸å‡½æ•°
            C=1.0,        # æ­£åˆ™åŒ–å‚æ•°
            gamma='scale', # æ ¸å‡½æ•°ç³»æ•°
            random_state=RANDOM_STATE,
            class_weight='balanced', # å¤„ç†ç±»åˆ«ä¸å¹³è¡¡
            probability=True # å¯ç”¨æ¦‚ç‡é¢„æµ‹
        )
        svm_model.fit(X_train, y_train)
        y_pred_svm = svm_model.predict(X_test)
        svm_all_y_pred.extend(y_pred_svm) # æ”¶é›†SVMé¢„æµ‹

        # ======================================================================
        # æ–°å¢ï¼šLightGBMæ¨¡å‹
        # ======================================================================
        print("  - æ­£åœ¨è®­ç»ƒ LightGBM (å«æ—©åœ)...")
        # æ³¨æ„ï¼šLightGBM ä½¿ç”¨ validation set è¿›è¡Œæ—©åœ
        X_train_lgb_sub, X_val_lgb, y_train_lgb_sub, y_val_lgb = train_test_split(
            X_train, y_train, test_size=0.2, stratify=y_train, random_state=RANDOM_STATE
        )

        # è®¡ç®—æ ·æœ¬æƒé‡ (LightGBM ä¹Ÿå¯ä»¥é€šè¿‡å‚æ•°å¤„ç†ï¼Œè¿™é‡Œæ¼”ç¤ºæ‰‹åŠ¨è®¡ç®—)
        # sample_weights_train_lgb = compute_balanced_sample_weights(y_train_lgb_sub)
        # sample_weights_val_lgb = compute_balanced_sample_weights(y_val_lgb)

        # LightGBM å‚æ•° (å‚è€ƒ XGBoost)
        lgb_params = {
            'objective': 'multiclass',
            'num_class': NUM_CLASSES,
            'metric': 'multi_logloss',
            'boosting_type': 'gbdt',
            'num_leaves': 31, # æ§åˆ¶æ¨¡å‹å¤æ‚åº¦
            'learning_rate': 0.05,
            'feature_fraction': 0.9, # ç±»ä¼¼ colsample_bytree
            'bagging_fraction': 0.8, # ç±»ä¼¼ subsample
            'bagging_freq': 5,
            'verbose': -1, # å‡å°‘è¾“å‡º
            'random_state': RANDOM_STATE,
            # 'class_weight': 'balanced' # ä¹Ÿå¯ä»¥ç›´æ¥è®¾ç½®
        }

        # åˆ›å»º LightGBM Dataset
        train_data = lgb.Dataset(X_train_lgb_sub, label=y_train_lgb_sub) # , weight=sample_weights_train_lgb)
        val_data = lgb.Dataset(X_val_lgb, label=y_val_lgb, reference=train_data) # , weight=sample_weights_val_lgb)

        # è®­ç»ƒæ¨¡å‹
        lgb_model = lgb.train(
            lgb_params,
            train_data,
            valid_sets=[val_data],
            num_boost_round=500,
            callbacks=[
                lgb.early_stopping(stopping_rounds=30),
                lgb.log_evaluation(period=0) # ç¦æ­¢è®­ç»ƒæ—¥å¿—è¾“å‡º
            ]
        )

        # é¢„æµ‹
        y_pred_lgb_prob = lgb_model.predict(X_test, num_iteration=lgb_model.best_iteration)
        y_pred_lgb = np.argmax(y_pred_lgb_prob, axis=1)
        lgb_all_y_pred.extend(y_pred_lgb) # æ”¶é›†LightGBMé¢„æµ‹

        # ======================================================================
        # æ–°å¢ï¼šCNN+LSTMæ¨¡å‹
        # ======================================================================
        print("  - æ­£åœ¨è®­ç»ƒ CNN+LSTM (å«æ—©åœ)...")
        X_train_cnn, X_test_cnn = np.expand_dims(X_train, axis=2), np.expand_dims(X_test, axis=2)
        y_train_cat = to_categorical(y_train, num_classes=NUM_CLASSES)
        cnn_lstm_model = create_cnn_lstm_model(input_shape=(X_train_cnn.shape[1], 1), num_classes=NUM_CLASSES)
        early_stopping = EarlyStopping(monitor='val_accuracy', patience=20, restore_best_weights=True)

        # è®¡ç®—ç±»åˆ«æƒé‡ï¼ˆæ·»åŠ è¿™éƒ¨åˆ†ï¼‰
        class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)
        class_weight_dict = dict(enumerate(class_weights))

        # ç‰¹åˆ«åŠ å¼ºBç±»å’ŒNç±»çš„æƒé‡
        # å‡è®¾Bç±»ç´¢å¼•ä¸º0ï¼ŒNç±»ç´¢å¼•ä¸º2ï¼ˆéœ€è¦æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´ï¼‰
        b_class_index = 0  # æ ¹æ®ä½ çš„æ ‡ç­¾ç¼–ç è°ƒæ•´
        n_class_index = 2  # æ ¹æ®ä½ çš„æ ‡ç­¾ç¼–ç è°ƒæ•´

        if b_class_index in class_weight_dict:
            class_weight_dict[b_class_index] = class_weight_dict[b_class_index] * 2.0  # Bç±»æƒé‡åŠ å€
        if n_class_index in class_weight_dict:
            class_weight_dict[n_class_index] = class_weight_dict[n_class_index] * 1.5  # Nç±»æƒé‡å¢åŠ 50%

        cnn_lstm_model.fit(
            X_train_cnn, y_train_cat,
            epochs=100,  # å‡å°‘è®­ç»ƒè½®æ•°
            batch_size=64,  # å¢å¤§æ‰¹æ¬¡å¤§å°èŠ‚çœæ˜¾å­˜
            verbose=0,
            validation_split=0.2,
            class_weight=class_weight_dict,  # æ·»åŠ ç±»åˆ«æƒé‡
            callbacks=[early_stopping]
        )
        y_pred_cnn_lstm_prob = cnn_lstm_model.predict(X_test_cnn)
        y_pred_cnn_lstm = np.argmax(y_pred_cnn_lstm_prob, axis=1)
        cnn_lstm_all_y_pred.extend(y_pred_cnn_lstm)

    print("\nğŸš€ æ­¥éª¤ 3: äº¤å‰éªŒè¯å®Œæˆï¼Œæ±‡æ€»è¯„ä¼°ç»“æœ...")
    # --- ä¿®æ”¹ï¼šæ›´æ–°æ¨¡å‹ç»“æœå­—å…¸ ---
    models_results = {
        "XGBoost": xgb_all_y_pred,
        "RandomForest": rf_all_y_pred,
        "SVM": svm_all_y_pred, # æ–°å¢
        "LightGBM": lgb_all_y_pred, # æ–°å¢
        "CNN-LSTM": cnn_lstm_all_y_pred
    }
    # --- ä¿®æ”¹ç»“æŸ ---

    for model_name, y_pred in models_results.items():
        accuracy = accuracy_score(all_y_test, y_pred)
        report_dict = classification_report(all_y_test, y_pred, target_names=le.classes_, output_dict=True)
        print(f"\n========== {model_name} æœ€ç»ˆæ€§èƒ½è¯„ä¼° ==========")
        print(f"ğŸ“Š æ€»ä½“å‡†ç¡®ç‡ (5æŠ˜äº¤å‰éªŒè¯): {accuracy:.4f}")
        print("\nğŸ“Š åˆ†ç±»æŠ¥å‘Š:")
        print(classification_report(all_y_test, y_pred, target_names=le.classes_))

        print("\n--- å…³é”®é£é™©æŒ‡æ ‡åˆ†æ ---")
        ir_recall = report_dict.get('IR', {}).get('recall', 'N/A')
        b_precision = report_dict.get('B', {}).get('precision', 'N/A')
        print(f"  - å†…åœˆæ•…éšœ(IR)çš„å¬å›ç‡ (é¿å…æ¼åˆ¤): {ir_recall:.4f}")
        print(f"  - æ»šåŠ¨ä½“æ•…éšœ(B)çš„ç²¾ç¡®ç‡ (é¿å…è¯¯åˆ¤): {b_precision:.4f}")

        cm = confusion_matrix(all_y_test, y_pred)
        np.fill_diagonal(cm, 0)
        if np.max(cm) > 0:
            misclass_idx = np.unravel_index(np.argmax(cm), cm.shape)
            true_label, pred_label = le.classes_[misclass_idx[0]], le.classes_[misclass_idx[1]]
            print(f"  - æœ€ä¸»è¦çš„æ··æ·†: æ¨¡å‹å€¾å‘äºå°†çœŸå®çš„'{true_label}'ç±»åˆ«è¯¯åˆ¤ä¸º'{pred_label}'ç±»åˆ« ({np.max(cm)}æ¬¡)ã€‚")

        cm_full = confusion_matrix(all_y_test, y_pred)
        plt.figure(figsize=(10, 8))
        sns.heatmap(cm_full, annot=True, fmt='d', cmap='Blues', xticklabels=le.classes_, yticklabels=le.classes_)
        plt.title(f'{model_name} æ¨¡å‹æ··æ·†çŸ©é˜µ (5æŠ˜äº¤å‰éªŒè¯)', fontsize=16)
        plt.savefig(os.path.join(OUTPUT_DIR, f'ä»»åŠ¡äºŒ-{model_name}-æ··æ·†çŸ©é˜µ.png'), dpi=300)
        plt.close()
        print(f"âœ… {model_name} çš„æ··æ·†çŸ©é˜µå·²ä¿å­˜ã€‚")

    # --- æ–°å¢ï¼šç”Ÿæˆæ‰€æœ‰æ¨¡å‹çš„æ··æ·†çŸ©é˜µå’Œç²¾åº¦å¯¹æ¯”å›¾ ---
    print("\nğŸš€ æ­¥éª¤ 3.5: ç”Ÿæˆæ‰€æœ‰æ¨¡å‹çš„æ€§èƒ½å¯¹æ¯”å›¾è¡¨...")
    try:
        # 1. æ··æ·†çŸ©é˜µå¯¹æ¯” (å­å›¾)
        fig_cm, axes = plt.subplots(2, 3, figsize=(20, 12)) # å‡è®¾æœ€å¤š5ä¸ªæ¨¡å‹ï¼Œç”¨2x3å¸ƒå±€
        axes = axes.ravel()

        for idx, (model_name, y_pred) in enumerate(models_results.items()):
            if idx < len(axes):
                cm = confusion_matrix(all_y_test, y_pred)
                sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=le.classes_, yticklabels=le.classes_, ax=axes[idx])
                axes[idx].set_title(f'{model_name} æ··æ·†çŸ©é˜µ', fontsize=14)
                axes[idx].set_xlabel('Predicted Label')
                axes[idx].set_ylabel('True Label')

        # éšè—å¤šä½™çš„å­å›¾
        for j in range(len(models_results), len(axes)):
            axes[j].set_visible(False)

        plt.tight_layout()
        save_path_cm_all = os.path.join(OUTPUT_DIR, 'ä»»åŠ¡äºŒ-æ‰€æœ‰æ¨¡å‹æ··æ·†çŸ©é˜µå¯¹æ¯”.png')
        plt.savefig(save_path_cm_all, dpi=300, bbox_inches='tight')
        plt.close()
        print(f"âœ… æ‰€æœ‰æ¨¡å‹æ··æ·†çŸ©é˜µå¯¹æ¯”å›¾å·²ä¿å­˜è‡³: {save_path_cm_all}")

        # 2. ç²¾åº¦/æŒ‡æ ‡å¯¹æ¯” (æ¡å½¢å›¾)
        comparison_data = []
        for model_name, y_pred in models_results.items():
            accuracy = accuracy_score(all_y_test, y_pred)
            report_dict = classification_report(all_y_test, y_pred, target_names=le.classes_, output_dict=True)
            macro_f1 = report_dict['macro avg']['f1-score']
            weighted_f1 = report_dict['weighted avg']['f1-score']
            ir_recall = report_dict.get('IR', {}).get('recall', 0)
            b_precision = report_dict.get('B', {}).get('precision', 0)
            n_recall = report_dict.get('N', {}).get('recall', 0) # å‡è®¾ N ç±»å­˜åœ¨

            comparison_data.append({
                'Model': model_name,
                'Accuracy': accuracy,
                'Macro F1': macro_f1,
                'Weighted F1': weighted_f1,
                'IR Recall': ir_recall,
                'B Precision': b_precision,
                'N Recall': n_recall
            })

        df_comparison = pd.DataFrame(comparison_data)
        # melted_df = df_comparison.melt(id_vars=['Model'], var_name='Metric', value_name='Score')

        fig_metrics, ax = plt.subplots(figsize=(12, 8))
        # x_pos = np.arange(len(df_comparison))
        # width = 0.15 # æ¯ä¸ªæŒ‡æ ‡æ¡å½¢çš„å®½åº¦
        # metrics_to_plot = ['Accuracy', 'Macro F1', 'Weighted F1', 'IR Recall', 'B Precision']
        # for i, metric in enumerate(metrics_to_plot):
        #     ax.bar(x_pos + i*width, df_comparison[metric], width, label=metric)
        # ax.set_xlabel('Models')
        # ax.set_ylabel('Score')
        # ax.set_title('æ¨¡å‹æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”')
        # ax.set_xticks(x_pos + width * (len(metrics_to_plot)-1) / 2)
        # ax.set_xticklabels(df_comparison['Model'])
        # ax.legend()
        # plt.xticks(rotation=45)

        # ä½¿ç”¨ Seaborn æ›´ç®€æ´åœ°ç»˜åˆ¶
        df_melted = df_comparison.melt(id_vars=['Model'], value_vars=['Accuracy', 'Macro F1', 'Weighted F1', 'IR Recall', 'B Precision', 'N Recall'],
                                     var_name='Metric', value_name='Score')
        sns.barplot(x='Model', y='Score', hue='Metric', data=df_melted, ax=ax)
        ax.set_title('æ¨¡å‹æ€§èƒ½æŒ‡æ ‡å¯¹æ¯” (5æŠ˜äº¤å‰éªŒè¯)', fontsize=16)
        ax.set_ylabel('Score')
        ax.set_xlabel('Model')
        ax.legend(title='Metric')
        plt.xticks(rotation=45)
        plt.tight_layout()

        save_path_metrics = os.path.join(OUTPUT_DIR, 'ä»»åŠ¡äºŒ-æ¨¡å‹æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”.png')
        plt.savefig(save_path_metrics, dpi=300, bbox_inches='tight')
        plt.close()
        print(f"âœ… æ¨¡å‹æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”å›¾å·²ä¿å­˜è‡³: {save_path_metrics}")

    except Exception as e:
        print(f"âš ï¸ ç”Ÿæˆæ€§èƒ½å¯¹æ¯”å›¾è¡¨æ—¶å‡ºé”™: {e}")
    # --- æ–°å¢ç»“æŸ ---


    print("\nğŸš€ æ­¥éª¤ 4: æ­£åœ¨ä½¿ç”¨å…¨éƒ¨æºåŸŸæ•°æ®è®­ç»ƒæœ€ç»ˆæ¨¡å‹ä»¥ç”¨äºä»»åŠ¡ä¸‰...")
    final_scaler = StandardScaler().fit(X_raw)
    X_scaled_full = final_scaler.transform(X_raw)

    # ======================================================================
    # XGBoostæœ€ç»ˆæ¨¡å‹ä¿æŒä¸å˜
    # ======================================================================
    print("  - æ­£åœ¨è®­ç»ƒæœ€ç»ˆçš„XGBoostæ¨¡å‹...")
    # è®¡ç®—æ ·æœ¬æƒé‡ï¼Œé‡ç‚¹æå‡Nç±»æƒé‡
    sample_weights_full = compute_balanced_sample_weights(y)

    final_xgb_model = xgb.XGBClassifier(
        objective='multi:softprob',
        eval_metric='mlogloss',
        random_state=RANDOM_STATE,
        n_estimators=500,  # å¢åŠ æ ‘æ•°é‡
        max_depth=6,  # é™ä½æ·±åº¦é¿å…è¿‡æ‹Ÿåˆ
        learning_rate=0.1,  # åˆé€‚çš„å­¦ä¹ ç‡
        subsample=0.8,
        colsample_bytree=0.8,
        min_child_weight=3,
        gamma=0.2,
    )
    final_xgb_model.fit(X_scaled_full, y, sample_weight=sample_weights_full)

    # ======================================================================
    # æ–°å¢ï¼šéšæœºæ£®æ—æœ€ç»ˆæ¨¡å‹
    # ======================================================================
    print("  - æ­£åœ¨è®­ç»ƒæœ€ç»ˆçš„éšæœºæ£®æ—æ¨¡å‹...")
    final_rf_model = RandomForestClassifier(
        n_estimators=300,
        max_depth=12,
        min_samples_split=5,
        min_samples_leaf=2,
        random_state=RANDOM_STATE,
        class_weight='balanced'
    )
    final_rf_model.fit(X_scaled_full, y)

    # ======================================================================
    # æ–°å¢ï¼šSVMæœ€ç»ˆæ¨¡å‹
    # ======================================================================
    print("  - æ­£åœ¨è®­ç»ƒæœ€ç»ˆçš„SVMæ¨¡å‹...")
    final_svm_model = svm.SVC(
        kernel='rbf',
        C=1.0,
        gamma='scale',
        random_state=RANDOM_STATE,
        class_weight='balanced',
        probability=True # ç¡®ä¿å¯ä»¥è·å–æ¦‚ç‡
    )
    final_svm_model.fit(X_scaled_full, y) # ä½¿ç”¨å…¨éƒ¨æ•°æ®è®­ç»ƒ

    # ======================================================================
    # æ–°å¢ï¼šLightGBMæœ€ç»ˆæ¨¡å‹
    # ======================================================================
    print("  - æ­£åœ¨è®­ç»ƒæœ€ç»ˆçš„LightGBMæ¨¡å‹...")
    # LightGBM å‚æ•° (å¯ä»¥å¾®è°ƒ)
    final_lgb_params = {
        'objective': 'multiclass',
        'num_class': NUM_CLASSES,
        'metric': 'multi_logloss',
        'boosting_type': 'gbdt',
        'num_leaves': 31,
        'learning_rate': 0.05,
        'feature_fraction': 0.9,
        'bagging_fraction': 0.8,
        'bagging_freq': 5,
        'verbose': -1,
        'random_state': RANDOM_STATE,
        # 'class_weight': 'balanced' # å¯é€‰
    }

    # åˆ›å»ºæœ€ç»ˆè®­ç»ƒæ•°æ®é›†
    final_train_data = lgb.Dataset(X_scaled_full, label=y) # , weight=sample_weights_full) # å¦‚æœéœ€è¦æƒé‡

    # è®­ç»ƒæœ€ç»ˆæ¨¡å‹ (ä¸ä½¿ç”¨æ—©åœï¼Œæˆ–ä½¿ç”¨å…¨éƒ¨æ•°æ®ä½œä¸ºéªŒè¯é›†è¿›è¡Œå°‘é‡è½®æ¬¡)
    final_lgb_model = lgb.train(
        final_lgb_params,
        final_train_data,
        num_boost_round=100 # æ ¹æ® CV ç»“æœè°ƒæ•´
    )


    # ======================================================================
    # æ–°å¢ï¼šCNN+LSTMæœ€ç»ˆæ¨¡å‹
    # ======================================================================
    print("  - æ­£åœ¨è®­ç»ƒæœ€ç»ˆçš„CNN+LSTMæ¨¡å‹...")
    X_cnn_full = np.expand_dims(X_scaled_full, axis=2)
    y_cat_full = to_categorical(y, num_classes=NUM_CLASSES)
    final_cnn_lstm_model = create_cnn_lstm_model(input_shape=(X_cnn_full.shape[1], 1), num_classes=NUM_CLASSES)
    # --- ã€æ ¸å¿ƒä¿®æ­£ã€‘ä¸ºæœ€ç»ˆæ¨¡å‹è®­ç»ƒæ·»åŠ æ—©åœï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ ---
    early_stopping_final = EarlyStopping(monitor='accuracy', patience=20, restore_best_weights=True)

    # ä¸ºæœ€ç»ˆæ¨¡å‹ä¹Ÿæ·»åŠ ç±»åˆ«æƒé‡ï¼ˆæ·»åŠ è¿™éƒ¨åˆ†ï¼‰
    class_weights_final = compute_class_weight('balanced', classes=np.unique(y), y=y)
    class_weight_dict_final = dict(enumerate(class_weights_final))

    # ç‰¹åˆ«åŠ å¼ºBç±»å’ŒNç±»çš„æƒé‡
    b_class_index = 0  # æ ¹æ®ä½ çš„æ ‡ç­¾ç¼–ç è°ƒæ•´
    n_class_index = 2  # æ ¹æ®ä½ çš„æ ‡ç­¾ç¼–ç è°ƒæ•´

    if b_class_index in class_weight_dict_final:
        class_weight_dict_final[b_class_index] = class_weight_dict_final[b_class_index] * 3.0
    if n_class_index in class_weight_dict_final:
        class_weight_dict_final[n_class_index] = class_weight_dict_final[n_class_index] * 1.5

    final_cnn_lstm_model.fit(X_cnn_full, y_cat_full, epochs=100, batch_size=64, verbose=0,
                             class_weight=class_weight_dict_final,  # æ·»åŠ ç±»åˆ«æƒé‡
                             callbacks=[early_stopping_final])

    print("\nğŸš€ æ­¥éª¤ 5: æ­£åœ¨ä¿å­˜æœ€ç»ˆäº§å‡º...")
    joblib.dump(final_xgb_model, os.path.join(OUTPUT_DIR, 'final_xgb_model.joblib'))
    joblib.dump(final_rf_model, os.path.join(OUTPUT_DIR, 'final_rf_model.joblib'))  # æ–°å¢
    # --- æ–°å¢ï¼šä¿å­˜ SVM å’Œ LightGBM æ¨¡å‹ ---
    joblib.dump(final_svm_model, os.path.join(OUTPUT_DIR, 'final_svm_model.joblib')) # ä¿å­˜SVM
    # LightGBM Booster å¯¹è±¡éœ€è¦ç‰¹æ®Šä¿å­˜
    final_lgb_model.save_model(os.path.join(OUTPUT_DIR, 'final_lgb_model.txt')) # ä¿å­˜LightGBM
    # --- æ–°å¢ç»“æŸ ---
    joblib.dump(final_scaler, os.path.join(OUTPUT_DIR, 'final_scaler.joblib'))
    joblib.dump(le, os.path.join(OUTPUT_DIR, 'final_label_encoder.joblib'))
    # --- ã€æ ¸å¿ƒä¿®æ­£ã€‘å°† .keras æ”¹ä¸º .h5 ä»¥ç¡®ä¿å…¼å®¹æ€§ ---
    # ä¿®æ”¹æ–‡ä»¶æ‰©å±•åä¸º .weights.h5
    final_cnn_lstm_model.save_weights(os.path.join(OUTPUT_DIR, 'final_cnn_lstm_model.weights.h5'))  # ä¿®æ”¹ä¸ºCNN+LSTM

    print(f"âœ… æœ€ç»ˆæ¨¡å‹åŠé¢„å¤„ç†å™¨å·²ä¿å­˜è‡³: {os.path.abspath(OUTPUT_DIR)}")
    print("\nğŸ‰ ä»»åŠ¡äºŒï¼šæºåŸŸæ•…éšœè¯Šæ–­ï¼ˆæœ€ç»ˆç‰ˆï¼‰å…¨éƒ¨å·¥ä½œå·²åœ†æ»¡å®Œæˆï¼")
