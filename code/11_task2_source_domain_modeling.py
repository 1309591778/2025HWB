# 11_task2_source_domain_modeling.py
import os
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from matplotlib import font_manager
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score
from sklearn.ensemble import RandomForestClassifier
import joblib
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, Dropout, Lambda
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.utils.class_weight import compute_class_weight
import xgboost as xgb
from sklearn.model_selection import StratifiedGroupKFold, train_test_split


# ==============================================================================
# 0. è¾…åŠ©å‡½æ•°
# ==============================================================================
def set_chinese_font():
    """è®¾ç½®ä¸­æ–‡å­—ä½“"""
    plt.rcParams['font.sans-serif'] = ['Microsoft YaHei']
    plt.rcParams['axes.unicode_minus'] = False
    print("âœ… å·²è®¾ç½®ä¸­æ–‡å­—ä½“ã€‚")


# --- æ–°å¢ï¼šå®šä¹‰æ¢¯åº¦åè½¬å±‚ (Gradient Reversal Layer) ---
@tf.custom_gradient
def grad_reverse(x, lambda_val=1.0):
    """æ¢¯åº¦åè½¬å‡½æ•°"""
    y = tf.identity(x)

    def custom_grad(dy):
        return -dy * lambda_val, None

    return y, custom_grad


class GradReverse(tf.keras.layers.Layer):
    """æ¢¯åº¦åè½¬å±‚ Keras å°è£…"""

    def __init__(self, lambda_val=1.0, **kwargs):
        super(GradReverse, self).__init__(**kwargs)
        self.lambda_val = lambda_val

    def call(self, x):
        return grad_reverse(x, self.lambda_val)

    def get_config(self):
        config = super(GradReverse, self).get_config()
        config.update({'lambda_val': self.lambda_val})
        return config


# --- æ–°å¢ç»“æŸ ---


# --- ä¿®æ”¹ï¼šå®šä¹‰åŒ…å«é¢†åŸŸè‡ªé€‚åº”çš„ç®€åŒ– MLP æ¨¡å‹ ---
def create_mlp_da_model(input_dim, num_classes, lambda_grl=1.0):
    """
    åˆ›å»ºç”¨äºæºåŸŸè®­ç»ƒçš„ç®€åŒ– MLP æ¨¡å‹ï¼ŒåŒ…å«é¢†åŸŸè‡ªé€‚åº”ç»„ä»¶ã€‚
    """
    # 1. è¾“å…¥å±‚
    inputs = Input(shape=(input_dim,))

    # 2. ç‰¹å¾æå–å™¨ (MLP)
    shared = Dense(128, activation='relu', name='feature_extractor_1')(inputs)
    shared = Dropout(0.5)(shared)
    shared = Dense(64, activation='relu', name='feature_extractor_2')(shared)
    shared = Dropout(0.5)(shared)
    features_before_grl = Dense(32, activation='relu', name='feature_extractor_3')(shared)

    # --- æ–°å¢ï¼šé¢†åŸŸè‡ªé€‚åº”åˆ†æ”¯ ---
    # 3a. æ¢¯åº¦åè½¬å±‚ (GRL)
    grl = GradReverse(lambda_val=lambda_grl)(features_before_grl)

    # 3b. é¢†åŸŸåˆ¤åˆ«å™¨ (Domain Discriminator)
    d_net = Dense(32, activation='relu')(grl)
    d_net = Dropout(0.5)(d_net)
    domain_output = Dense(1, activation='sigmoid', name='domain_output')(d_net)
    # --- æ–°å¢ç»“æŸ ---

    # 4. ä¸»ä»»åŠ¡åˆ†ç±»å¤´
    c_net = Dense(64, activation='relu')(features_before_grl)
    c_net = Dropout(0.5)(c_net)
    class_output = Dense(num_classes, activation='softmax', name='class_output')(c_net)

    # 5. æ„å»ºæ¨¡å‹
    model = Model(inputs=inputs, outputs=[class_output, domain_output])

    # ç¼–è¯‘æ¨¡å‹
    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),
        loss={
            'class_output': 'categorical_crossentropy',
            'domain_output': 'binary_crossentropy'
        },
        loss_weights={
            'class_output': 1.0,
            'domain_output': 1.0  # å¯ä»¥è°ƒæ•´è¿™ä¸ªæƒé‡ lambda_domain
        },
        metrics={
            'class_output': 'accuracy',
            'domain_output': 'accuracy'
        }
    )

    return model


# --- ä¿®æ”¹ç»“æŸ ---


def compute_balanced_sample_weights(y, n_class_index=2, n_class_weight=3.0):
    """è®¡ç®—å¹³è¡¡çš„æ ·æœ¬æƒé‡ï¼Œç»™Nç±»æ›´é«˜çš„æƒé‡"""
    n_classes = len(np.unique(y))
    class_weights = {i: 1.0 for i in range(n_classes)}
    class_weights[n_class_index] = n_class_weight
    sample_weights = np.array([class_weights[label] for label in y])
    return sample_weights


# ==============================================================================
# æ–°å¢ï¼šXGBoostæ¨¡å‹ç›¸å…³å‡½æ•°
# ==============================================================================
def create_xgb_model_params():
    """åˆ›å»ºXGBoostæ¨¡å‹å‚æ•°"""
    params = {
        'objective': 'multi:softprob',
        'eval_metric': 'mlogloss',
        'num_class': 4,
        'random_state': 42,
        'max_depth': 6,
        'learning_rate': 0.1,
        'subsample': 0.8,
        'colsample_bytree': 0.8,
        'min_child_weight': 3,
        'gamma': 0.2,
    }
    return params


def train_xgb_model_cv(X_train, y_train, X_val, y_val, sample_weights_train=None, sample_weights_val=None):
    """è®­ç»ƒXGBoostæ¨¡å‹ï¼ˆäº¤å‰éªŒè¯ç”¨ï¼‰"""
    if sample_weights_train is not None and sample_weights_val is not None:
        dtrain = xgb.DMatrix(X_train, label=y_train, weight=sample_weights_train)
        dval = xgb.DMatrix(X_val, label=y_val, weight=sample_weights_val)
    else:
        dtrain = xgb.DMatrix(X_train, label=y_train)
        dval = xgb.DMatrix(X_val, label=y_val)

    params = create_xgb_model_params()
    params['num_class'] = len(np.unique(y_train))

    evals_result = {}
    bst = xgb.train(
        params,
        dtrain,
        num_boost_round=500,
        evals=[(dtrain, 'train'), (dval, 'val')],
        early_stopping_rounds=30,
        evals_result=evals_result,
        verbose_eval=False
    )

    return bst


def predict_xgb_model(model, X_test):
    """ä½¿ç”¨XGBoostæ¨¡å‹è¿›è¡Œé¢„æµ‹"""
    dtest = xgb.DMatrix(X_test)
    class_probs = model.predict(dtest)
    y_pred = np.argmax(class_probs, axis=1)
    return y_pred, class_probs


# ==============================================================================
# 1. ä¸»ç¨‹åº
# ==============================================================================
if __name__ == "__main__":
    set_chinese_font()

    RANDOM_STATE = 42
    N_SPLITS = 5
    PROCESSED_DIR = os.path.join('..', 'data', 'processed')
    FEATURES_PATH = os.path.join(PROCESSED_DIR, 'source_features_selected.csv')
    OUTPUT_DIR = os.path.join(PROCESSED_DIR, 'task2_outputs_final')
    os.makedirs(OUTPUT_DIR, exist_ok=True)

    print("ğŸš€ æ­¥éª¤ 1: åŠ è½½æ•°æ®...")
    df_features = pd.read_csv(FEATURES_PATH)
    X_raw = df_features.drop(columns=['label', 'rpm', 'filename'])
    y_str = df_features['label']
    groups = df_features['filename']
    le = LabelEncoder()
    y = le.fit_transform(y_str)
    NUM_CLASSES = len(le.classes_)
    input_dim = X_raw.shape[1]  # è·å–ç‰¹å¾ç»´åº¦

    print(f"\nğŸš€ æ­¥éª¤ 2: å¼€å§‹ {N_SPLITS} æŠ˜åˆ†ç»„åˆ†å±‚äº¤å‰éªŒè¯...")
    sgkf = StratifiedGroupKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)

    all_y_test, xgb_all_y_pred, rf_all_y_pred, mlp_da_all_y_pred = [], [], [], []

    for fold, (train_idx, test_idx) in enumerate(sgkf.split(X_raw, y, groups)):
        print(f"\n--- ç¬¬ {fold + 1}/{N_SPLITS} æŠ˜ ---")
        X_train_raw, X_test_raw = X_raw.iloc[train_idx], X_raw.iloc[test_idx]
        y_train, y_test = y[train_idx], y[test_idx]

        scaler = StandardScaler()
        X_train = scaler.fit_transform(X_train_raw)
        X_test = scaler.transform(X_test_raw)
        all_y_test.extend(y_test)

        # ======================================================================
        # XGBoostæ¨¡å‹ä¿æŒä¸å˜
        # ======================================================================
        print("  - æ­£åœ¨è®­ç»ƒ XGBoost (å«æ—©åœè°ƒå‚)...")
        X_train_sub, X_val, y_train_sub, y_val = train_test_split(
            X_train, y_train, test_size=0.2, stratify=y_train, random_state=RANDOM_STATE
        )

        sample_weights_train = compute_balanced_sample_weights(y_train_sub)
        sample_weights_val = compute_balanced_sample_weights(y_val)

        bst = train_xgb_model_cv(
            X_train_sub, y_train_sub, X_val, y_val,
            sample_weights_train, sample_weights_val
        )

        print(f"    æœ€ä½³æ ‘æ•°é‡: {bst.best_iteration}")

        y_pred_xgb, class_probs = predict_xgb_model(bst, X_test)
        xgb_all_y_pred.extend(y_pred_xgb)

        # ======================================================================
        # éšæœºæ£®æ—æ¨¡å‹ä¿æŒä¸å˜
        # ======================================================================
        print("  - æ­£åœ¨è®­ç»ƒéšæœºæ£®æ—...")
        rf_model = RandomForestClassifier(
            n_estimators=200,
            max_depth=10,
            min_samples_split=5,
            min_samples_leaf=2,
            random_state=RANDOM_STATE,
            class_weight='balanced'
        )
        rf_model.fit(X_train, y_train)
        y_pred_rf = rf_model.predict(X_test)
        rf_all_y_pred.extend(y_pred_rf)

        # ======================================================================
        # ä¿®æ”¹ï¼šç®€åŒ– MLP + DA æ¨¡å‹ (æ›¿ä»£ CNN-LSTM)
        # ======================================================================
        print("  - æ­£åœ¨è®­ç»ƒ ç®€åŒ– MLP + DA æ¨¡å‹ (å«æ—©åœ)...")
        y_train_cat = to_categorical(y_train, num_classes=NUM_CLASSES)
        # æ³¨æ„ï¼šè¿™é‡Œä¸å†éœ€è¦ expand_dimsï¼Œç›´æ¥ä½¿ç”¨ (batch_size, features)
        # åˆ›å»ºæ¨¡å‹
        mlp_da_model = create_mlp_da_model(input_dim=input_dim, num_classes=NUM_CLASSES, lambda_grl=1.0)
        early_stopping = EarlyStopping(monitor='val_class_output_accuracy', mode='max', patience=20,
                                       restore_best_weights=True)

        # å‡†å¤‡ä¼ªé¢†åŸŸæ ‡ç­¾ (åœ¨æºåŸŸè®­ç»ƒä¸­ï¼Œæ‰€æœ‰æ ·æœ¬éƒ½æ ‡è®°ä¸ºæºåŸŸ 0)
        domain_labels_source_train = np.zeros((X_train.shape[0], 1))
        domain_labels_source_val = np.zeros((X_train_sub.shape[0], 1))  # ç”¨äºéªŒè¯é›†

        # è®­ç»ƒæ¨¡å‹ (ç®€åŒ–å¤„ç†ï¼Œå®é™…åº”ç”¨ä¸­å¯ä½¿ç”¨è‡ªå®šä¹‰å¾ªç¯è¿›è¡Œæ›´ç²¾ç¡®çš„DA)
        # è¿™é‡Œæˆ‘ä»¬ç®€åŒ–å¤„ç†ï¼Œåªåœ¨è®­ç»ƒé›†ä¸Šè®­ç»ƒä¸»ä»»åŠ¡å’Œé¢†åŸŸä»»åŠ¡
        # éªŒè¯é›†ä¹Ÿæ ‡è®°ä¸ºæºåŸŸ
        X_train_sub_scaled = scaler.transform(X_train_sub)  # éªŒè¯é›†ä¹Ÿéœ€è¦æ ‡å‡†åŒ–
        history = mlp_da_model.fit(
            X_train,
            {"class_output": y_train_cat, "domain_output": domain_labels_source_train},
            epochs=100,
            batch_size=64,
            verbose=0,
            validation_data=(
                X_train_sub_scaled,
                {"class_output": to_categorical(y_train_sub, num_classes=NUM_CLASSES),
                 "domain_output": domain_labels_source_val}
            ),
            callbacks=[early_stopping]
        )
        print(f"    è®­ç»ƒå®Œæˆï¼Œæœ€ä½³Epoch: {len(history.history['loss']) - early_stopping.patience}")

        # é¢„æµ‹ (åªä½¿ç”¨åˆ†ç±»è¾“å‡º)
        y_pred_mlp_da_prob, _ = mlp_da_model.predict(X_test)
        y_pred_mlp_da = np.argmax(y_pred_mlp_da_prob, axis=1)
        mlp_da_all_y_pred.extend(y_pred_mlp_da)

    print("\nğŸš€ æ­¥éª¤ 3: äº¤å‰éªŒè¯å®Œæˆï¼Œæ±‡æ€»è¯„ä¼°ç»“æœ...")
    # æ›´æ–°æ¨¡å‹ç»“æœå­—å…¸
    models_results = {
        "XGBoost": xgb_all_y_pred,
        "RandomForest": rf_all_y_pred,
        "MLP-DA": mlp_da_all_y_pred  # æ›´æ–°é”®å
    }

    for model_name, y_pred in models_results.items():
        accuracy = accuracy_score(all_y_test, y_pred)
        report_dict = classification_report(all_y_test, y_pred, target_names=le.classes_, output_dict=True)
        print(f"\n========== {model_name} æœ€ç»ˆæ€§èƒ½è¯„ä¼° ==========")
        print(f"ğŸ“Š æ€»ä½“å‡†ç¡®ç‡ (5æŠ˜äº¤å‰éªŒè¯): {accuracy:.4f}")
        print("\nğŸ“Š åˆ†ç±»æŠ¥å‘Š:")
        print(classification_report(all_y_test, y_pred, target_names=le.classes_))

        print("\n--- å…³é”®é£é™©æŒ‡æ ‡åˆ†æ ---")
        ir_recall = report_dict.get('IR', {}).get('recall', 'N/A')
        b_precision = report_dict.get('B', {}).get('precision', 'N/A')
        print(f"  - å†…åœˆæ•…éšœ(IR)çš„å¬å›ç‡ (é¿å…æ¼åˆ¤): {ir_recall:.4f}")
        print(f"  - æ»šåŠ¨ä½“æ•…éšœ(B)çš„ç²¾ç¡®ç‡ (é¿å…è¯¯åˆ¤): {b_precision:.4f}")

        cm = confusion_matrix(all_y_test, y_pred)
        np.fill_diagonal(cm, 0)
        if np.max(cm) > 0:
            misclass_idx = np.unravel_index(np.argmax(cm), cm.shape)
            true_label, pred_label = le.classes_[misclass_idx[0]], le.classes_[misclass_idx[1]]
            print(f"  - æœ€ä¸»è¦çš„æ··æ·†: æ¨¡å‹å€¾å‘äºå°†çœŸå®çš„'{true_label}'ç±»åˆ«è¯¯åˆ¤ä¸º'{pred_label}'ç±»åˆ« ({np.max(cm)}æ¬¡)ã€‚")

        cm_full = confusion_matrix(all_y_test, y_pred)
        plt.figure(figsize=(10, 8))
        sns.heatmap(cm_full, annot=True, fmt='d', cmap='Blues', xticklabels=le.classes_, yticklabels=le.classes_)
        plt.title(f'{model_name} æ¨¡å‹æ··æ·†çŸ©é˜µ (5æŠ˜äº¤å‰éªŒè¯)', fontsize=16)
        plt.savefig(os.path.join(OUTPUT_DIR, f'ä»»åŠ¡äºŒ-{model_name}-æ··æ·†çŸ©é˜µ.png'), dpi=300)
        plt.close()
        print(f"âœ… {model_name} çš„æ··æ·†çŸ©é˜µå·²ä¿å­˜ã€‚")

    print("\nğŸš€ æ­¥éª¤ 4: æ­£åœ¨ä½¿ç”¨å…¨éƒ¨æºåŸŸæ•°æ®è®­ç»ƒæœ€ç»ˆæ¨¡å‹ä»¥ç”¨äºä»»åŠ¡ä¸‰...")
    final_scaler = StandardScaler().fit(X_raw)
    X_scaled_full = final_scaler.transform(X_raw)

    # ======================================================================
    # XGBoostæœ€ç»ˆæ¨¡å‹ä¿æŒä¸å˜
    # ======================================================================
    print("  - æ­£åœ¨è®­ç»ƒæœ€ç»ˆçš„XGBoostæ¨¡å‹...")
    sample_weights_full = compute_balanced_sample_weights(y)

    final_xgb_model = xgb.XGBClassifier(
        objective='multi:softprob',
        eval_metric='mlogloss',
        random_state=RANDOM_STATE,
        n_estimators=500,
        max_depth=6,
        learning_rate=0.1,
        subsample=0.8,
        colsample_bytree=0.8,
        min_child_weight=3,
        gamma=0.2,
    )
    final_xgb_model.fit(X_scaled_full, y, sample_weight=sample_weights_full)

    # ======================================================================
    # éšæœºæ£®æ—æœ€ç»ˆæ¨¡å‹ä¿æŒä¸å˜
    # ======================================================================
    print("  - æ­£åœ¨è®­ç»ƒæœ€ç»ˆçš„éšæœºæ£®æ—æ¨¡å‹...")
    final_rf_model = RandomForestClassifier(
        n_estimators=300,
        max_depth=12,
        min_samples_split=5,
        min_samples_leaf=2,
        random_state=RANDOM_STATE,
        class_weight='balanced'
    )
    final_rf_model.fit(X_scaled_full, y)

    # ======================================================================
    # ä¿®æ”¹ï¼šè®­ç»ƒæœ€ç»ˆçš„ MLP + DA æ¨¡å‹ (æ›¿ä»£ CNN-LSTM)
    # ======================================================================
    print("  - æ­£åœ¨è®­ç»ƒæœ€ç»ˆçš„ MLP + DA æ¨¡å‹...")
    y_cat_full = to_categorical(y, num_classes=NUM_CLASSES)
    # åˆ›å»ºæœ€ç»ˆæ¨¡å‹
    final_mlp_da_model = create_mlp_da_model(input_dim=input_dim, num_classes=NUM_CLASSES, lambda_grl=1.0)
    early_stopping_final = EarlyStopping(monitor='class_output_accuracy', mode='max', patience=20,
                                         restore_best_weights=True)

    # å‡†å¤‡ä¼ªé¢†åŸŸæ ‡ç­¾
    domain_labels_full = np.zeros((X_scaled_full.shape[0], 1))

    # è®­ç»ƒæœ€ç»ˆæ¨¡å‹
    final_mlp_da_model.fit(
        X_scaled_full,
        {"class_output": y_cat_full, "domain_output": domain_labels_full},
        epochs=100,
        batch_size=64,
        verbose=0,
        callbacks=[early_stopping_final]
    )

    print("\nğŸš€ æ­¥éª¤ 5: æ­£åœ¨ä¿å­˜æœ€ç»ˆäº§å‡º...")
    joblib.dump(final_xgb_model, os.path.join(OUTPUT_DIR, 'final_xgb_model.joblib'))
    joblib.dump(final_rf_model, os.path.join(OUTPUT_DIR, 'final_rf_model.joblib'))
    joblib.dump(final_scaler, os.path.join(OUTPUT_DIR, 'final_scaler.joblib'))
    joblib.dump(le, os.path.join(OUTPUT_DIR, 'final_label_encoder.joblib'))

    # --- ä¿®æ”¹ï¼šä¿å­˜ MLP + DA æ¨¡å‹çš„æƒé‡ ---
    # ä¿å­˜æƒé‡ï¼Œä½¿ç”¨ .weights.h5 æ‰©å±•å
    final_mlp_da_model.save_weights(os.path.join(OUTPUT_DIR, 'final_mlp_da_model.weights.h5'))
    print("âœ… MLP-DA æ¨¡å‹æƒé‡å·²ä¿å­˜ã€‚")
    # --- ä¿®æ”¹ç»“æŸ ---

    print(f"âœ… æœ€ç»ˆæ¨¡å‹åŠé¢„å¤„ç†å™¨å·²ä¿å­˜è‡³: {os.path.abspath(OUTPUT_DIR)}")
    print("\nğŸ‰ ä»»åŠ¡äºŒï¼šæºåŸŸæ•…éšœè¯Šæ–­ï¼ˆæœ€ç»ˆç‰ˆï¼‰å…¨éƒ¨å·¥ä½œå·²åœ†æ»¡å®Œæˆï¼")
