import os
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from matplotlib import font_manager
import xgboost as xgb
# ã€æ ¸å¿ƒã€‘å¯¼å…¥GroupShuffleSplitï¼Œä¸å†ä½¿ç”¨train_test_split
from sklearn.model_selection import GroupShuffleSplit
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.utils.class_weight import compute_sample_weight
import joblib


# (å­—ä½“è®¾ç½®å‡½æ•° set_chinese_font ä¿æŒä¸å˜)
def set_chinese_font():
    font_path = os.path.join(os.path.dirname(__file__), 'fonts', 'SourceHanSansSC-Regular.otf')
    if os.path.exists(font_path):
        font_prop = font_manager.FontProperties(fname=font_path)
        plt.rcParams['font.family'] = font_prop.get_name()
    else:
        plt.rcParams['font.sans-serif'] = ['Microsoft YaHei']
    plt.rcParams['axes.unicode_minus'] = False
    print("âœ… å·²è®¾ç½®ä¸­æ–‡å­—ä½“ã€‚")


if __name__ == "__main__":
    set_chinese_font()

    PROCESSED_DIR = os.path.join('..', 'data', 'processed')
    # ã€æ³¨æ„ã€‘ç¡®ä¿ä½¿ç”¨çš„æ˜¯ç»è¿‡ç‰¹å¾ç­›é€‰åçš„æœ€ä¼˜ç‰¹å¾é›†
    FEATURES_PATH = os.path.join(PROCESSED_DIR, 'source_features_selected.csv')
    OUTPUT_DIR = os.path.join(PROCESSED_DIR, 'task2_outputs')
    os.makedirs(OUTPUT_DIR, exist_ok=True)

    print("ğŸš€ æ­¥éª¤ 1: åŠ è½½æ•°æ®...")
    try:
        df_features = pd.read_csv(FEATURES_PATH)
    except FileNotFoundError:
        print(f"â€¼ï¸ é”™è¯¯ï¼šæ‰¾ä¸åˆ°ç‰¹å¾æ–‡ä»¶ {FEATURES_PATH}ã€‚è¯·å…ˆè¿è¡Œä»»åŠ¡ä¸€çš„è„šæœ¬ã€‚")
        exit()

    # --- æ•°æ®å‡†å¤‡ ---
    if 'filename' not in df_features.columns:
        print("â€¼ï¸ é”™è¯¯ï¼šç‰¹å¾æ–‡ä»¶ä¸­ç¼ºå°‘'filename'åˆ—ã€‚è¯·æŒ‰æŒ‡å¯¼ä¿®æ”¹å¹¶é‡æ–°è¿è¡Œ02å’Œ03è„šæœ¬ã€‚")
        exit()

    X_raw = df_features.drop(columns=['label', 'rpm', 'filename'])
    y_str = df_features['label']
    groups = df_features['filename']

    le = LabelEncoder()
    y = le.fit_transform(y_str)
    print("âœ… æ•°æ®åŠ è½½å’Œæ ‡ç­¾ç¼–ç å®Œæˆã€‚")

    # --- æ­¥éª¤ 2: ä½¿ç”¨åˆ†ç»„åˆ’åˆ†(GroupShuffleSplit)ï¼Œä»æ ¹æºé¿å…æ³„éœ² ---
    print("\nğŸš€ æ­¥éª¤ 2: ä½¿ç”¨åˆ†ç»„åˆ’åˆ†è®­ç»ƒé›†ä¸æµ‹è¯•é›†...")
    gss = GroupShuffleSplit(n_splits=1, test_size=0.3, random_state=42)
    # groupså‚æ•°æ˜¯å…³é”®ï¼Œå®ƒç¡®ä¿äº†æ¥è‡ªåŒä¸€ä¸ªæ–‡ä»¶çš„æ ·æœ¬ä¸ä¼šåŒæ—¶å‡ºç°åœ¨è®­ç»ƒé›†å’Œæµ‹è¯•é›†
    train_idx, test_idx = next(gss.split(X_raw, y, groups))

    X_train_raw, X_test_raw = X_raw.iloc[train_idx], X_raw.iloc[test_idx]
    y_train, y_test = y[train_idx], y[test_idx]

    train_groups = set(groups.iloc[train_idx])
    test_groups = set(groups.iloc[test_idx])
    print(f"è®­ç»ƒé›†åŒ…å« {len(train_groups)} ä¸ªç‹¬ç«‹æ–‡ä»¶ã€‚")
    print(f"æµ‹è¯•é›†åŒ…å« {len(test_groups)} ä¸ªç‹¬ç«‹æ–‡ä»¶ã€‚")
    print(f"è®­ç»ƒé›†å’Œæµ‹è¯•é›†æ–‡ä»¶é‡å æ•°é‡: {len(train_groups.intersection(test_groups))}")  # ç»“æœåº”ä¸º0
    print(f"âœ… åˆ’åˆ†å®Œæˆã€‚è®­ç»ƒé›†æ ·æœ¬æ•°: {len(X_train_raw)}, æµ‹è¯•é›†æ ·æœ¬æ•°: {len(X_test_raw)}")

    # --- æ­¥éª¤ 3: è¿›è¡Œç‰¹å¾ç¼©æ”¾ (æ­£ç¡®æµç¨‹ï¼šå…ˆåˆ’åˆ†ï¼Œåç¼©æ”¾) ---
    print("\nğŸš€ æ­¥éª¤ 3: è¿›è¡Œç‰¹å¾ç¼©æ”¾...")
    scaler = StandardScaler()
    scaler.fit(X_train_raw)
    X_train = scaler.transform(X_train_raw)
    X_test = scaler.transform(X_test_raw)
    print("âœ… ç‰¹å¾ç¼©æ”¾å®Œæˆã€‚")

    # --- æ­¥éª¤ 4: æ¨¡å‹è®­ç»ƒ (XGBoost) ---
    print("\nğŸš€ æ­¥éª¤ 4: è®­ç»ƒXGBoostæ¨¡å‹...")
    sample_weights = compute_sample_weight(class_weight='balanced', y=y_train)
    model = xgb.XGBClassifier(objective='multi:softprob', eval_metric='mlogloss', use_label_encoder=False,
                              random_state=42)
    model.fit(X_train, y_train, sample_weight=sample_weights)
    print("âœ… æ¨¡å‹è®­ç»ƒå®Œæˆã€‚")

    # (åç»­çš„è¯„ä¼°ã€ç‰¹å¾é‡è¦æ€§ã€ä¿å­˜ä»£ç ä¸ä½ æä¾›çš„ç‰ˆæœ¬åŸºæœ¬ä¸€è‡´ï¼Œæ­¤å¤„ä¸ºå®Œæ•´ç‰ˆ)
    print("\nğŸš€ æ­¥éª¤ 5: è¯„ä¼°æ¨¡å‹æ€§èƒ½...")
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    print(f"\nğŸ“Š æ€»ä½“å‡†ç¡®ç‡ (æ— æ•°æ®æ³„éœ²): {accuracy:.4f}")

    class_names = le.classes_
    print("\nğŸ“Š åˆ†ç±»æŠ¥å‘Š:")
    print(classification_report(y_test, y_pred, target_names=class_names))

    cm = confusion_matrix(y_test, y_pred)
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)
    plt.title('æºåŸŸè¯Šæ–­æ¨¡å‹æ··æ·†çŸ©é˜µ (æ— æ•°æ®æ³„éœ²)', fontsize=16)
    plt.ylabel('çœŸå®ç±»åˆ«', fontsize=12)
    plt.xlabel('é¢„æµ‹ç±»åˆ«', fontsize=12)
    conf_matrix_path = os.path.join(OUTPUT_DIR, 'ä»»åŠ¡äºŒ-æºåŸŸè¯Šæ–­æ··æ·†çŸ©é˜µ(æ— æ³„éœ²).png')
    plt.savefig(conf_matrix_path, dpi=300)
    print(f"âœ… æ··æ·†çŸ©é˜µå·²ä¿å­˜è‡³: {os.path.abspath(conf_matrix_path)}")
    plt.close()

    print("\nğŸš€ æ­¥éª¤ 6: åˆ†æç‰¹å¾é‡è¦æ€§...")
    importances = model.feature_importances_
    df_importance = pd.DataFrame({'feature': X_raw.columns, 'importance': importances}).sort_values(by='importance',
                                                                                                    ascending=False)
    plt.figure(figsize=(12, 10))
    sns.barplot(x='importance', y='feature', data=df_importance)
    plt.title('ç‰¹å¾é‡è¦æ€§æ’åº', fontsize=16)
    plt.xlabel('é‡è¦æ€§åˆ†æ•°', fontsize=12)
    plt.ylabel('ç‰¹å¾åç§°', fontsize=12)
    plt.grid(True)
    plt.tight_layout()
    feature_importance_path = os.path.join(OUTPUT_DIR, 'ä»»åŠ¡äºŒ-ç‰¹å¾é‡è¦æ€§æ’åº.png')
    plt.savefig(feature_importance_path, dpi=300)
    print(f"âœ… ç‰¹å¾é‡è¦æ€§å›¾å·²ä¿å­˜è‡³: {os.path.abspath(feature_importance_path)}")
    plt.close()

    print("\nğŸš€ æ­¥éª¤ 7: ä¿å­˜æ¨¡å‹...")
    joblib.dump(model, os.path.join(OUTPUT_DIR, 'xgb_model.joblib'))
    joblib.dump(scaler, os.path.join(OUTPUT_DIR, 'scaler.joblib'))
    joblib.dump(le, os.path.join(OUTPUT_DIR, 'label_encoder.joblib'))
    print(f"âœ… æ¨¡å‹ã€ç¼©æ”¾å™¨å’Œæ ‡ç­¾ç¼–ç å™¨å·²ä¿å­˜è‡³: {os.path.abspath(OUTPUT_DIR)}")

    print("\nğŸ‰ ä»»åŠ¡äºŒï¼šæºåŸŸæ•…éšœè¯Šæ–­å…¨éƒ¨å·¥ä½œå·²å®Œæˆï¼")