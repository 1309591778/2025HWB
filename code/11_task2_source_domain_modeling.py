import os
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from matplotlib import font_manager
import xgboost as xgb
# ã€æ³¨æ„ã€‘æˆ‘ä»¬ç°åœ¨åªä½¿ç”¨ train_test_splitï¼Œå› ä¸ºè¿™ä¸ªç‰ˆæœ¬çš„ä»£ç ä¸å¤„ç†åˆ†ç»„é—®é¢˜
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.utils.class_weight import compute_sample_weight
import joblib


# ==============================================================================
# 0. å­—ä½“è®¾ç½®å‡½æ•° (ä¿æŒä¸å˜)
# ==============================================================================
def set_chinese_font():
    """ç›´æ¥ä»é¡¹ç›®æ–‡ä»¶å¤¹åŠ è½½æŒ‡å®šçš„å­—ä½“æ–‡ä»¶ã€‚"""
    # ä¸ºäº†è®©ä»£ç ç›´æ¥è¿è¡Œï¼Œæˆ‘ä»¬å…ˆæ³¨é‡Šæ‰å­—ä½“åŠ è½½ï¼Œä½ å¯ä»¥æ ¹æ®éœ€è¦æ¢å¤
    # font_path = os.path.join(os.path.dirname(__file__), 'fonts', 'SourceHanSansSC-Regular.otf')
    # if os.path.exists(font_path):
    #     font_prop = font_manager.FontProperties(fname=font_path)
    #     plt.rcParams['font.family'] = font_prop.get_name()
    # else:
    #     print(f"â€¼ï¸ è­¦å‘Šï¼šæ‰¾ä¸åˆ°å­—ä½“æ–‡ä»¶ {font_path}ï¼Œå»ºè®®ä¸‹è½½ã€‚")
    #     plt.rcParams['font.sans-serif'] = ['sans-serif']

    # é‡‡ç”¨ä¸€ä¸ªæ›´é€šç”¨çš„è®¾ç½®
    plt.rcParams['font.sans-serif'] = ['Microsoft YaHei']
    plt.rcParams['axes.unicode_minus'] = False
    print("âœ… å·²è®¾ç½®ä¸­æ–‡å­—ä½“ã€‚")


# ==============================================================================
# 1. ä¸»ç¨‹åº
# ==============================================================================
if __name__ == "__main__":
    set_chinese_font()

    # --- è·¯å¾„å®šä¹‰ ---
    PROCESSED_DIR = os.path.join('..', 'data', 'processed')
    # ã€æ³¨æ„ã€‘ç¡®ä¿ä½ ä½¿ç”¨çš„æ˜¯å¢å¼ºäº†ç‰¹å¾ã€æ‰©å……äº†æ•°æ®é‡çš„æœ€æ–°ç‰ˆç‰¹å¾æ–‡ä»¶
    FEATURES_PATH = os.path.join(PROCESSED_DIR, 'source_features_selected.csv')
    OUTPUT_DIR = os.path.join(PROCESSED_DIR, 'task2_outputs')
    os.makedirs(OUTPUT_DIR, exist_ok=True)

    # --- 1. æ•°æ®åŠ è½½ä¸å‡†å¤‡ ---
    print("ğŸš€ æ­¥éª¤ 1: åŠ è½½æ•°æ®...")
    try:
        df_features = pd.read_csv(FEATURES_PATH)
    except FileNotFoundError:
        print(f"â€¼ï¸ é”™è¯¯ï¼šæ‰¾ä¸åˆ°ç‰¹å¾æ–‡ä»¶ {FEATURES_PATH}ã€‚è¯·å…ˆé‡æ–°è¿è¡Œä»»åŠ¡ä¸€çš„è„šæœ¬ã€‚")
        exit()

    # ä»ç‰¹å¾æ–‡ä»¶ä¸­åˆ†ç¦»å‡ºç‰¹å¾Xå’Œæ ‡ç­¾y
    # æˆ‘ä»¬å‡è®¾'filename'åˆ—å­˜åœ¨å¹¶éœ€è¦è¢«ç§»é™¤
    if 'filename' in df_features.columns:
        X = df_features.drop(columns=['label', 'rpm', 'filename'])
    else:
        X = df_features.drop(columns=['label', 'rpm'])

    y_str = df_features['label']

    # æ ‡ç­¾ç¼–ç  (å°†'N','B','IR','OR'è½¬ä¸º0,1,2,3)
    le = LabelEncoder()
    y = le.fit_transform(y_str)
    print("âœ… æ•°æ®åŠ è½½å’Œæ ‡ç­¾ç¼–ç å®Œæˆã€‚")

    # --- ã€æ ¸å¿ƒä¿®æ”¹ã€‘æ­¥éª¤2: å…ˆåˆ’åˆ†è®­ç»ƒé›†ä¸æµ‹è¯•é›† ---
    print("\nğŸš€ æ­¥éª¤ 2: åˆ’åˆ†è®­ç»ƒé›†ä¸æµ‹è¯•é›† (åœ¨ç¼©æ”¾å‰è¿›è¡Œ)...")
    # æˆ‘ä»¬ç°åœ¨å¯¹åŸå§‹çš„ã€æœªç¼©æ”¾çš„ç‰¹å¾Xè¿›è¡Œåˆ’åˆ†
    X_train_raw, X_test_raw, y_train, y_test = train_test_split(
        X, y,
        test_size=0.3,
        random_state=42,
        stratify=y  # å¿…é¡»è¿›è¡Œåˆ†å±‚æŠ½æ ·
    )
    print(f"âœ… åˆ’åˆ†å®Œæˆã€‚è®­ç»ƒé›†æ ·æœ¬æ•°: {len(X_train_raw)}, æµ‹è¯•é›†æ ·æœ¬æ•°: {len(X_test_raw)}")

    # --- ã€æ ¸å¿ƒä¿®æ”¹ã€‘æ­¥éª¤3: åè¿›è¡Œç‰¹å¾ç¼©æ”¾ ---
    print("\nğŸš€ æ­¥éª¤ 3: è¿›è¡Œç‰¹å¾ç¼©æ”¾ (æ­£ç¡®æµç¨‹)...")
    scaler = StandardScaler()

    # åªç”¨è®­ç»ƒé›†çš„æ•°æ®æ¥'è®­ç»ƒ'ç¼©æ”¾å™¨ (fit)
    scaler.fit(X_train_raw)

    # ç”¨è¿™ä¸ªè®­ç»ƒå¥½çš„ç¼©æ”¾å™¨ï¼Œåˆ†åˆ«å¯¹è®­ç»ƒé›†å’Œæµ‹è¯•é›†è¿›è¡Œè½¬æ¢ (transform)
    X_train = scaler.transform(X_train_raw)
    X_test = scaler.transform(X_test_raw)
    print("âœ… ç‰¹å¾ç¼©æ”¾å®Œæˆã€‚")

    # --- æ­¥éª¤ 4: æ¨¡å‹è®­ç»ƒ (XGBoost) ---
    print("\nğŸš€ æ­¥éª¤ 4: è®­ç»ƒXGBoostæ¨¡å‹...")
    sample_weights = compute_sample_weight(class_weight='balanced', y=y_train)

    model = xgb.XGBClassifier(
        objective='multi:softprob',
        eval_metric='mlogloss',
        use_label_encoder=False,
        random_state=42
    )

    model.fit(X_train, y_train, sample_weight=sample_weights)
    print("âœ… æ¨¡å‹è®­ç»ƒå®Œæˆã€‚")

    # --- æ­¥éª¤ 5: æ¨¡å‹è¯„ä¼° ---
    print("\nğŸš€ æ­¥éª¤ 5: è¯„ä¼°æ¨¡å‹æ€§èƒ½...")
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    print(f"\nğŸ“Š æ€»ä½“å‡†ç¡®ç‡ (ä¿®æ­£å): {accuracy:.4f}")

    class_names = le.classes_
    print("\nğŸ“Š åˆ†ç±»æŠ¥å‘Š:")
    print(classification_report(y_test, y_pred, target_names=class_names))

    cm = confusion_matrix(y_test, y_pred)
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)
    plt.title('æºåŸŸè¯Šæ–­æ¨¡å‹æ··æ·†çŸ©é˜µ (ä¿®æ­£å)', fontsize=16)
    plt.ylabel('çœŸå®ç±»åˆ«', fontsize=12)
    plt.xlabel('é¢„æµ‹ç±»åˆ«', fontsize=12)
    conf_matrix_path = os.path.join(OUTPUT_DIR, 'ä»»åŠ¡äºŒ-æºåŸŸè¯Šæ–­æ··æ·†çŸ©é˜µ(ä¿®æ­£å).png')
    plt.savefig(conf_matrix_path, dpi=300)
    print(f"âœ… æ··æ·†çŸ©é˜µå·²ä¿å­˜è‡³: {os.path.abspath(conf_matrix_path)}")
    plt.close()

    # --- æ­¥éª¤ 6: ç‰¹å¾é‡è¦æ€§åˆ†æ ---
    print("\nğŸš€ æ­¥éª¤ 6: åˆ†æç‰¹å¾é‡è¦æ€§...")
    importances = model.feature_importances_
    df_importance = pd.DataFrame({'feature': X.columns, 'importance': importances})
    df_importance = df_importance.sort_values(by='importance', ascending=False)

    plt.figure(figsize=(12, 12))
    sns.barplot(x='importance', y='feature', data=df_importance.head(30))
    plt.title('Top 30 ç‰¹å¾é‡è¦æ€§æ’åº', fontsize=16)
    plt.xlabel('é‡è¦æ€§åˆ†æ•°', fontsize=12)
    plt.ylabel('ç‰¹å¾åç§°', fontsize=12)
    plt.grid(True)
    plt.tight_layout()
    feature_importance_path = os.path.join(OUTPUT_DIR, 'ä»»åŠ¡äºŒ-ç‰¹å¾é‡è¦æ€§æ’åº.png')
    plt.savefig(feature_importance_path, dpi=300)
    print(f"âœ… ç‰¹å¾é‡è¦æ€§å›¾å·²ä¿å­˜è‡³: {os.path.abspath(feature_importance_path)}")
    plt.close()

    # --- æ­¥éª¤ 7: ä¿å­˜æ¨¡å‹åŠé¢„å¤„ç†å™¨ ---
    print("\nğŸš€ æ­¥éª¤ 7: ä¿å­˜æ¨¡å‹...")
    joblib.dump(model, os.path.join(OUTPUT_DIR, 'xgb_model.joblib'))
    joblib.dump(scaler, os.path.join(OUTPUT_DIR, 'scaler.joblib'))
    joblib.dump(le, os.path.join(OUTPUT_DIR, 'label_encoder.joblib'))
    print(f"âœ… æ¨¡å‹ã€ç¼©æ”¾å™¨å’Œæ ‡ç­¾ç¼–ç å™¨å·²ä¿å­˜è‡³: {os.path.abspath(OUTPUT_DIR)}")

    print("\nğŸ‰ ä»»åŠ¡äºŒï¼šæºåŸŸæ•…éšœè¯Šæ–­å…¨éƒ¨å·¥ä½œå·²å®Œæˆï¼")