import os
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from matplotlib import font_manager
import xgboost as xgb
from sklearn.model_selection import StratifiedGroupKFold, train_test_split  # ã€æ ¸å¿ƒã€‘å¯¼å…¥åˆ†ç»„åˆ†å±‚KæŠ˜äº¤å‰éªŒè¯
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score
import joblib
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Conv1D, BatchNormalization, ReLU, MaxPooling1D, GlobalAveragePooling1D, \
    Dense, Dropout, multiply, Activation
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping
# ä¿®æ­£importè¯­å¥
from sklearn.utils.class_weight import compute_class_weight  # æ­£ç¡®çš„å‡½æ•°å


# ==============================================================================
# 0. è¾…åŠ©å‡½æ•°
# ==============================================================================
def set_chinese_font():
    """è®¾ç½®ä¸­æ–‡å­—ä½“"""
    plt.rcParams['font.sans-serif'] = ['Microsoft YaHei']
    plt.rcParams['axes.unicode_minus'] = False
    print("âœ… å·²è®¾ç½®ä¸­æ–‡å­—ä½“ã€‚")


def categorical_focal_loss(gamma=2., alpha=0.25):
    """
    Focal Loss for addressing class imbalance in categorical classification
    """

    def categorical_focal_loss_fixed(y_true, y_pred):
        epsilon = tf.keras.backend.epsilon()
        y_pred = tf.keras.backend.clip(y_pred, epsilon, 1. - epsilon)
        cross_entropy = -y_true * tf.keras.backend.log(y_pred)
        weight = alpha * y_true * tf.keras.backend.pow((1 - y_pred), gamma)
        loss = weight * cross_entropy
        loss = tf.keras.backend.sum(loss, axis=1)
        return loss

    return categorical_focal_loss_fixed


def create_improved_cnn_model(input_shape, num_classes):
    """åˆ›å»ºæ”¹è¿›çš„1D-CNNæ¨¡å‹"""
    """ä¿®å¤æ³¨æ„åŠ›æœºåˆ¶çš„CNNæ¨¡å‹"""
    inputs = Input(shape=input_shape)

    # ç‰¹å¾æå–
    x = Conv1D(filters=64, kernel_size=3, padding='same')(inputs)
    x = BatchNormalization()(x)
    x = ReLU()(x)
    x = MaxPooling1D(pool_size=2, padding='same')(x)

    x = Conv1D(filters=128, kernel_size=3, padding='same')(x)
    x = BatchNormalization()(x)
    x = ReLU()(x)
    x = MaxPooling1D(pool_size=2, padding='same')(x)

    x = Conv1D(filters=256, kernel_size=3, padding='same')(x)
    x = BatchNormalization()(x)
    x = ReLU()(x)
    cnn_out = MaxPooling1D(pool_size=2, padding='same')(x)  # Shape: (batch, steps, 256)

    # ä¿®å¤çš„æ³¨æ„åŠ›æœºåˆ¶ - åœ¨ç‰¹å¾ç»´åº¦ä¸Šè®¡ç®—æ³¨æ„åŠ›
    # Step 1: å°†ç‰¹å¾ç»´åº¦å˜æ¢ç”¨äºæ³¨æ„åŠ›è®¡ç®—
    attention_weights = Dense(256, activation='tanh')(cnn_out)  # Shape: (batch, steps, 256)
    attention_scores = Dense(1, activation='tanh')(attention_weights)  # Shape: (batch, steps, 1)
    # ä¿®å¤ï¼šæ­£ç¡®ä½¿ç”¨softmaxçš„axiså‚æ•°
    attention_probs = Activation(lambda x: tf.nn.softmax(x, axis=1), name='attention_weights')(
        attention_scores)  # åœ¨æ—¶é—´æ­¥ç»´åº¦softmax

    # Step 2: åº”ç”¨æ³¨æ„åŠ›æƒé‡
    attention_mul = multiply([cnn_out, attention_probs])  # Shape: (batch, steps, 256)

    # Step 3: èšåˆ
    x = GlobalAveragePooling1D()(attention_mul)
    x = Dense(128, activation='relu')(x)
    x = Dropout(0.3)(x)
    outputs = Dense(num_classes, activation='softmax')(x)

    model = Model(inputs=inputs, outputs=outputs)
    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
        loss=categorical_focal_loss(gamma=2., alpha=0.5),  # ä½¿ç”¨Focal Loss
        metrics=['accuracy']
    )
    return model


def compute_balanced_sample_weights(y, n_class_index=2, n_class_weight=3.0):
    """è®¡ç®—å¹³è¡¡çš„æ ·æœ¬æƒé‡ï¼Œç»™Nç±»æ›´é«˜çš„æƒé‡"""
    # è·å–ç±»åˆ«æ•°é‡
    n_classes = len(np.unique(y))

    # é»˜è®¤æƒé‡ä¸º1.0
    class_weights = {i: 1.0 for i in range(n_classes)}

    # ç»™Nç±»ï¼ˆå‡è®¾ç´¢å¼•ä¸º2ï¼‰æ›´é«˜çš„æƒé‡
    class_weights[n_class_index] = n_class_weight

    # ä¸ºæ¯ä¸ªæ ·æœ¬åˆ†é…æƒé‡
    sample_weights = np.array([class_weights[label] for label in y])
    return sample_weights


# ==============================================================================
# 1. ä¸»ç¨‹åº
# ==============================================================================
if __name__ == "__main__":
    set_chinese_font()

    RANDOM_STATE = 42
    N_SPLITS = 5
    PROCESSED_DIR = os.path.join('..', 'data', 'processed')
    FEATURES_PATH = os.path.join(PROCESSED_DIR, 'source_features_selected.csv')
    OUTPUT_DIR = os.path.join(PROCESSED_DIR, 'task2_outputs_final')
    os.makedirs(OUTPUT_DIR, exist_ok=True)

    print("ğŸš€ æ­¥éª¤ 1: åŠ è½½æ•°æ®...")
    df_features = pd.read_csv(FEATURES_PATH)
    X_raw = df_features.drop(columns=['label', 'rpm', 'filename'])
    y_str = df_features['label']
    groups = df_features['filename']
    le = LabelEncoder()
    y = le.fit_transform(y_str)
    # --- ã€æ ¸å¿ƒä¿®æ­£ã€‘å®šä¹‰å…¨å±€å¸¸é‡ ---
    NUM_CLASSES = len(le.classes_)

    print(f"\nğŸš€ æ­¥éª¤ 2: å¼€å§‹ {N_SPLITS} æŠ˜åˆ†ç»„åˆ†å±‚äº¤å‰éªŒè¯...")
    sgkf = StratifiedGroupKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)

    all_y_test, xgb_all_y_pred, cnn_all_y_pred = [], [], []

    for fold, (train_idx, test_idx) in enumerate(sgkf.split(X_raw, y, groups)):
        print(f"\n--- ç¬¬ {fold + 1}/{N_SPLITS} æŠ˜ ---")
        X_train_raw, X_test_raw = X_raw.iloc[train_idx], X_raw.iloc[test_idx]
        y_train, y_test = y[train_idx], y[test_idx]

        scaler = StandardScaler()
        X_train = scaler.fit_transform(X_train_raw)
        X_test = scaler.transform(X_test_raw)
        all_y_test.extend(y_test)

        print("  - æ­£åœ¨è®­ç»ƒ XGBoost (å«æ—©åœè°ƒå‚)...")
        X_train_sub, X_val, y_train_sub, y_val = train_test_split(
            X_train, y_train, test_size=0.2, stratify=y_train, random_state=RANDOM_STATE
        )

        # è®¡ç®—æ ·æœ¬æƒé‡ï¼Œé‡ç‚¹æå‡Nç±»æƒé‡
        sample_weights_train = compute_balanced_sample_weights(y_train_sub)
        sample_weights_val = compute_balanced_sample_weights(y_val)

        # è½¬æ¢ä¸º DMatrix æ ¼å¼ï¼ŒåŠ å…¥æ ·æœ¬æƒé‡
        dtrain = xgb.DMatrix(X_train_sub, label=y_train_sub, weight=sample_weights_train)
        dval = xgb.DMatrix(X_val, label=y_val, weight=sample_weights_val)

        # ä¼˜åŒ–å‚æ•°ï¼šé’ˆå¯¹ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜
        params = {
            'objective': 'multi:softprob',
            'eval_metric': 'mlogloss',
            'num_class': NUM_CLASSES,
            'random_state': RANDOM_STATE,
            # ä¼˜åŒ–å‚æ•°
            'max_depth': 6,  # é™ä½æ·±åº¦é¿å…è¿‡æ‹Ÿåˆ
            'learning_rate': 0.1,  # æé«˜å­¦ä¹ ç‡
            'subsample': 0.8,
            'colsample_bytree': 0.8,
            'min_child_weight': 3,  # å¢åŠ æœ€å°æƒé‡
            'gamma': 0.2,  # å¢åŠ æœ€å°æŸå¤±å‡å°‘é‡
        }

        # ä½¿ç”¨åŸç”Ÿè®­ç»ƒ API + æ—©åœ
        evals_result = {}
        bst = xgb.train(
            params,
            dtrain,
            num_boost_round=500,  # å‡å°‘è¿­ä»£æ¬¡æ•°
            evals=[(dtrain, 'train'), (dval, 'val')],
            early_stopping_rounds=30,  # æ—©åœè½®æ•°
            evals_result=evals_result,
            verbose_eval=False
        )

        print(f"    æœ€ä½³æ ‘æ•°é‡: {bst.best_iteration}")

        # ä½¿ç”¨ DMatrix è¿›è¡Œé¢„æµ‹
        dtest = xgb.DMatrix(X_test)
        class_probs = bst.predict(dtest)
        y_pred_xgb = np.argmax(class_probs, axis=1)
        xgb_all_y_pred.extend(y_pred_xgb)

        print("  - æ­£åœ¨è®­ç»ƒ 1D-CNN + Attention (å«æ—©åœ)...")
        X_train_cnn, X_test_cnn = np.expand_dims(X_train, axis=2), np.expand_dims(X_test, axis=2)
        y_train_cat = to_categorical(y_train, num_classes=NUM_CLASSES)
        # ä½¿ç”¨æ”¹è¿›çš„CNNæ¨¡å‹
        cnn_model = create_improved_cnn_model(input_shape=(X_train_cnn.shape[1], 1), num_classes=NUM_CLASSES)
        # è°ƒæ•´æ—©åœå‚æ•°
        early_stopping = EarlyStopping(monitor='val_accuracy', patience=30, restore_best_weights=True)

        # è®¡ç®—ç±»åˆ«æƒé‡
        class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)
        class_weight_dict = dict(enumerate(class_weights))

        # ä½¿ç”¨Focal Losså’Œç±»åˆ«æƒé‡è®­ç»ƒ
        cnn_model.fit(
            X_train_cnn, y_train_cat,
            epochs=200,  # å¢åŠ è®­ç»ƒè½®æ•°
            batch_size=32,  # å‡å°æ‰¹æ¬¡å¤§å°
            verbose=0,
            validation_split=0.2,
            class_weight=class_weight_dict,
            callbacks=[early_stopping]
        )
        y_pred_cnn_prob = cnn_model.predict(X_test_cnn)
        y_pred_cnn = np.argmax(y_pred_cnn_prob, axis=1)
        cnn_all_y_pred.extend(y_pred_cnn)

    print("\nğŸš€ æ­¥éª¤ 3: äº¤å‰éªŒè¯å®Œæˆï¼Œæ±‡æ€»è¯„ä¼°ç»“æœ...")
    models_results = {"XGBoost": xgb_all_y_pred, "1D-CNN-Attention": cnn_all_y_pred}
    for model_name, y_pred in models_results.items():
        accuracy = accuracy_score(all_y_test, y_pred)
        report_dict = classification_report(all_y_test, y_pred, target_names=le.classes_, output_dict=True)
        print(f"\n========== {model_name} æœ€ç»ˆæ€§èƒ½è¯„ä¼° ==========")
        print(f"ğŸ“Š æ€»ä½“å‡†ç¡®ç‡ (5æŠ˜äº¤å‰éªŒè¯): {accuracy:.4f}")
        print("\nğŸ“Š åˆ†ç±»æŠ¥å‘Š:")
        print(classification_report(all_y_test, y_pred, target_names=le.classes_))

        print("\n--- å…³é”®é£é™©æŒ‡æ ‡åˆ†æ ---")
        ir_recall = report_dict.get('IR', {}).get('recall', 'N/A')
        b_precision = report_dict.get('B', {}).get('precision', 'N/A')
        print(f"  - å†…åœˆæ•…éšœ(IR)çš„å¬å›ç‡ (é¿å…æ¼åˆ¤): {ir_recall:.4f}")
        print(f"  - æ»šåŠ¨ä½“æ•…éšœ(B)çš„ç²¾ç¡®ç‡ (é¿å…è¯¯åˆ¤): {b_precision:.4f}")

        cm = confusion_matrix(all_y_test, y_pred)
        np.fill_diagonal(cm, 0)
        if np.max(cm) > 0:
            misclass_idx = np.unravel_index(np.argmax(cm), cm.shape)
            true_label, pred_label = le.classes_[misclass_idx[0]], le.classes_[misclass_idx[1]]
            print(f"  - æœ€ä¸»è¦çš„æ··æ·†: æ¨¡å‹å€¾å‘äºå°†çœŸå®çš„'{true_label}'ç±»åˆ«è¯¯åˆ¤ä¸º'{pred_label}'ç±»åˆ« ({np.max(cm)}æ¬¡)ã€‚")

        cm_full = confusion_matrix(all_y_test, y_pred)
        plt.figure(figsize=(10, 8))
        sns.heatmap(cm_full, annot=True, fmt='d', cmap='Blues', xticklabels=le.classes_, yticklabels=le.classes_)
        plt.title(f'{model_name} æ¨¡å‹æ··æ·†çŸ©é˜µ (5æŠ˜äº¤å‰éªŒè¯)', fontsize=16)
        plt.savefig(os.path.join(OUTPUT_DIR, f'ä»»åŠ¡äºŒ-{model_name}-æ··æ·†çŸ©é˜µ.png'), dpi=300)
        plt.close()
        print(f"âœ… {model_name} çš„æ··æ·†çŸ©é˜µå·²ä¿å­˜ã€‚")

    print("\nğŸš€ æ­¥éª¤ 4: æ­£åœ¨ä½¿ç”¨å…¨éƒ¨æºåŸŸæ•°æ®è®­ç»ƒæœ€ç»ˆæ¨¡å‹ä»¥ç”¨äºä»»åŠ¡ä¸‰...")
    final_scaler = StandardScaler().fit(X_raw)
    X_scaled_full = final_scaler.transform(X_raw)

    # è®¡ç®—æ ·æœ¬æƒé‡ï¼Œé‡ç‚¹æå‡Nç±»æƒé‡
    sample_weights_full = compute_balanced_sample_weights(y)

    print("  - æ­£åœ¨è®­ç»ƒæœ€ç»ˆçš„XGBoostæ¨¡å‹...")
    final_xgb_model = xgb.XGBClassifier(
        objective='multi:softprob',
        eval_metric='mlogloss',
        random_state=RANDOM_STATE,
        n_estimators=500,  # å¢åŠ æ ‘æ•°é‡
        max_depth=6,  # é™ä½æ·±åº¦é¿å…è¿‡æ‹Ÿåˆ
        learning_rate=0.1,  # åˆé€‚çš„å­¦ä¹ ç‡
        subsample=0.8,
        colsample_bytree=0.8,
        min_child_weight=3,
        gamma=0.2,
    )
    final_xgb_model.fit(X_scaled_full, y, sample_weight=sample_weights_full)

    print("  - æ­£åœ¨è®­ç»ƒæœ€ç»ˆçš„1D-CNN+Attentionæ¨¡å‹...")
    X_cnn_full = np.expand_dims(X_scaled_full, axis=2)
    y_cat_full = to_categorical(y, num_classes=NUM_CLASSES)
    # ä½¿ç”¨æ”¹è¿›çš„CNNæ¨¡å‹
    final_cnn_model = create_improved_cnn_model(input_shape=(X_cnn_full.shape[1], 1), num_classes=NUM_CLASSES)
    # --- ã€æ ¸å¿ƒä¿®æ­£ã€‘ä¸ºæœ€ç»ˆæ¨¡å‹è®­ç»ƒæ·»åŠ æ—©åœï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ ---
    early_stopping_final = EarlyStopping(monitor='accuracy', patience=30, restore_best_weights=True)

    # ä¸ºæœ€ç»ˆæ¨¡å‹ä¹Ÿæ·»åŠ ç±»åˆ«æƒé‡
    class_weights_final = compute_class_weight('balanced', classes=np.unique(y), y=y)
    class_weight_dict_final = dict(enumerate(class_weights_final))

    final_cnn_model.fit(X_cnn_full, y_cat_full, epochs=200, batch_size=32, verbose=0,
                        class_weight=class_weight_dict_final,  # æ·»åŠ ç±»åˆ«æƒé‡
                        callbacks=[early_stopping_final])

    print("\nğŸš€ æ­¥éª¤ 5: æ­£åœ¨ä¿å­˜æœ€ç»ˆäº§å‡º...")
    joblib.dump(final_xgb_model, os.path.join(OUTPUT_DIR, 'final_xgb_model.joblib'))
    joblib.dump(final_scaler, os.path.join(OUTPUT_DIR, 'final_scaler.joblib'))
    joblib.dump(le, os.path.join(OUTPUT_DIR, 'final_label_encoder.joblib'))
    # --- ã€æ ¸å¿ƒä¿®æ­£ã€‘å°† .keras æ”¹ä¸º .h5 ä»¥ç¡®ä¿å…¼å®¹æ€§ ---
    final_cnn_model.save(os.path.join(OUTPUT_DIR, 'final_cnn_model.h5'))

    print(f"âœ… æœ€ç»ˆæ¨¡å‹åŠé¢„å¤„ç†å™¨å·²ä¿å­˜è‡³: {os.path.abspath(OUTPUT_DIR)}")
    print("\nğŸ‰ ä»»åŠ¡äºŒï¼šæºåŸŸæ•…éšœè¯Šæ–­ï¼ˆæœ€ç»ˆç‰ˆï¼‰å…¨éƒ¨å·¥ä½œå·²åœ†æ»¡å®Œæˆï¼")