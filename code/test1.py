# 22_task3_prediction_and_visualization.py
import os
import numpy as np
import pandas as pd
import joblib
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.manifold import TSNE
from matplotlib import font_manager
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, Dropout, Lambda, Layer
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping
import scipy.io
from scipy.signal import hilbert


# è®¾ç½®ä¸­æ–‡å­—ä½“
def set_chinese_font():
    """è®¾ç½®ä¸­æ–‡å­—ä½“"""
    plt.rcParams['font.sans-serif'] = ['Microsoft YaHei']
    plt.rcParams['axes.unicode_minus'] = False
    print("âœ… å·²è®¾ç½®ä¸­æ–‡å­—ä½“ã€‚")


# åˆ›å»ºè¾“å‡ºç›®å½•
def create_output_dir():
    """åˆ›å»ºè¾“å‡ºç›®å½•"""
    output_dir = os.path.join('..', 'data', 'processed', 'task3_outputs_dann')
    os.makedirs(output_dir, exist_ok=True)
    return output_dir


# --- æ–°å¢ï¼šå®šä¹‰æ¢¯åº¦åè½¬å±‚ (Gradient Reversal Layer) ---
@tf.custom_gradient
def grad_reverse(x, lambda_val=1.0):
    """æ¢¯åº¦åè½¬å‡½æ•°"""
    y = tf.identity(x)

    def custom_grad(dy):
        return -dy * lambda_val, None

    return y, custom_grad


class GradReverse(tf.keras.layers.Layer):
    """æ¢¯åº¦åè½¬å±‚ Keras å°è£…"""

    def __init__(self, lambda_val=1.0, **kwargs):
        super(GradReverse, self).__init__(**kwargs)
        self.lambda_val = tf.Variable(lambda_val, trainable=False, dtype=tf.float32)

    def call(self, x):
        return grad_reverse(x, self.lambda_val)

    def get_config(self):
        config = super(GradReverse, self).get_config()
        config.update({'lambda_val': float(self.lambda_val.numpy())})
        return config


# --- æ–°å¢ç»“æŸ ---


# --- æ–°å¢ï¼št-SNE å¯è§†åŒ– ---
def visualize_tsne(X_source, y_source, X_target, source_le, target_predictions_dict, output_dir, title_suffix=""):
    """ä½¿ç”¨ t-SNE å¯è§†åŒ–æºåŸŸå’Œç›®æ ‡åŸŸæ•°æ®"""
    print(f"  - æ­£åœ¨å¯¹ç‰¹å¾è¿›è¡Œ t-SNE é™ç»´ ({title_suffix})...")
    try:
        # åˆå¹¶ç‰¹å¾å’Œæ ‡ç­¾
        all_features = np.vstack([X_source, X_target])
        # åˆ›å»ºåŸŸæ ‡ç­¾
        domain_labels = np.array(['Source'] * len(X_source) + ['Target'] * len(X_target))

        # t-SNE é™ç»´
        tsne = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000)
        tsne_results = tsne.fit_transform(all_features)

        plt.figure(figsize=(12, 10))

        # ç»˜åˆ¶æºåŸŸæ•°æ®ï¼ˆæŒ‰ç±»åˆ«ç€è‰²ï¼‰
        unique_source_classes = np.unique(y_source)
        colors_classes = plt.cm.get_cmap('tab10', len(unique_source_classes))

        # ä¿®å¤t-SNEå¯è§†åŒ–é—®é¢˜ï¼šæ‰©å±•æ ‡ç­¾æ•°ç»„ä»¥åŒ¹é…åˆå¹¶ç‰¹å¾çš„é•¿åº¦
        extended_y_source = np.concatenate([y_source, np.zeros(len(X_target))])  # æ‰©å±•æºåŸŸæ ‡ç­¾

        for i, cls_int in enumerate(unique_source_classes):
            cls_name = source_le.inverse_transform([cls_int])[0]
            # ä¿®æ­£ï¼šä½¿ç”¨æ‰©å±•çš„æ ‡ç­¾æ•°ç»„è¿›è¡Œç­›é€‰
            source_mask = domain_labels == 'Source'
            cls_mask = extended_y_source == cls_int
            idx = source_mask & cls_mask
            plt.scatter(tsne_results[idx, 0], tsne_results[idx, 1],
                        c=[colors_classes(i)], label=f'Source-{cls_name}', alpha=0.6, s=20)

        # ç»˜åˆ¶ç›®æ ‡åŸŸæ•°æ®ï¼ˆæŒ‰é¢„æµ‹ç±»åˆ«ç€è‰²ï¼‰
        target_filenames_in_order = list(target_predictions_dict.keys())
        y_target_pred_str = [target_predictions_dict[fname] for fname in target_filenames_in_order]

        # å¤„ç†ç›®æ ‡åŸŸé¢„æµ‹æ ‡ç­¾ä¸åœ¨æºåŸŸæ ‡ç­¾ä¸­çš„æƒ…å†µ
        valid_target_mask = np.array([label in source_le.classes_ for label in y_target_pred_str])
        valid_target_filenames = np.array(target_filenames_in_order)[valid_target_mask]
        valid_y_target_pred_str = np.array(y_target_pred_str)[valid_target_mask]

        if len(valid_y_target_pred_str) > 0:
            try:
                y_target_pred_int_valid = source_le.transform(valid_y_target_pred_str)
                target_indices = np.where(domain_labels == 'Target')[0]
                # ä¿®å¤ç»´åº¦ä¸åŒ¹é…é—®é¢˜
                valid_target_mask_full = np.isin(domain_labels, ['Target']) & np.isin(
                    domain_labels[domain_labels == 'Target'], ['Target'])

                # æ­£ç¡®çš„ç´¢å¼•æ˜ å°„
                target_filename_map = {}
                for i, fname in enumerate(target_filenames):
                    if fname not in target_filename_map:
                        target_filename_map[fname] = []
                    target_filename_map[fname].append(i)

                for i, cls_int in enumerate(unique_source_classes):
                    cls_name = source_le.inverse_transform([cls_int])[0]
                    # æ‰¾åˆ°è¯¥ç±»åˆ«çš„æ–‡ä»¶
                    files_for_class = [fname for fname, pred in target_predictions_dict.items() if
                                       pred['predicted_label'] == cls_name]

                    for fname in files_for_class:
                        if fname in target_filename_map:
                            file_indices = target_filename_map[fname]
                            # åœ¨tsne_resultsä¸­æ‰¾åˆ°å¯¹åº”çš„ç›®æ ‡åŸŸç´¢å¼•
                            target_start_idx = len(X_source)
                            tsne_file_indices = [target_start_idx + idx for idx in file_indices]
                            # ç¡®ä¿ç´¢å¼•åœ¨èŒƒå›´å†…
                            tsne_file_indices = [idx for idx in tsne_file_indices if idx < len(tsne_results)]
                            if tsne_file_indices:
                                plt.scatter(tsne_results[tsne_file_indices, 0], tsne_results[tsne_file_indices, 1],
                                            c=[colors_classes(i)], label=f'Target-{cls_name}', alpha=0.5, s=20,
                                            marker='x')
            except ValueError as e:
                print(f"  âš ï¸  æ ‡ç­¾è½¬æ¢é”™è¯¯: {e}ã€‚å°†ä½¿ç”¨ç»Ÿä¸€é¢œè‰²ç»˜åˆ¶ç›®æ ‡åŸŸæ•°æ®ã€‚")
                target_indices = np.where(domain_labels == 'Target')[0]
                plt.scatter(tsne_results[target_indices, 0], tsne_results[target_indices, 1],
                            c='red', label='Target (Uncertain Labels)', alpha=0.5, s=20, marker='x')
        else:
            print("  âš ï¸  æ²¡æœ‰æœ‰æ•ˆçš„ç›®æ ‡åŸŸé¢„æµ‹æ ‡ç­¾ç”¨äºå¯è§†åŒ–ã€‚")
            target_indices = np.where(domain_labels == 'Target')[0]
            plt.scatter(tsne_results[target_indices, 0], tsne_results[target_indices, 1],
                        c='red', label='Target (No Valid Labels)', alpha=0.5, s=20, marker='x')

        plt.title(f't-SNE å¯è§†åŒ–: æºåŸŸä¸ç›®æ ‡åŸŸç‰¹å¾åˆ†å¸ƒ {title_suffix}', fontsize=16, weight='bold')
        plt.xlabel('t-SNE Component 1')
        plt.ylabel('t-SNE Component 2')
        plt.legend(loc='upper right', bbox_to_anchor=(1.3, 1))
        plt.grid(True, alpha=0.3)
        plt.tight_layout()

        save_path = os.path.join(output_dir, f'22_tsne_features_{title_suffix.replace(" ", "_")}.png')
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
        print(f"  - âœ… t-SNE å›¾å·²ä¿å­˜è‡³: {save_path}")
    except Exception as e:
        print(f"  - âš ï¸ t-SNE å¯è§†åŒ–å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


# --- æ–°å¢ç»“æŸ ---


# ä¸»ç¨‹åº
if __name__ == "__main__":
    set_chinese_font()
    output_dir = create_output_dir()

    print("ğŸš€ ä»»åŠ¡ä¸‰ï¼šé¢„æµ‹ä¸å¯è§†åŒ– (åŸºäºå·²è®­ç»ƒçš„DANNæ¨¡å‹) å¼€å§‹æ‰§è¡Œ...")

    # --- é˜¶æ®µä¸€ï¼šåŠ è½½æºåŸŸæ¨¡å‹ä¸é¢„å¤„ç†å™¨ ---
    print("\n--- é˜¶æ®µä¸€ï¼šåŠ è½½æºåŸŸæ¨¡å‹ä¸é¢„å¤„ç†å™¨ ---")

    PROCESSED_DIR = os.path.join('..', 'data', 'processed')
    TASK2_OUTPUTS_DIR = os.path.join(PROCESSED_DIR, 'task2_outputs_final')

    SCALER_PATH = os.path.join(TASK2_OUTPUTS_DIR, 'final_scaler.joblib')
    LE_PATH = os.path.join(TASK2_OUTPUTS_DIR, 'final_label_encoder.joblib')

    try:
        scaler = joblib.load(SCALER_PATH)
        le = joblib.load(LE_PATH)
        print("âœ… æˆåŠŸåŠ è½½ StandardScaler å’Œ LabelEncoderã€‚")
    except FileNotFoundError as e:
        print(f"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ–‡ä»¶ {e.filename}ã€‚è¯·ç¡®ä¿ä»»åŠ¡äºŒå·²æˆåŠŸè¿è¡Œå¹¶ç”Ÿæˆäº†è¾“å‡ºæ–‡ä»¶ã€‚")
        exit(1)
    except Exception as e:
        print(f"âŒ åŠ è½½é¢„å¤„ç†å™¨æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        exit(1)

    # --- é˜¶æ®µäºŒï¼šåŠ è½½ç›®æ ‡åŸŸæ•°æ® ---
    print("\n--- é˜¶æ®µäºŒï¼šåŠ è½½ç›®æ ‡åŸŸæ•°æ® ---")

    # 1. åŠ è½½ç›®æ ‡åŸŸæ•°æ®
    TARGET_FEATURES_PATH = os.path.join(PROCESSED_DIR, 'target_features.csv')

    try:
        df_target_features = pd.read_csv(TARGET_FEATURES_PATH)
        print(f"âœ… æˆåŠŸåŠ è½½ç›®æ ‡åŸŸç‰¹å¾æ•°æ®: {df_target_features.shape}")

        # 2. åˆ†ç¦»æ–‡ä»¶åå’Œç‰¹å¾
        target_filenames = df_target_features['source_file']

        # ä¿®å¤ï¼šè·å–ä¸æºåŸŸè®­ç»ƒæ—¶ç›¸åŒçš„ç‰¹å¾åˆ—ï¼ˆæ’é™¤labelã€rpmã€filenameç­‰ï¼‰
        SOURCE_FEATURES_PATH = os.path.join(PROCESSED_DIR, 'source_features_selected.csv')
        df_source_features = pd.read_csv(SOURCE_FEATURES_PATH)
        selected_feature_names = df_source_features.drop(columns=['label', 'rpm', 'filename']).columns.tolist()

        X_target_raw = df_target_features[selected_feature_names]  # ä½¿ç”¨ä¸æºåŸŸç›¸åŒçš„ç‰¹å¾

        # 3. æ ‡å‡†åŒ–ç‰¹å¾
        X_target_scaled = scaler.transform(X_target_raw)

        print(f"âœ… ç›®æ ‡åŸŸæ•°æ®é¢„å¤„ç†å®Œæˆã€‚")

    except FileNotFoundError as e:
        print(f"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ•°æ®æ–‡ä»¶ {e.filename}ã€‚")
        exit(1)
    except Exception as e:
        print(f"âŒ å¤„ç†æ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        exit(1)

    # --- é˜¶æ®µä¸‰ï¼šåŠ è½½å·²è®­ç»ƒçš„5ä¸ªæ¨¡å‹å¹¶è¿›è¡Œé¢„æµ‹ ---
    print("\n--- é˜¶æ®µä¸‰ï¼šåŠ è½½å·²è®­ç»ƒæ¨¡å‹å¹¶è¿›è¡Œé¢„æµ‹ ---")

    # ä½¿ç”¨5ä¸ªéšæœºç§å­çš„æ¨¡å‹
    random_seeds = [42, 123, 456, 789, 999]
    all_model_file_predictions = []  # å­˜å‚¨æ¯ä¸ªæ¨¡å‹çš„æ–‡ä»¶çº§é¢„æµ‹ç»“æœ

    for seed_idx, seed in enumerate(random_seeds):
        print(f"\n--- åŠ è½½ç¬¬ {seed_idx + 1} ä¸ªæ¨¡å‹ (éšæœºç§å­: {seed}) ---")

        try:
            # åŠ è½½å·²è®­ç»ƒçš„æ¨¡å‹
            model_save_path = os.path.join(output_dir, f'dann_model_seed_{seed}.h5')
            print(f"  - æ­£åœ¨åŠ è½½æ¨¡å‹: {model_save_path}")

            # åŠ è½½æ¨¡å‹ï¼ŒåŒ…å«è‡ªå®šä¹‰å±‚
            loaded_model = tf.keras.models.load_model(model_save_path, custom_objects={'GradReverse': GradReverse})
            print("  - æ¨¡å‹åŠ è½½æˆåŠŸã€‚")

            # âœ… å…³é”®ä¿®æ”¹ï¼šä»…ä½¿ç”¨åˆ†ç±»å¤´è¿›è¡Œé¢„æµ‹ï¼Œä¸ä¾èµ– domain adaptation
            print("  - æ„å»ºçº¯åˆ†ç±»å™¨ï¼ˆä»…ä½¿ç”¨ class_output å±‚ï¼‰...")
            classifier = Model(inputs=loaded_model.input, outputs=loaded_model.get_layer('class_output').output)

            # æ‰§è¡Œé¢„æµ‹ï¼ˆä½¿ç”¨ training=False é¿å… dropout å½±å“ï¼‰
            print("  - æ­£åœ¨å¯¹ç›®æ ‡åŸŸæ•°æ®è¿›è¡Œé¢„æµ‹...")
            y_target_pred_proba = classifier.predict(X_target_scaled, batch_size=32)
            y_target_pred_int = np.argmax(y_target_pred_proba, axis=1)
            y_target_pred_labels = le.inverse_transform(y_target_pred_int)

            # æŒ‰æ–‡ä»¶ååˆ†ç»„é¢„æµ‹ç»“æœè¿›è¡ŒæŠ•ç¥¨ - æ¯ä¸ªæ¨¡å‹å•ç‹¬è¿›è¡Œ
            target_results_df = pd.DataFrame({
                'filename': target_filenames,
                'predicted_label': y_target_pred_labels,
                'confidence': np.max(y_target_pred_proba, axis=1)
            })

            # æŒ‰æ–‡ä»¶ååˆ†ç»„å¹¶æŠ•ç¥¨
            file_predictions_for_this_model = {}
            for filename in target_filenames.unique():
                file_data = target_results_df[target_results_df['filename'] == filename]
                # æŠ•ç¥¨å†³å®šæ–‡ä»¶ç±»åˆ«
                votes = file_data['predicted_label'].value_counts()
                predicted_class = votes.index[0]  # å¾—ç¥¨æœ€å¤šçš„ç±»åˆ«
                confidence = file_data['confidence'].mean()  # å¹³å‡ç½®ä¿¡åº¦
                file_predictions_for_this_model[filename] = {'predicted_label': predicted_class,
                                                             'confidence': confidence}

            all_model_file_predictions.append(file_predictions_for_this_model)
            print(f"âœ… ç¬¬ {seed_idx + 1} ä¸ªæ¨¡å‹é¢„æµ‹å®Œæˆã€‚")

        except Exception as e:
            print(f"âŒ åœ¨ç¬¬ {seed_idx + 1} ä¸ªæ¨¡å‹åŠ è½½æˆ–é¢„æµ‹é˜¶æ®µå‘ç”Ÿé”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
            continue

    # --- é˜¶æ®µå››ï¼šé›†æˆé¢„æµ‹ç»“æœ ---
    print("\n--- é˜¶æ®µå››ï¼šé›†æˆé¢„æµ‹ç»“æœ ---")

    if len(all_model_file_predictions) == 0:
        print("âŒ æ‰€æœ‰æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œæ— æ³•è¿›è¡Œé›†æˆé¢„æµ‹ã€‚")
        exit(1)

    # å°†5ä¸ªæ¨¡å‹çš„æ–‡ä»¶çº§é¢„æµ‹ç»“æœè¿›è¡Œæœ€ç»ˆæŠ•ç¥¨
    final_file_predictions = {}
    for filename in target_filenames.unique():
        # æ”¶é›†5ä¸ªæ¨¡å‹å¯¹è¿™ä¸ªæ–‡ä»¶çš„é¢„æµ‹
        model_predictions = []
        confidence_scores = []

        for model_pred_dict in all_model_file_predictions:
            if filename in model_pred_dict:
                model_predictions.append(model_pred_dict[filename]['predicted_label'])
                confidence_scores.append(model_pred_dict[filename]['confidence'])

        # æŠ•ç¥¨å†³å®šæœ€ç»ˆç±»åˆ«
        votes = pd.Series(model_predictions).value_counts()
        final_predicted_class = votes.index[0]  # å¾—ç¥¨æœ€å¤šçš„ç±»åˆ«

        # è®¡ç®—æœ€ç»ˆç½®ä¿¡åº¦
        vote_ratio = votes.iloc[0] / len(model_predictions)
        avg_confidence = np.mean(confidence_scores)
        final_confidence = vote_ratio * avg_confidence

        final_file_predictions[filename] = {
            'predicted_label': final_predicted_class,
            'confidence': final_confidence
        }

    print("âœ… é›†æˆé¢„æµ‹å®Œæˆã€‚")

    # --- é˜¶æ®µäº”ï¼šè¿ç§»ç»“æœå¯è§†åŒ–å±•ç¤ºä¸åˆ†æ ---
    print("\n--- é˜¶æ®µäº”ï¼šè¿ç§»ç»“æœå¯è§†åŒ–å±•ç¤ºä¸åˆ†æ ---")

    # 1. æ–‡ä»¶çº§é¢„æµ‹æ ‡ç­¾åˆ†å¸ƒ
    file_pred_labels = [v['predicted_label'] for v in final_file_predictions.values()]
    unique_labels, counts = np.unique(file_pred_labels, return_counts=True)
    plt.figure(figsize=(8, 6))
    colors = sns.color_palette("husl", len(unique_labels))
    plt.pie(counts, labels=unique_labels, autopct='%1.1f%%', startangle=140, colors=colors)
    plt.title('ç›®æ ‡åŸŸæ–‡ä»¶é¢„æµ‹ç»“æœç±»åˆ«åˆ†å¸ƒ (DANN é›†æˆ)', fontsize=14, weight='bold')
    plt.axis('equal')
    save_path_pie = os.path.join(output_dir, '22_target_prediction_distribution_dann_ensemble.png')
    plt.savefig(save_path_pie, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"âœ… æ–‡ä»¶çº§é¢„æµ‹æ ‡ç­¾åˆ†å¸ƒé¥¼å›¾å·²ä¿å­˜è‡³: {save_path_pie}")

    # 2. æ–‡ä»¶çº§é¢„æµ‹ç½®ä¿¡åº¦åˆ†æ
    file_confidences = [v['confidence'] for v in final_file_predictions.values()]
    plt.figure(figsize=(10, 6))
    plt.hist(file_confidences, bins=30, edgecolor='black', alpha=0.7, color='skyblue')
    plt.xlabel('æ–‡ä»¶é¢„æµ‹ç½®ä¿¡åº¦', fontsize=12)
    plt.ylabel('æ–‡ä»¶æ•°é‡', fontsize=12)
    plt.title('ç›®æ ‡åŸŸæ–‡ä»¶é¢„æµ‹ç½®ä¿¡åº¦åˆ†å¸ƒ (DANN é›†æˆ)', fontsize=14, weight='bold')
    plt.grid(True, alpha=0.3)
    mean_conf = np.mean(file_confidences)
    median_conf = np.median(file_confidences)
    stats_text = f'å‡å€¼: {mean_conf:.3f}\nä¸­ä½æ•°: {median_conf:.3f}'
    plt.text(0.05, 0.95, stats_text, transform=plt.gca().transAxes, fontsize=10,
             verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
    save_path_hist = os.path.join(output_dir, '22_prediction_confidence_histogram_dann_ensemble.png')
    plt.savefig(save_path_hist, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"âœ… æ–‡ä»¶çº§é¢„æµ‹ç½®ä¿¡åº¦ç›´æ–¹å›¾å·²ä¿å­˜è‡³: {save_path_hist}")
    print(f"ğŸ“Š æ–‡ä»¶çº§é¢„æµ‹ç½®ä¿¡åº¦ç»Ÿè®¡ - å‡å€¼: {mean_conf:.4f}, ä¸­ä½æ•°: {median_conf:.4f}")

    # 3. ä¿å­˜é¢„æµ‹ç»“æœ
    file_results_df = pd.DataFrame({
        'filename': list(final_file_predictions.keys()),
        'predicted_label': [v['predicted_label'] for v in final_file_predictions.values()],
        'confidence': [v['confidence'] for v in final_file_predictions.values()]
    })
    file_results_df = file_results_df.sort_values(by='filename').reset_index(drop=True)
    RESULTS_CSV_PATH = os.path.join(output_dir, '22_target_domain_predictions_dann_ensemble.csv')
    file_results_df.to_csv(RESULTS_CSV_PATH, index=False)
    print(f"âœ… ç›®æ ‡åŸŸæ–‡ä»¶é¢„æµ‹ç»“æœå·²ä¿å­˜è‡³: {RESULTS_CSV_PATH}")

    # 4. åŠ è½½æºåŸŸæ•°æ®ç”¨äºt-SNEå¯è§†åŒ–
    print("\n--- é˜¶æ®µå…­ï¼št-SNEå¯è§†åŒ– ---")
    SOURCE_FEATURES_PATH = os.path.join(PROCESSED_DIR, 'source_features_selected.csv')
    try:
        df_source_features = pd.read_csv(SOURCE_FEATURES_PATH)
        selected_feature_names = df_source_features.drop(columns=['label', 'rpm', 'filename']).columns.tolist()
        X_source_raw = df_source_features[selected_feature_names]
        y_source_str = df_source_features['label']
        y_source = le.transform(y_source_str)
        X_source_scaled = scaler.transform(X_source_raw)

        # t-SNE å¯è§†åŒ– (è¿ç§»å‰åå¯¹æ¯”)
        visualize_tsne(X_source_scaled, y_source, X_target_scaled, le,
                       {fname: v for fname, v in final_file_predictions.items()},
                       output_dir, title_suffix="è¿ç§»å_é›†æˆ")
    except Exception as e:
        print(f"âŒ t-SNEå¯è§†åŒ–å‡†å¤‡å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

    print(f"\nğŸ† ä»»åŠ¡ä¸‰é¢„æµ‹ä¸å¯è§†åŒ–å®Œæˆï¼")
    print(f"   - ä½¿ç”¨çš„åŸºç¡€æ¨¡å‹: å·²è®­ç»ƒçš„5ä¸ªDANNæ¨¡å‹ï¼ˆä»…ä½¿ç”¨åˆ†ç±»å¤´ï¼‰")
    print(f"   - é¢„æµ‹æ–¹å¼: é›†æˆæŠ•ç¥¨ (æŒ‰æ–‡ä»¶åˆ†ç»„)")
    print(f"   - é¢„æµ‹ç»“æœå·²ä¿å­˜åœ¨: {RESULTS_CSV_PATH}")
    print(f"   - å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜åœ¨: {os.path.abspath(output_dir)}")