# 21_task3_transfer_learning.py
import os
import numpy as np
import pandas as pd
import joblib
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix
from matplotlib import font_manager
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, Dropout, Lambda  # å¯¼å…¥ MLP-DA éœ€è¦çš„å±‚
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.model_selection import train_test_split


# --- æ–°å¢ï¼šå¯¼å…¥ GradReverse å±‚å®šä¹‰ ---
@tf.custom_gradient
def grad_reverse(x, lambda_val=1.0):
    """æ¢¯åº¦åè½¬å‡½æ•°"""
    y = tf.identity(x)

    def custom_grad(dy):
        return -dy * lambda_val, None

    return y, custom_grad


class GradReverse(tf.keras.layers.Layer):
    """æ¢¯åº¦åè½¬å±‚ Keras å°è£…"""

    def __init__(self, lambda_val=1.0, **kwargs):
        super(GradReverse, self).__init__(**kwargs)
        self.lambda_val = lambda_val

    def call(self, x):
        return grad_reverse(x, self.lambda_val)

    def get_config(self):
        config = super(GradReverse, self).get_config()
        config.update({'lambda_val': self.lambda_val})
        return config


# --- æ–°å¢ç»“æŸ ---


# è®¾ç½®ä¸­æ–‡å­—ä½“
def set_chinese_font():
    """è®¾ç½®ä¸­æ–‡å­—ä½“"""
    plt.rcParams['font.sans-serif'] = ['Microsoft YaHei']
    plt.rcParams['axes.unicode_minus'] = False
    print("âœ… å·²è®¾ç½®ä¸­æ–‡å­—ä½“ã€‚")


# åˆ›å»ºè¾“å‡ºç›®å½•
def create_output_dir():
    """åˆ›å»ºè¾“å‡ºç›®å½•"""
    # --- ä¿®æ”¹ï¼šæ›´æ–°è¾“å‡ºç›®å½•åç§°ä»¥åæ˜ æ¨¡å‹å˜åŒ– ---
    output_dir = os.path.join('..', 'data', 'processed', 'task3_outputs_mlp_da')  # <--- ä¿®æ”¹è¿™é‡Œ
    os.makedirs(output_dir, exist_ok=True)
    return output_dir


# --- ä¿®æ”¹ï¼šå®šä¹‰ä¸ä»»åŠ¡äºŒå®Œå…¨ä¸€è‡´çš„ MLP-DA æ¨¡å‹æ¶æ„ ---
def create_transfer_model(input_dim, num_classes, lambda_grl=1.0):  # <--- ä¿®æ”¹å‡½æ•°ç­¾å
    """
    åˆ›å»ºç”¨äºè¿ç§»å­¦ä¹ çš„ MLP æ¨¡å‹ï¼ŒåŒ…å«é¢†åŸŸè‡ªé€‚åº”ç»„ä»¶ã€‚
    """
    # 1. è¾“å…¥å±‚
    inputs = Input(shape=(input_dim,))  # <--- ä¿®æ”¹è¾“å…¥å½¢çŠ¶

    # 2. ç‰¹å¾æå–å™¨ (MLP)
    shared = Dense(128, activation='relu', name='feature_extractor_1')(inputs)
    shared = Dropout(0.5)(shared)
    shared = Dense(64, activation='relu', name='feature_extractor_2')(shared)
    shared = Dropout(0.5)(shared)
    features_before_grl = Dense(32, activation='relu', name='feature_extractor_3')(shared)  # <--- ä¿®æ”¹ç‰¹å¾å±‚

    # --- é¢†åŸŸè‡ªé€‚åº”åˆ†æ”¯ ---
    grl = GradReverse(lambda_val=lambda_grl)(features_before_grl)  # <--- ä½¿ç”¨ç‰¹å¾å±‚
    d_net = Dense(32, activation='relu')(grl)
    d_net = Dropout(0.5)(d_net)
    domain_output = Dense(1, activation='sigmoid', name='domain_output')(d_net)
    # --- é¢†åŸŸè‡ªé€‚åº”åˆ†æ”¯ç»“æŸ ---

    # 4. ä¸»ä»»åŠ¡åˆ†ç±»å¤´
    c_net = Dense(64, activation='relu')(features_before_grl)  # <--- ä½¿ç”¨ç‰¹å¾å±‚
    c_net = Dropout(0.5)(c_net)
    class_output = Dense(num_classes, activation='softmax', name='class_output')(c_net)

    model = Model(inputs=inputs, outputs=[class_output, domain_output])

    # ç¼–è¯‘æ¨¡å‹
    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),  # <--- å¯èƒ½è°ƒæ•´å­¦ä¹ ç‡
        loss={
            'class_output': 'categorical_crossentropy',
            'domain_output': 'binary_crossentropy'
        },
        loss_weights={
            'class_output': 1.0,
            'domain_output': 1.0  # lambda_domain
        },
        metrics={
            'class_output': 'accuracy',
            'domain_output': 'accuracy'
        }
    )
    return model


# --- ä¿®æ”¹ç»“æŸ ---


## --- æ–°å¢ï¼šè‡ªå®šä¹‰è®­ç»ƒå¾ªç¯ (ä¸º MLP-DA è°ƒæ•´) ---
def train_da_model(model, X_s_train, y_s_train_cat, X_t_train, X_s_val, y_s_val_cat, epochs, batch_size):
    """
    ä½¿ç”¨è‡ªå®šä¹‰è®­ç»ƒå¾ªç¯è®­ç»ƒé¢†åŸŸè‡ªé€‚åº”çš„ MLP æ¨¡å‹ã€‚
    """
    print("  - å¼€å§‹è‡ªå®šä¹‰å¯¹æŠ—è®­ç»ƒ (é¢†åŸŸè‡ªé€‚åº”)...")

    # å‡†å¤‡ TensorFlow æ•°æ®é›†
    ds_source_train = tf.data.Dataset.from_tensor_slices((X_s_train, y_s_train_cat))
    ds_source_train = ds_source_train.batch(batch_size).prefetch(tf.data.AUTOTUNE)

    ds_target_train = tf.data.Dataset.from_tensor_slices((X_t_train, tf.ones((X_t_train.shape[0], 1))))
    ds_target_train = ds_target_train.batch(batch_size).prefetch(tf.data.AUTOTUNE)

    ds_source_val = tf.data.Dataset.from_tensor_slices((X_s_val, y_s_val_cat))
    ds_source_val = ds_source_val.batch(batch_size).prefetch(tf.data.AUTOTUNE)

    # ä¼˜åŒ–å™¨
    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)  # <--- ä¸æ¨¡å‹ç¼–è¯‘ä¿æŒä¸€è‡´

    # æŸå¤±å‡½æ•°
    classification_loss_fn = tf.keras.losses.CategoricalCrossentropy()
    domain_loss_fn = tf.keras.losses.BinaryCrossentropy()

    # Metrics
    train_class_acc = tf.keras.metrics.CategoricalAccuracy(name='train_class_accuracy')
    train_domain_acc = tf.keras.metrics.BinaryAccuracy(name='train_domain_accuracy')
    val_class_acc = tf.keras.metrics.CategoricalAccuracy(name='val_class_accuracy')

    @tf.function
    def train_step(x_s, y_s, x_t):
        domain_labels_s = tf.zeros((tf.shape(x_s)[0], 1))
        domain_labels_t = tf.ones((tf.shape(x_t)[0], 1))

        with tf.GradientTape(persistent=True) as tape:
            y_pred_s, d_pred_s = model(x_s, training=True)
            class_loss_s = classification_loss_fn(y_s, y_pred_s)
            domain_loss_s = domain_loss_fn(domain_labels_s, d_pred_s)

            _, d_pred_t = model(x_t, training=True)
            domain_loss_t = domain_loss_fn(domain_labels_t, d_pred_t)

            total_class_loss = class_loss_s
            total_domain_loss = domain_loss_s + domain_loss_t
            total_loss = total_class_loss + total_domain_loss

        grads = tape.gradient(total_loss, model.trainable_variables)
        optimizer.apply_gradients(zip(grads, model.trainable_variables))

        train_class_acc.update_state(y_s, y_pred_s)
        train_domain_acc.update_state(tf.concat([domain_labels_s, domain_labels_t], axis=0),
                                      tf.concat([d_pred_s, d_pred_t], axis=0))

        return total_class_loss, total_domain_loss

    @tf.function
    def val_step(x, y):
        y_pred, _ = model(x, training=False)
        val_class_acc.update_state(y, y_pred)

    for epoch in range(epochs):
        print(f"Epoch {epoch + 1}/{epochs}")
        # --- å…³é”®ä¿®æ”¹ï¼šä½¿ç”¨ reset_state() ---
        train_class_acc.reset_state()  # <--- ä¿®æ”¹è¿™é‡Œ
        train_domain_acc.reset_state()  # <--- ä¿®æ”¹è¿™é‡Œ
        # --- å…³é”®ä¿®æ”¹ç»“æŸ ---

        # æ³¨æ„ï¼šç›´æ¥ zip ä¸åŒé•¿åº¦çš„ dataset å¯èƒ½å¯¼è‡´è¾ƒçŸ­çš„ dataset ç”¨å®Œååœæ­¢ã€‚
        # ä¸ºç¡®ä¿æºåŸŸæ•°æ®è¢«å……åˆ†åˆ©ç”¨ï¼Œå¯ä»¥ä½¿ç”¨ cycle æˆ–è€…ç¡®ä¿ ds_source_train æ˜¯è¾ƒé•¿çš„é‚£ä¸ªã€‚
        # è¿™é‡Œæˆ‘ä»¬ç®€å•å¤„ç†ï¼Œå‡è®¾æºåŸŸæ•°æ®è¶³å¤Ÿå¤šæˆ–ä¸ç›®æ ‡åŸŸæ‰¹æ¬¡å¯¹é½ã€‚
        # å¦‚æœéœ€è¦æ›´ç²¾ç¡®çš„æ§åˆ¶ï¼Œå¯ä»¥ä½¿ç”¨ tf.data.experimental.sample_from_datasets æˆ–å…¶ä»–ç­–ç•¥ã€‚
        num_batches_source = len(ds_source_train)
        num_batches_target = len(ds_target_train)
        num_batches = max(num_batches_source, num_batches_target)

        # åˆ›å»º zip dataset
        # repeat() ç¡®ä¿è¾ƒçŸ­çš„æ•°æ®é›†åœ¨éœ€è¦æ—¶é‡å¤
        ds_source_train_repeat = ds_source_train.repeat()
        ds_target_train_repeat = ds_target_train.repeat()
        ds_train_combined = tf.data.Dataset.zip((ds_source_train_repeat, ds_target_train_repeat)).take(num_batches)

        total_cls_loss = 0
        total_dom_loss = 0
        num_steps = 0
        for (x_s, y_s), (x_t, _) in ds_train_combined:
            cls_loss, dom_loss = train_step(x_s, y_s, x_t)
            total_cls_loss += cls_loss
            total_dom_loss += dom_loss
            num_steps += 1

        # --- å…³é”®ä¿®æ”¹ï¼šä½¿ç”¨ reset_state() ---
        val_class_acc.reset_state()  # <--- ä¿®æ”¹è¿™é‡Œ
        # --- å…³é”®ä¿®æ”¹ç»“æŸ ---
        for x_val, y_val in ds_source_val:
            val_step(x_val, y_val)

        # è®¡ç®—å¹³å‡æŸå¤±è¿›è¡Œæ‰“å°
        avg_cls_loss = total_cls_loss / num_steps if num_steps > 0 else 0
        avg_dom_loss = total_dom_loss / num_steps if num_steps > 0 else 0

        print(f" - Avg Class Loss: {avg_cls_loss:.4f}, Avg Domain Loss: {avg_dom_loss:.4f}, "
              f"Train Acc: {train_class_acc.result():.4f}, Train Dom Acc: {train_domain_acc.result():.4f}, "
              f"Val Acc: {val_class_acc.result():.4f}")

    print("âœ… è‡ªå®šä¹‰å¯¹æŠ—è®­ç»ƒå®Œæˆã€‚")
# --- æ–°å¢ç»“æŸ ---


# ä¸»ç¨‹åº
if __name__ == "__main__":
    set_chinese_font()
    output_dir = create_output_dir()

    # --- ä¿®æ”¹ï¼šæ›´æ–°æ‰“å°ä¿¡æ¯ ---
    print("ğŸš€ ä»»åŠ¡ä¸‰ï¼šè¿ç§»è¯Šæ–­ (åŸºäº MLP-DA å’Œé¢†åŸŸè‡ªé€‚åº”) å¼€å§‹æ‰§è¡Œ...")  # <--- ä¿®æ”¹è¿™é‡Œ

    # --- é˜¶æ®µä¸€ï¼šåŠ è½½æºåŸŸæ¨¡å‹ä¸é¢„å¤„ç†å™¨ ---
    print("\n--- é˜¶æ®µä¸€ï¼šåŠ è½½æºåŸŸæ¨¡å‹ä¸é¢„å¤„ç†å™¨ ---")
    PROCESSED_DIR = os.path.join('..', 'data', 'processed')
    TASK2_OUTPUTS_DIR = os.path.join(PROCESSED_DIR, 'task2_outputs_final')
    SCALER_PATH = os.path.join(TASK2_OUTPUTS_DIR, 'final_scaler.joblib')
    LE_PATH = os.path.join(TASK2_OUTPUTS_DIR, 'final_label_encoder.joblib')

    try:
        scaler = joblib.load(SCALER_PATH)
        le = joblib.load(LE_PATH)
        print("âœ… æˆåŠŸåŠ è½½ StandardScaler å’Œ LabelEncoderã€‚")
    except FileNotFoundError as e:
        print(f"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ–‡ä»¶ {e.filename}ã€‚è¯·ç¡®ä¿ä»»åŠ¡äºŒå·²æˆåŠŸè¿è¡Œå¹¶ç”Ÿæˆäº†è¾“å‡ºæ–‡ä»¶ã€‚")
        exit(1)
    except Exception as e:
        print(f"âŒ åŠ è½½é¢„å¤„ç†å™¨æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        exit(1)

    # --- é˜¶æ®µäºŒï¼šåŠ è½½å¹¶é¢„å¤„ç†æºåŸŸå’Œç›®æ ‡åŸŸæ•°æ® ---
    print("\n--- é˜¶æ®µäºŒï¼šåŠ è½½å¹¶é¢„å¤„ç†æºåŸŸå’Œç›®æ ‡åŸŸæ•°æ® ---")
    # --- ä¿®æ”¹ï¼šæŒ‡å‘èšåˆç‰¹å¾æ–‡ä»¶ ---
    SOURCE_FEATURES_PATH = os.path.join(PROCESSED_DIR, 'source_features_selected.csv')  # <--- ä¿æŒä¸å˜ï¼Œå› ä¸ºæ˜¯èšåˆç‰¹å¾
    TARGET_FEATURES_PATH = os.path.join(PROCESSED_DIR, 'target_features.csv')  # <--- ä¿æŒä¸å˜ï¼Œå› ä¸ºæ˜¯èšåˆç‰¹å¾

    try:
        # --- ä¿®æ”¹ï¼šåŠ è½½æºåŸŸå’Œç›®æ ‡åŸŸæ•°æ® (èšåˆç‰¹å¾) ---
        print("  - æ­£åœ¨åŠ è½½æºåŸŸèšåˆç‰¹å¾æ•°æ®...")
        df_source_features = pd.read_csv(SOURCE_FEATURES_PATH)
        print("  - æ­£åœ¨åŠ è½½ç›®æ ‡åŸŸèšåˆç‰¹å¾æ•°æ®...")
        df_target_features = pd.read_csv(TARGET_FEATURES_PATH)
        print(f"âœ… æˆåŠŸåŠ è½½æºåŸŸç‰¹å¾æ•°æ® (èšåˆ): {df_source_features.shape}")
        print(f"âœ… æˆåŠŸåŠ è½½ç›®æ ‡åŸŸç‰¹å¾æ•°æ® (èšåˆ): {df_target_features.shape}")

        # --- ä¿®æ”¹ï¼šå¤„ç†æºåŸŸæ•°æ® (èšåˆç‰¹å¾) ---
        X_source_raw = df_source_features.drop(columns=['label', 'rpm', 'filename'])  # <--- è·å–ç‰¹å¾åˆ—
        y_source_str = df_source_features['label']
        y_source = le.transform(y_source_str)
        num_classes = len(le.classes_)
        y_source_cat = to_categorical(y_source, num_classes=num_classes)
        print(f"âœ… æºåŸŸèšåˆç‰¹å¾æ•°æ®å½¢çŠ¶: {X_source_raw.shape}")  # <--- æ›´æ–°æ‰“å°ä¿¡æ¯

        # --- ä¿®æ”¹ï¼šå¤„ç†ç›®æ ‡åŸŸæ•°æ® (èšåˆç‰¹å¾) ---
        target_filenames = df_target_features['source_file']
        X_target_raw = df_target_features.drop(columns=['source_file', 'rpm'])  # <--- è·å–ç‰¹å¾åˆ—
        print(f"âœ… ç›®æ ‡åŸŸèšåˆç‰¹å¾æ•°æ®å½¢çŠ¶: {X_target_raw.shape}")  # <--- æ›´æ–°æ‰“å°ä¿¡æ¯

        # --- ä¿®æ”¹ï¼šæ ‡å‡†åŒ– (ç›´æ¥å¯¹2Dç‰¹å¾è¿›è¡Œ) ---
        X_source_scaled = scaler.transform(X_source_raw)  # Shape: (N_source, F)
        X_target_scaled = scaler.transform(X_target_raw)  # Shape: (N_target, F)

        input_dim = X_source_scaled.shape[1]  # <--- è·å–è¾“å…¥ç»´åº¦ (ç‰¹å¾æ•°)
        print(f"âœ… æ•°æ®æ ‡å‡†åŒ–å®Œæˆã€‚è¾“å…¥ç»´åº¦: {input_dim}")  # <--- æ›´æ–°æ‰“å°ä¿¡æ¯

    except FileNotFoundError as e:
        print(f"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ•°æ®æ–‡ä»¶ {e.filename}ã€‚")
        exit(1)
    except Exception as e:
        print(f"âŒ å¤„ç†æ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        import traceback

        traceback.print_exc()
        exit(1)

    # --- é˜¶æ®µä¸‰ï¼šæ·±å…¥åˆ†ææºåŸŸä¸ç›®æ ‡åŸŸçš„å…±æ€§ä¸å·®å¼‚ (å¯è§†åŒ–) ---
    # (æ­¤éƒ¨åˆ†å¯ä»¥ä¿æŒä¸å˜æˆ–æ ¹æ®æ–°ç‰¹å¾è°ƒæ•´ï¼Œæš‚æ—¶çœç•¥ä»¥èšç„¦æ ¸å¿ƒè®­ç»ƒ)

    # --- é˜¶æ®µå››ï¼šè®¾è®¡å¹¶è®­ç»ƒè¿ç§»æ¨¡å‹ (é¢†åŸŸè‡ªé€‚åº”) ---
    print("\n--- é˜¶æ®µå››ï¼šè®¾è®¡å¹¶è®­ç»ƒè¿ç§»æ¨¡å‹ (é¢†åŸŸè‡ªé€‚åº”) ---")
    try:
        # --- ä¿®æ”¹ï¼šæ„å»º MLP-DA æ¨¡å‹ ---
        print("  - æ„å»ºåŒ…å«é¢†åŸŸè‡ªé€‚åº”çš„è¿ç§»æ¨¡å‹ (MLP-DA)...")
        lambda_grl = 1.0
        # transfer_model = create_transfer_model(input_shape, num_classes, lambda_grl=lambda_grl) # <--- æ—§çš„ CNN-LSTM è°ƒç”¨
        transfer_model = create_transfer_model(input_dim, num_classes, lambda_grl=lambda_grl)  # <--- æ–°çš„ MLP-DA è°ƒç”¨
        transfer_model.summary()
        print("âœ… è¿ç§»æ¨¡å‹æ„å»ºå®Œæˆã€‚")

        # --- å…³é”®ä¿®æ”¹ï¼šåˆ’åˆ†æºåŸŸè®­ç»ƒ/éªŒè¯é›† ---
        print("  - åˆ’åˆ†æºåŸŸè®­ç»ƒé›†å’ŒéªŒè¯é›†...")
        # X_s_train, X_s_val, y_s_train, y_s_val, y_s_train_cat, y_s_val_cat = train_test_split(
        #     X_source_cnn, y_source, y_source_cat, test_size=0.1, random_state=42, stratify=y_source) # <--- æ—§çš„ CNN-LSTM æ•°æ®
        X_s_train, X_s_val, y_s_train, y_s_val, y_s_train_cat, y_s_val_cat = train_test_split(
            X_source_scaled, y_source, y_source_cat, test_size=0.1, random_state=42,
            stratify=y_source)  # <--- æ–°çš„ MLP-DA æ•°æ®
        # X_t_train = X_target_cnn  # ç›®æ ‡åŸŸæ•°æ®å…¨éƒ¨ç”¨äºè®­ç»ƒ # <--- æ—§çš„ CNN-LSTM æ•°æ®
        X_t_train = X_target_scaled  # ç›®æ ‡åŸŸæ•°æ®å…¨éƒ¨ç”¨äºè®­ç»ƒ # <--- æ–°çš„ MLP-DA æ•°æ®
        print(f"    - æºåŸŸè®­ç»ƒé›†: {X_s_train.shape}, éªŒè¯é›†: {X_s_val.shape}")
        print(f"    - ç›®æ ‡åŸŸè®­ç»ƒé›†: {X_t_train.shape}")

        # --- å…³é”®ä¿®æ”¹ï¼šä½¿ç”¨è‡ªå®šä¹‰è®­ç»ƒå¾ªç¯ ---
        epochs = 20
        batch_size = 64
        train_da_model(transfer_model, X_s_train, y_s_train_cat, X_t_train, X_s_val, y_s_val_cat, epochs, batch_size)
        print("âœ… å¯¹æŠ—è®­ç»ƒå®Œæˆã€‚")

        # --- é˜¶æ®µäº”ï¼šç›®æ ‡åŸŸé¢„æµ‹ä¸æ ‡å®š ---
        print("\n--- é˜¶æ®µäº”ï¼šç›®æ ‡åŸŸé¢„æµ‹ä¸æ ‡å®š ---")
        # final_model = Model(inputs=transfer_model.input, outputs=transfer_model.get_layer('class_output').output) # <--- å¯ä»¥ç®€åŒ–
        # --- ä¿®æ”¹ï¼šç›´æ¥ä½¿ç”¨ä¸»è¾“å‡ºè¿›è¡Œé¢„æµ‹ ---
        y_target_pred_proba, _ = transfer_model.predict(X_target_scaled)  # <--- ç›´æ¥è·å–åˆ†ç±»è¾“å‡º
        y_target_pred_int = np.argmax(y_target_pred_proba, axis=1)
        y_target_pred_labels = le.inverse_transform(y_target_pred_int)
        print("âœ… ç›®æ ‡åŸŸæ•°æ®é¢„æµ‹å®Œæˆã€‚")

        # --- é˜¶æ®µå…­ï¼šè¿ç§»ç»“æœå¯è§†åŒ–å±•ç¤ºä¸åˆ†æ ---
        print("\n--- é˜¶æ®µå…­ï¼šè¿ç§»ç»“æœå¯è§†åŒ–å±•ç¤ºä¸åˆ†æ ---")
        unique_labels, counts = np.unique(y_target_pred_labels, return_counts=True)
        plt.figure(figsize=(8, 6))
        colors = sns.color_palette("husl", len(unique_labels))
        plt.pie(counts, labels=unique_labels, autopct='%1.1f%%', startangle=140, colors=colors)
        # --- ä¿®æ”¹ï¼šæ›´æ–°å›¾è¡¨æ ‡é¢˜ ---
        plt.title('ç›®æ ‡åŸŸé¢„æµ‹ç»“æœç±»åˆ«åˆ†å¸ƒ (MLP-DA è¿ç§»)', fontsize=14, weight='bold')  # <--- ä¿®æ”¹è¿™é‡Œ
        plt.axis('equal')
        # --- ä¿®æ”¹ï¼šæ›´æ–°ä¿å­˜è·¯å¾„ ---
        save_path_pie = os.path.join(output_dir, '21_target_prediction_distribution_mlp_da.png')  # <--- ä¿®æ”¹è¿™é‡Œ
        plt.savefig(save_path_pie, dpi=300, bbox_inches='tight')
        plt.close()
        print(f"âœ… é¢„æµ‹æ ‡ç­¾åˆ†å¸ƒé¥¼å›¾å·²ä¿å­˜è‡³: {save_path_pie}")

        max_probs = np.max(y_target_pred_proba, axis=1)
        plt.figure(figsize=(10, 6))
        plt.hist(max_probs, bins=30, edgecolor='black', alpha=0.7, color='skyblue')
        plt.xlabel('é¢„æµ‹ç½®ä¿¡åº¦ (æœ€å¤§ç±»æ¦‚ç‡)', fontsize=12)
        plt.ylabel('æ ·æœ¬æ•°é‡', fontsize=12)
        # --- ä¿®æ”¹ï¼šæ›´æ–°å›¾è¡¨æ ‡é¢˜ ---
        plt.title('ç›®æ ‡åŸŸé¢„æµ‹ç½®ä¿¡åº¦åˆ†å¸ƒ (MLP-DA è¿ç§»)', fontsize=14, weight='bold')  # <--- ä¿®æ”¹è¿™é‡Œ
        plt.grid(True, alpha=0.3)
        mean_conf = np.mean(max_probs)
        median_conf = np.median(max_probs)
        stats_text = f'å‡å€¼: {mean_conf:.3f}\nä¸­ä½æ•°: {median_conf:.3f}'
        plt.text(0.05, 0.95, stats_text, transform=plt.gca().transAxes, fontsize=10,
                 verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
        # --- ä¿®æ”¹ï¼šæ›´æ–°ä¿å­˜è·¯å¾„ ---
        save_path_hist = os.path.join(output_dir, '21_prediction_confidence_histogram_mlp_da.png')  # <--- ä¿®æ”¹è¿™é‡Œ
        plt.savefig(save_path_hist, dpi=300, bbox_inches='tight')
        plt.close()
        print(f"âœ… é¢„æµ‹ç½®ä¿¡åº¦ç›´æ–¹å›¾å·²ä¿å­˜è‡³: {save_path_hist}")
        print(f"ğŸ“Š é¢„æµ‹ç½®ä¿¡åº¦ç»Ÿè®¡ - å‡å€¼: {mean_conf:.4f}, ä¸­ä½æ•°: {median_conf:.4f}")

        results_df = pd.DataFrame({
            'filename': target_filenames,
            'predicted_label': y_target_pred_labels,
            'confidence': max_probs
        })
        results_df = results_df.sort_values(by='filename').reset_index(drop=True)
        # --- ä¿®æ”¹ï¼šæ›´æ–°ä¿å­˜è·¯å¾„ ---
        RESULTS_CSV_PATH = os.path.join(output_dir, '21_target_domain_predictions_mlp_da.csv')  # <--- ä¿®æ”¹è¿™é‡Œ
        results_df.to_csv(RESULTS_CSV_PATH, index=False)
        print(f"âœ… ç›®æ ‡åŸŸé¢„æµ‹ç»“æœå·²ä¿å­˜è‡³: {RESULTS_CSV_PATH}")

        # --- ä¿®æ”¹ï¼šæ›´æ–°æœ€ç»ˆæ‰“å°ä¿¡æ¯ ---
        print(f"\nğŸ† ä»»åŠ¡ä¸‰è¿ç§»è¯Šæ–­å®Œæˆ (MLP-DA + é¢†åŸŸè‡ªé€‚åº”)ï¼")  # <--- ä¿®æ”¹è¿™é‡Œ
        print(f"   - ä½¿ç”¨çš„åŸºç¡€æ¨¡å‹: MLP-DA (é€‰è‡ªä»»åŠ¡äºŒï¼Œè¾“å…¥ä¸ºèšåˆç‰¹å¾)")  # <--- ä¿®æ”¹è¿™é‡Œ
        print(f"   - è¿ç§»ç­–ç•¥: é¢†åŸŸè‡ªé€‚åº” (Domain Adaptation with GRL + è‡ªå®šä¹‰è®­ç»ƒå¾ªç¯)")  # <--- ä¿æŒä¸å˜æˆ–å¾®è°ƒ
        print(f"   - é¢„æµ‹ç»“æœå·²ä¿å­˜åœ¨: {RESULTS_CSV_PATH}")
        print(f"   - å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜åœ¨: {os.path.abspath(output_dir)}")

    except Exception as e:
        print(f"âŒ åœ¨æ¨¡å‹æ„å»ºã€è®­ç»ƒæˆ–é¢„æµ‹é˜¶æ®µå‘ç”Ÿé”™è¯¯: {e}")
        import traceback

        traceback.print_exc()
        exit(1)
