# 21_task3_transfer_learning_dann.py
import os
import numpy as np
import pandas as pd
import joblib
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, Dropout
from tensorflow.keras.utils import to_categorical
import tensorflow as tf
import scipy.io
from scipy.signal import hilbert


# è®¾ç½®ä¸­æ–‡å­—ä½“
def set_chinese_font():
    """è®¾ç½®ä¸­æ–‡å­—ä½“"""
    plt.rcParams['font.sans-serif'] = ['Microsoft YaHei']
    plt.rcParams['axes.unicode_minus'] = False
    print("âœ… å·²è®¾ç½®ä¸­æ–‡å­—ä½“ã€‚")


# åˆ›å»ºè¾“å‡ºç›®å½•
def create_output_dir():
    """åˆ›å»ºè¾“å‡ºç›®å½•"""
    output_dir = os.path.join('..', 'data', 'processed', 'task3_outputs_dann')
    os.makedirs(output_dir, exist_ok=True)
    return output_dir


# --- æ–°å¢ï¼šå®šä¹‰æ¢¯åº¦åè½¬å±‚ (Gradient Reversal Layer) ---
@tf.custom_gradient
def grad_reverse(x, lambda_val=1.0):
    """æ¢¯åº¦åè½¬å‡½æ•°"""
    y = tf.identity(x)

    def custom_grad(dy):
        return -dy * lambda_val, None

    return y, custom_grad


class GradReverse(tf.keras.layers.Layer):
    """æ¢¯åº¦åè½¬å±‚ Keras å°è£…"""

    def __init__(self, lambda_val=1.0, **kwargs):
        super(GradReverse, self).__init__(**kwargs)
        self.lambda_val = tf.Variable(lambda_val, trainable=False, dtype=tf.float32)

    def call(self, x):
        return grad_reverse(x, self.lambda_val)

    def get_config(self):
        config = super(GradReverse, self).get_config()
        config.update({'lambda_val': float(self.lambda_val.numpy())})
        return config


# --- æ–°å¢ç»“æŸ ---


# --- æ–°å¢ï¼šå®šä¹‰åŒ…å«é¢†åŸŸè‡ªé€‚åº”çš„ DANN æ¨¡å‹æ¶æ„ ---
def create_improved_dann_model(input_dim, num_classes, lambda_grl=0.01, dropout_rate=0.3):
    """
    æ”¹è¿›çš„DANNæ¨¡å‹ï¼Œä½¿ç”¨æ›´å°çš„lambdaå€¼å’Œæ›´æ·±çš„ç‰¹å¾æå–å™¨
    """
    inputs = Input(shape=(input_dim,), name='input')

    # æ›´æ·±çš„ç‰¹å¾æå–å™¨
    shared = Dense(256, activation='relu', name='feature_extractor_1')(inputs)
    shared = Dropout(dropout_rate, name='feature_extractor_dropout_1')(shared)
    shared = Dense(128, activation='relu', name='feature_extractor_2')(shared)
    shared = Dropout(dropout_rate, name='feature_extractor_dropout_2')(shared)
    shared = Dense(64, activation='relu', name='feature_extractor_3')(shared)
    shared = Dropout(dropout_rate, name='feature_extractor_dropout_3')(shared)
    features_before_grl = Dense(32, activation='relu', name='feature_extractor_4')(shared)

    # é¢†åŸŸåˆ†æ”¯
    grl = GradReverse(lambda_val=lambda_grl)(features_before_grl)
    d_net = Dense(32, activation='relu')(grl)
    d_net = Dropout(dropout_rate)(d_net)
    domain_output = Dense(1, activation='sigmoid', name='domain_output')(d_net)

    # åˆ†ç±»åˆ†æ”¯
    c_net = Dense(64, activation='relu')(features_before_grl)
    c_net = Dropout(dropout_rate)(c_net)
    class_output = Dense(num_classes, activation='softmax', name='class_output')(c_net)

    model = Model(inputs=inputs, outputs=[class_output, domain_output], name='Improved_DANN_Model')
    return model


# --- æ–°å¢ç»“æŸ ---


# --- æ–°å¢ï¼šè‡ªå®šä¹‰è®­ç»ƒå¾ªç¯ (ä¿®æ­£ç‰ˆ) ---
def train_improved_dann(model, X_s_train, y_s_train_cat, X_t_train, X_s_val, y_s_val_cat,
                        epochs, batch_size, lambda_domain=0.001):
    """
    æ”¹è¿›çš„DANNè®­ç»ƒå‡½æ•°ï¼Œä½¿ç”¨æ›´ç¨³å®šçš„è®­ç»ƒç­–ç•¥
    """
    print("  - å¼€å§‹æ”¹è¿›çš„å¯¹æŠ—è®­ç»ƒ...")

    # å‡†å¤‡æ•°æ®é›†ï¼Œç¡®ä¿é•¿åº¦åŒ¹é…
    ds_source_train = tf.data.Dataset.from_tensor_slices((X_s_train, y_s_train_cat))
    ds_source_train = ds_source_train.batch(batch_size).repeat().shuffle(1000).prefetch(tf.data.AUTOTUNE)

    ds_target_train = tf.data.Dataset.from_tensor_slices(X_t_train)
    ds_target_train = ds_target_train.batch(batch_size).repeat().shuffle(1000).prefetch(tf.data.AUTOTUNE)

    # ä½¿ç”¨zipå¹¶é™åˆ¶æ­¥æ•°
    steps_per_epoch = min(len(X_s_train) // batch_size, len(X_t_train) // batch_size)
    ds_combined = tf.data.Dataset.zip((ds_source_train, ds_target_train))

    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)  # é™ä½å­¦ä¹ ç‡
    classification_loss_fn = tf.keras.losses.CategoricalCrossentropy()
    domain_loss_fn = tf.keras.losses.BinaryCrossentropy()

    # Metrics
    train_class_acc = tf.keras.metrics.CategoricalAccuracy(name='train_class_accuracy')
    train_domain_acc = tf.keras.metrics.BinaryAccuracy(name='train_domain_accuracy')
    val_class_acc = tf.keras.metrics.CategoricalAccuracy(name='val_class_accuracy')

    # æ·»åŠ æ—©åœæœºåˆ¶
    best_val_acc = 0.0
    patience = 5
    patience_counter = 0

    for epoch in range(epochs):
        print(f"Epoch {epoch + 1}/{epochs}")

        # é‡ç½®metrics
        train_class_acc.reset_state()
        train_domain_acc.reset_state()

        total_cls_loss = 0
        total_dom_loss = 0
        num_batches = 0

        for step, ((x_s, y_s), x_t) in enumerate(ds_combined.take(steps_per_epoch)):
            # ä¸ºç›®æ ‡åŸŸåˆ›å»ºæ ‡ç­¾
            domain_labels_s = tf.zeros((tf.shape(x_s)[0], 1))  # æºåŸŸä¸º0
            domain_labels_t = tf.ones((tf.shape(x_t)[0], 1))  # ç›®æ ‡åŸŸä¸º1

            with tf.GradientTape() as tape:
                # æºåŸŸå‰å‘ä¼ æ’­
                y_pred_s, d_pred_s = model(x_s, training=True)
                class_loss_s = classification_loss_fn(y_s, y_pred_s)
                domain_loss_s = domain_loss_fn(domain_labels_s, d_pred_s)

                # ç›®æ ‡åŸŸå‰å‘ä¼ æ’­
                _, d_pred_t = model(x_t, training=True)
                domain_loss_t = domain_loss_fn(domain_labels_t, d_pred_t)

                # æ€»æŸå¤± - å…³é”®ä¿®æ”¹ï¼šå¤§å¹…å¢åŠ åˆ†ç±»æŸå¤±æƒé‡ï¼Œé™ä½åŸŸå¯¹æŠ—æƒé‡
                total_class_loss = class_loss_s
                total_domain_loss = domain_loss_s + domain_loss_t
                # é‡è¦ï¼šç¡®ä¿åˆ†ç±»ä»»åŠ¡ä¸è¢«åŸŸå¯¹æŠ—ä»»åŠ¡å‹åˆ¶
                total_loss = 5.0 * total_class_loss + lambda_domain * total_domain_loss

            # æ›´æ–°å‚æ•°
            grads = tape.gradient(total_loss, model.trainable_variables)
            optimizer.apply_gradients(zip(grads, model.trainable_variables))

            # æ›´æ–°metrics
            train_class_acc.update_state(y_s, y_pred_s)
            combined_d_pred = tf.concat([d_pred_s, d_pred_t], axis=0)
            combined_d_labels = tf.concat([domain_labels_s, domain_labels_t], axis=0)
            train_domain_acc.update_state(combined_d_labels, combined_d_pred)

            total_cls_loss += total_class_loss
            total_dom_loss += total_domain_loss
            num_batches += 1

        # éªŒè¯
        val_class_acc.reset_state()
        val_pred = model(X_s_val, training=False)[0]  # åªå–åˆ†ç±»è¾“å‡º
        val_class_acc.update_state(y_s_val_cat, val_pred)

        avg_cls_loss = total_cls_loss / num_batches if num_batches > 0 else 0
        avg_dom_loss = total_dom_loss / num_batches if num_batches > 0 else 0

        current_val_acc = val_class_acc.result()
        print(f" - Cls Loss: {avg_cls_loss:.4f}, Dom Loss: {avg_dom_loss:.4f}, "
              f"Train Cls Acc: {train_class_acc.result():.4f}, Train Dom Acc: {train_domain_acc.result():.4f}, "
              f"Val Acc: {current_val_acc:.4f}")

        # æ—©åœæœºåˆ¶
        if current_val_acc > best_val_acc:
            best_val_acc = current_val_acc
            patience_counter = 0
        else:
            patience_counter += 1
            if patience_counter >= patience:
                print(f"  - æ—©åœè§¦å‘ï¼šéªŒè¯å‡†ç¡®ç‡è¿ç»­{patience}è½®æœªæå‡ï¼Œåœæ­¢è®­ç»ƒ")
                break

    print("âœ… æ”¹è¿›çš„å¯¹æŠ—è®­ç»ƒå®Œæˆã€‚")


# --- æ–°å¢ç»“æŸ ---


# ä¸»ç¨‹åº
if __name__ == "__main__":
    set_chinese_font()
    output_dir = create_output_dir()

    print("ğŸš€ ä»»åŠ¡ä¸‰ï¼šè¿ç§»è¯Šæ–­ (åŸºäº DANN å’Œé¢†åŸŸè‡ªé€‚åº”) å¼€å§‹æ‰§è¡Œ...")

    # --- é˜¶æ®µä¸€ï¼šåŠ è½½æºåŸŸæ¨¡å‹ä¸é¢„å¤„ç†å™¨ ---
    print("\n--- é˜¶æ®µä¸€ï¼šåŠ è½½æºåŸŸæ¨¡å‹ä¸é¢„å¤„ç†å™¨ ---")

    PROCESSED_DIR = os.path.join('..', 'data', 'processed')
    TASK2_OUTPUTS_DIR = os.path.join(PROCESSED_DIR, 'task2_outputs_final')

    SCALER_PATH = os.path.join(TASK2_OUTPUTS_DIR, 'final_scaler.joblib')
    LE_PATH = os.path.join(TASK2_OUTPUTS_DIR, 'final_label_encoder.joblib')

    try:
        scaler = joblib.load(SCALER_PATH)
        le = joblib.load(LE_PATH)
        print("âœ… æˆåŠŸåŠ è½½ StandardScaler å’Œ LabelEncoderã€‚")
    except FileNotFoundError as e:
        print(f"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ–‡ä»¶ {e.filename}ã€‚è¯·ç¡®ä¿ä»»åŠ¡äºŒå·²æˆåŠŸè¿è¡Œå¹¶ç”Ÿæˆäº†è¾“å‡ºæ–‡ä»¶ã€‚")
        exit(1)
    except Exception as e:
        print(f"âŒ åŠ è½½é¢„å¤„ç†å™¨æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        exit(1)

    # --- é˜¶æ®µäºŒï¼šåŠ è½½å¹¶é¢„å¤„ç†æºåŸŸå’Œç›®æ ‡åŸŸæ•°æ® ---
    print("\n--- é˜¶æ®µäºŒï¼šåŠ è½½å¹¶é¢„å¤„ç†æºåŸŸå’Œç›®æ ‡åŸŸæ•°æ® ---")

    SOURCE_FEATURES_PATH = os.path.join(PROCESSED_DIR, 'source_features_selected.csv')
    TARGET_FEATURES_PATH = os.path.join(PROCESSED_DIR, 'target_features.csv')

    try:
        df_source_features = pd.read_csv(SOURCE_FEATURES_PATH)
        df_target_features = pd.read_csv(TARGET_FEATURES_PATH)
        print(f"âœ… æˆåŠŸåŠ è½½æºåŸŸç‰¹å¾æ•°æ®: {df_source_features.shape}")
        print(f"âœ… æˆåŠŸåŠ è½½ç›®æ ‡åŸŸç‰¹å¾æ•°æ®: {df_target_features.shape}")

        selected_feature_names = df_source_features.drop(columns=['label', 'rpm', 'filename']).columns.tolist()
        X_source_raw = df_source_features[selected_feature_names]
        y_source_str = df_source_features['label']
        y_source = le.transform(y_source_str)
        num_classes = len(le.classes_)
        y_source_cat = to_categorical(y_source, num_classes=num_classes)

        X_target_raw = df_target_features[selected_feature_names]

        X_source_scaled = scaler.transform(X_source_raw)
        X_target_scaled = scaler.transform(X_target_raw)

        input_dim = X_source_scaled.shape[1]
        print(f"âœ… æ•°æ®é¢„å¤„ç†å®Œæˆã€‚è¾“å…¥ç»´åº¦: {input_dim}")

    except FileNotFoundError as e:
        print(f"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ•°æ®æ–‡ä»¶ {e.filename}ã€‚")
        exit(1)
    except Exception as e:
        print(f"âŒ å¤„ç†æ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        exit(1)

    # --- é˜¶æ®µå››ï¼šè®¾è®¡å¹¶è®­ç»ƒè¿ç§»æ¨¡å‹ (é¢†åŸŸè‡ªé€‚åº”) ---
    print("\n--- é˜¶æ®µå››ï¼šè®¾è®¡å¹¶è®­ç»ƒè¿ç§»æ¨¡å‹ (é¢†åŸŸè‡ªé€‚åº”) ---")

    # ä½¿ç”¨5ä¸ªéšæœºç§å­è®­ç»ƒ5ä¸ªæ¨¡å‹
    random_seeds = [42, 123, 456, 789, 999]

    for seed_idx, seed in enumerate(random_seeds):
        print(f"\n--- è®­ç»ƒç¬¬ {seed_idx + 1} ä¸ªæ¨¡å‹ (éšæœºç§å­: {seed}) ---")

        tf.random.set_seed(seed)
        np.random.seed(seed)

        try:
            print("  - æ„å»ºåŒ…å«é¢†åŸŸè‡ªé€‚åº”çš„ DANN æ¨¡å‹...")
            lambda_grl = 0.01  # å…³é”®ï¼šæ›´å°çš„ GRL lambda
            dropout_rate = 0.3
            dann_model = create_improved_dann_model(input_dim, num_classes, lambda_grl=lambda_grl,
                                                    dropout_rate=dropout_rate)
            print("âœ… DANN æ¨¡å‹æ„å»ºå®Œæˆã€‚")

            # åˆ’åˆ†æºåŸŸè®­ç»ƒ/éªŒè¯é›†
            print("  - åˆ’åˆ†æºåŸŸè®­ç»ƒé›†å’ŒéªŒè¯é›†...")
            X_s_train, X_s_val, y_s_train, y_s_val, y_s_train_cat, y_s_val_cat = train_test_split(
                X_source_scaled, y_source, y_source_cat, test_size=0.2, random_state=42, stratify=y_source
            )
            X_t_train = X_target_scaled
            print(f"    - æºåŸŸè®­ç»ƒé›†: {X_s_train.shape}, éªŒè¯é›†: {X_s_val.shape}")
            print(f"    - ç›®æ ‡åŸŸè®­ç»ƒé›†: {X_t_train.shape}")

            # è®­ç»ƒæ¨¡å‹
            print("  - å¼€å§‹å¯¹æŠ—è®­ç»ƒ (é¢†åŸŸè‡ªé€‚åº”)...")
            epochs = 20
            batch_size = 64
            lambda_domain = 0.001  # å…³é”®ï¼šæä½çš„ domain loss æƒé‡
            train_improved_dann(dann_model, X_s_train, y_s_train_cat, X_t_train, X_s_val, y_s_val_cat, epochs,
                                batch_size, lambda_domain)
            print("âœ… å¯¹æŠ—è®­ç»ƒå®Œæˆã€‚")

            # ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹
            model_save_path = os.path.join(output_dir, f'dann_model_seed_{seed}.h5')
            dann_model.save(model_save_path)
            print(f"âœ… æ¨¡å‹å·²ä¿å­˜è‡³: {model_save_path}")

        except Exception as e:
            print(f"âŒ åœ¨ç¬¬ {seed_idx + 1} ä¸ªæ¨¡å‹è®­ç»ƒé˜¶æ®µå‘ç”Ÿé”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
            continue

    print(f"\nğŸ† ä»»åŠ¡ä¸‰è¿ç§»è¯Šæ–­å®Œæˆ (DANN + é¢†åŸŸè‡ªé€‚åº”)ï¼")
    print(f"   - å·²è®­ç»ƒå¹¶ä¿å­˜ 5 ä¸ª DANN æ¨¡å‹ï¼ˆä¸åŒéšæœºç§å­ï¼‰")
    print(f"   - æ¨¡å‹ä¿å­˜è·¯å¾„: {os.path.abspath(output_dir)}")