# 21_task3_transfer_learning_dann.py
import os
import numpy as np
import pandas as pd
import joblib
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.manifold import TSNE

# --- æ–°å¢ï¼šå¯¼å…¥æ¦‚ç‡æ ¡å‡†ç›¸å…³åº“ ---
try:
    from netcal.scaling import TemperatureScaling

    HAS_NETCAL = True
except ImportError:
    HAS_NETCAL = False
    print("âš ï¸  æœªå®‰è£… netcal åº“ã€‚è¯·è¿è¡Œ 'pip install netcal' ä»¥å¯ç”¨ Temperature Scaling æ¦‚ç‡æ ¡å‡†ã€‚")
# --- æ–°å¢ç»“æŸ ---
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, Dropout, Lambda, Layer
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping
import scipy.io
from scipy.signal import hilbert
import argparse
import datetime


# è®¾ç½® GPU å†…å­˜å¢é•¿ (å¦‚æœä½¿ç”¨GPU)
# gpus = tf.config.experimental.list_physical_devices('GPU')
# if gpus:
#     try:
#         for gpu in gpus:
#             tf.config.experimental.set_memory_growth(gpu, True)
#     except RuntimeError as e:
#         print(e)

# è®¾ç½®ä¸­æ–‡å­—ä½“
def set_chinese_font():
    """è®¾ç½®ä¸­æ–‡å­—ä½“"""
    plt.rcParams['font.sans-serif'] = ['Microsoft YaHei']
    plt.rcParams['axes.unicode_minus'] = False
    print("âœ… å·²è®¾ç½®ä¸­æ–‡å­—ä½“ã€‚")


# åˆ›å»ºè¾“å‡ºç›®å½•
def create_output_dir(base_name="task3_outputs_dann"):
    """åˆ›å»ºè¾“å‡ºç›®å½•"""
    output_dir = os.path.join('..', 'data', 'processed')
    os.makedirs(output_dir, exist_ok=True)
    return output_dir


# --- æ–°å¢ï¼šå®šä¹‰æ¢¯åº¦åè½¬å±‚ (Gradient Reversal Layer) ---
class GradientReversalLayer(Layer):
    """
    æ¢¯åº¦åè½¬å±‚ (Gradient Reversal Layer, GRL)
    """

    def __init__(self, lambda_val=1.0, **kwargs):
        super(GradientReversalLayer, self).__init__(**kwargs)
        self.lambda_val = lambda_val

    def call(self, inputs):
        return inputs

    def compute_output_shape(self, input_shape):
        return input_shape

    def get_config(self):
        config = super(GradientReversalLayer, self).get_config()
        config.update({'lambda_val': self.lambda_val})
        return config

    @classmethod
    def from_config(cls, config):
        return cls(**config)


# è‡ªå®šä¹‰æ¢¯åº¦å‡½æ•°
@tf.custom_gradient
def gradient_reversal(x, lambda_val):
    def grad(dy):
        return -lambda_val * dy, None

    return x, grad


class GradientReversalLayerCustom(Layer):
    """
    ä½¿ç”¨è‡ªå®šä¹‰æ¢¯åº¦çš„æ¢¯åº¦åè½¬å±‚
    """

    def __init__(self, lambda_val=1.0, **kwargs):
        super(GradientReversalLayerCustom, self).__init__(**kwargs)
        self.lambda_val = lambda_val

    def call(self, inputs):
        return gradient_reversal(inputs, self.lambda_val)

    def get_config(self):
        config = super(GradientReversalLayerCustom, self).get_config()
        config.update({'lambda_val': self.lambda_val})
        return config

    @classmethod
    def from_config(cls, config):
        return cls(**config)


# --- æ–°å¢ç»“æŸ ---


# --- æ–°å¢ï¼šå®šä¹‰ DANN æ¨¡å‹ ---
def create_dann_model(input_dim, num_classes, lambda_grl=1.0, dropout_rate=0.5):
    """
    åˆ›å»º DANN æ¨¡å‹ã€‚
    """
    # 1. è¾“å…¥å±‚
    inputs = Input(shape=(input_dim,), name='input')

    # 2. ç‰¹å¾æå–å™¨ (å…±äº«ç‰¹å¾)
    shared = Dense(128, activation='relu', name='feature_extractor_1')(inputs)
    shared = Dropout(dropout_rate, name='feature_extractor_dropout_1')(shared)
    shared = Dense(64, activation='relu', name='feature_extractor_2')(shared)
    shared = Dropout(dropout_rate, name='feature_extractor_dropout_2')(shared)
    features_shared = Dense(32, activation='relu', name='feature_extractor_3')(shared)

    # --- é¢†åŸŸè‡ªé€‚åº”åˆ†æ”¯ ---
    # 3a. æ¢¯åº¦åè½¬å±‚ (GRL)
    grl = GradientReversalLayerCustom(lambda_val=lambda_grl, name='grl')(features_shared)

    # 3b. é¢†åŸŸåˆ¤åˆ«å™¨ (Domain Discriminator)
    d_net = Dense(32, activation='relu', name='domain_discriminator_1')(grl)
    d_net = Dropout(dropout_rate, name='domain_discriminator_dropout_1')(d_net)
    domain_output = Dense(1, activation='sigmoid', name='domain_output')(d_net)
    # --- é¢†åŸŸè‡ªé€‚åº”åˆ†æ”¯ç»“æŸ ---

    # 4. ä¸»ä»»åŠ¡åˆ†ç±»å¤´ (Task Classifier)
    c_net = Dense(64, activation='relu', name='classifier_1')(features_shared)
    c_net = Dropout(dropout_rate, name='classifier_dropout_1')(c_net)
    class_output = Dense(num_classes, activation='softmax', name='class_output')(c_net)

    # 5. æ„å»ºæ¨¡å‹
    model = Model(inputs=inputs, outputs=[class_output, domain_output], name='DANN_Model')

    return model


# --- æ–°å¢ç»“æŸ ---


# --- æ–°å¢ï¼šè‡ªå®šä¹‰è®­ç»ƒå¾ªç¯ ---
def train_dann(model, X_s_train, y_s_train_cat, X_t_train, X_s_val, y_s_val_cat, epochs, batch_size, lambda_domain=1.0):
    """
    ä½¿ç”¨è‡ªå®šä¹‰è®­ç»ƒå¾ªç¯è®­ç»ƒ DANN æ¨¡å‹ã€‚
    """
    print("  - å¼€å§‹è‡ªå®šä¹‰å¯¹æŠ—è®­ç»ƒ (é¢†åŸŸè‡ªé€‚åº”)...")

    # å‡†å¤‡ TensorFlow æ•°æ®é›†
    # æºåŸŸè®­ç»ƒé›† (ç”¨äºåˆ†ç±»å’Œé¢†åŸŸæŸå¤±)
    ds_source_train = tf.data.Dataset.from_tensor_slices((X_s_train, y_s_train_cat))
    ds_source_train = ds_source_train.batch(batch_size).prefetch(tf.data.AUTOTUNE)

    # ç›®æ ‡åŸŸè®­ç»ƒé›† (ä»…ç”¨äºé¢†åŸŸæŸå¤±)
    domain_labels_target_train = tf.ones((X_t_train.shape[0], 1))
    ds_target_train = tf.data.Dataset.from_tensor_slices((X_t_train, domain_labels_target_train))
    ds_target_train = ds_target_train.batch(batch_size).prefetch(tf.data.AUTOTUNE)

    # æºåŸŸéªŒè¯é›† (ç”¨äºç›‘æ§åˆ†ç±»æ€§èƒ½)
    ds_source_val = tf.data.Dataset.from_tensor_slices((X_s_val, y_s_val_cat))
    ds_source_val = ds_source_val.batch(batch_size).prefetch(tf.data.AUTOTUNE)

    # ä¼˜åŒ–å™¨
    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)

    # æŸå¤±å‡½æ•°
    classification_loss_fn = tf.keras.losses.CategoricalCrossentropy()
    domain_loss_fn = tf.keras.losses.BinaryCrossentropy()

    # Metrics
    train_class_acc = tf.keras.metrics.CategoricalAccuracy(name='train_class_accuracy')
    train_domain_acc = tf.keras.metrics.BinaryAccuracy(name='train_domain_accuracy')
    val_class_acc = tf.keras.metrics.CategoricalAccuracy(name='val_class_accuracy')

    @tf.function
    def train_step(x_s, y_s, x_t, lambda_d):
        # æºåŸŸæ•°æ®åŸŸæ ‡ç­¾ä¸º 0ï¼Œç›®æ ‡åŸŸæ•°æ®åŸŸæ ‡ç­¾ä¸º 1
        domain_labels_s = tf.zeros((tf.shape(x_s)[0], 1))
        domain_labels_t = tf.ones((tf.shape(x_t)[0], 1))

        with tf.GradientTape() as tape:
            # å‰å‘ä¼ æ’­
            y_pred_s, d_pred_s = model(x_s, training=True)
            _, d_pred_t = model(x_t, training=True)

            # è®¡ç®—æŸå¤±
            class_loss_s = classification_loss_fn(y_s, y_pred_s)
            domain_loss_s = domain_loss_fn(domain_labels_s, d_pred_s)
            domain_loss_t = domain_loss_fn(domain_labels_t, d_pred_t)
            total_domain_loss = domain_loss_s + domain_loss_t
            total_loss = class_loss_s + lambda_d * total_domain_loss

        # è®¡ç®—æ¢¯åº¦å¹¶æ›´æ–°
        grads = tape.gradient(total_loss, model.trainable_variables)
        optimizer.apply_gradients(zip(grads, model.trainable_variables))

        # æ›´æ–°æŒ‡æ ‡
        train_class_acc.update_state(y_s, y_pred_s)
        combined_d_pred = tf.concat([d_pred_s, d_pred_t], axis=0)
        combined_d_labels = tf.concat([domain_labels_s, domain_labels_t], axis=0)
        train_domain_acc.update_state(combined_d_labels, combined_d_pred)

        return class_loss_s, total_domain_loss, total_loss

    @tf.function
    def val_step(x, y):
        y_pred, _ = model(x, training=False)
        val_class_acc.update_state(y, y_pred)

    # è®­ç»ƒå¾ªç¯
    history = {
        'epoch': [],
        'class_loss': [], 'domain_loss': [], 'total_loss': [],
        'train_acc': [], 'train_dom_acc': [], 'val_acc': []
    }

    for epoch in range(epochs):
        print(f"Epoch {epoch + 1}/{epochs}")

        # é‡ç½®æŒ‡æ ‡
        train_class_acc.reset_state()
        train_domain_acc.reset_state()

        # è®­ç»ƒ
        total_cls_loss = 0
        total_dom_loss = 0
        num_batches = 0

        # å°†ç›®æ ‡åŸŸæ•°æ®é‡å¤ä»¥åŒ¹é…æºåŸŸæ•°æ®æ‰¹æ¬¡
        ds_target_train_cycled = ds_target_train.repeat()
        ds_train_combined = tf.data.Dataset.zip((ds_source_train, ds_target_train_cycled)).take(len(ds_source_train))

        for (x_s, y_s), (x_t, _) in ds_train_combined:
            cls_loss, dom_loss, total_loss = train_step(x_s, y_s, x_t, lambda_domain)
            total_cls_loss += cls_loss
            total_dom_loss += dom_loss
            num_batches += 1

        # éªŒè¯
        val_class_acc.reset_state()
        for x_val, y_val in ds_source_val:
            val_step(x_val, y_val)

        avg_cls_loss = total_cls_loss / num_batches
        avg_dom_loss = total_dom_loss / num_batches

        print(f" - Avg Class Loss: {avg_cls_loss:.4f}, Avg Domain Loss: {avg_dom_loss:.4f}, "
              f"Train Acc: {train_class_acc.result():.4f}, Train Dom Acc: {train_domain_acc.result():.4f}, "
              f"Val Acc: {val_class_acc.result():.4f}")

        # è®°å½•å†å²
        history['epoch'].append(epoch + 1)
        history['class_loss'].append(avg_cls_loss)
        history['domain_loss'].append(avg_dom_loss)
        history['total_loss'].append(avg_cls_loss + lambda_domain * avg_dom_loss)
        history['train_acc'].append(train_class_acc.result())
        history['train_dom_acc'].append(train_domain_acc.result())
        history['val_acc'].append(val_class_acc.result())

    print("âœ… è‡ªå®šä¹‰å¯¹æŠ—è®­ç»ƒå®Œæˆã€‚")
    return history


# --- æ–°å¢ç»“æŸ ---


# --- æ–°å¢ï¼šæ¦‚ç‡æ ¡å‡†å‡½æ•° ---
def calibrate_model_with_temperature_scaling(model, X_val, y_val_int):
    """
    ä½¿ç”¨ Temperature Scaling æ ¡å‡†æ¨¡å‹è¾“å‡ºçš„æ¦‚ç‡ã€‚
    """
    print("  - æ­£åœ¨æ ¡å‡†æ¨¡å‹æ¦‚ç‡ (Temperature Scaling)...")
    if not HAS_NETCAL:
        print("  âš ï¸  æœªå®‰è£… netcal åº“ï¼Œè·³è¿‡æ¦‚ç‡æ ¡å‡†ã€‚")
        return None

    try:
        print("  âš ï¸  è­¦å‘Šï¼šå½“å‰æ¨¡å‹ç»“æ„è¾“å‡º Softmax æ¦‚ç‡ï¼Œæ— æ³•ç›´æ¥åº”ç”¨ Temperature Scalingã€‚")
        print("      å»ºè®®ä¿®æ”¹æ¨¡å‹ï¼Œä½¿å…¶è¾“å‡º logitsï¼Œå¹¶é‡æ–°è®­ç»ƒã€‚")
        print("      å½“å‰å°†è·³è¿‡æ¦‚ç‡æ ¡å‡†ã€‚")
        return None

    except Exception as e:
        print(f"  âš ï¸  æ¨¡å‹æ ¡å‡†è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        return None


def apply_calibration_if_available(calibrator, y_pred_proba):
    """
    å¦‚æœæ ¡å‡†å™¨å­˜åœ¨ï¼Œåˆ™åº”ç”¨æ ¡å‡†ã€‚
    """
    if calibrator is not None and HAS_NETCAL:
        print("  - åº”ç”¨å·²è®­ç»ƒçš„æ ¡å‡†å™¨...")
        try:
            y_pred_proba_calibrated = calibrator.transform(y_pred_proba)
            return y_pred_proba_calibrated
        except Exception as e:
            print(f"  âš ï¸  åº”ç”¨æ ¡å‡†å™¨æ—¶å‡ºé”™: {e}ã€‚ä½¿ç”¨åŸå§‹æ¨¡å‹é¢„æµ‹ã€‚")
            return y_pred_proba
    else:
        print("  - æœªæ‰¾åˆ°æ ¡å‡†å™¨æˆ–æœªå®‰è£… netcalï¼Œä½¿ç”¨åŸå§‹æ¨¡å‹é¢„æµ‹...")
        return y_pred_proba


# --- æ–°å¢ç»“æŸ ---


# --- æ–°å¢ï¼šå¤šæ¬¡è®­ç»ƒä¸æŠ•ç¥¨é¢„æµ‹å‡½æ•° ---
def train_and_predict_with_voting(
        input_dim, num_classes, selected_feature_names,
        X_s_train, y_s_train_cat, X_t_train, X_s_val, y_s_val_cat,
        df_target_features, scaler, le,
        seeds=[42, 123, 456, 789, 999],  # æŒ‡å®šéšæœºç§å­
        epochs=20, batch_size=64, lambda_domain=1.0, lambda_grl=1.0, dropout_rate=0.5
):
    """
    æ‰§è¡Œå¤šæ¬¡è®­ç»ƒå’Œé¢„æµ‹ï¼Œå¹¶æ”¶é›†ç»“æœç”¨äºæŠ•ç¥¨ã€‚
    """
    print("  - å¼€å§‹å¤šæ¬¡è®­ç»ƒä¸é¢„æµ‹ä»¥æ”¯æŒæŠ•ç¥¨æœºåˆ¶...")
    all_predictions_per_file_per_run = {}  # {run_id: {filename: [pred_label_int, ...]}}
    all_probabilities_per_file_per_run = {}  # {run_id: {filename: [pred_proba_array, ...]}}

    # 1. å‡†å¤‡ç›®æ ‡åŸŸç‰¹å¾æ•°æ®
    target_filenames = df_target_features['source_file'].values  # å‡è®¾åˆ—åæ˜¯ source_file
    X_target_raw = df_target_features[selected_feature_names]
    X_target_scaled = scaler.transform(X_target_raw)

    for run_id, seed in enumerate(seeds):
        print(f"\n    --- ç¬¬ {run_id + 1}/{len(seeds)} æ¬¡è®­ç»ƒ (ç§å­: {seed}) ---")
        # 1. è®¾ç½®éšæœºç§å­
        tf.random.set_seed(seed)
        np.random.seed(seed)  # å½±å“ numpy ç›¸å…³çš„éšæœºæ“ä½œ

        # 2. é‡æ–°åˆ›å»ºæ¨¡å‹ (ç¡®ä¿æ¯æ¬¡éƒ½æ˜¯å…¨æ–°åˆå§‹åŒ–)
        print(f"      - æ­£åœ¨æ„å»ºç¬¬ {run_id + 1} ä¸ª DANN æ¨¡å‹...")
        model = create_dann_model(input_dim, num_classes, lambda_grl, dropout_rate)

        # 3. ç¼–è¯‘æ¨¡å‹
        model.compile(
            optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),
            loss={
                'class_output': 'categorical_crossentropy',
                'domain_output': 'binary_crossentropy'
            },
            loss_weights={
                'class_output': 1.0,
                'domain_output': lambda_domain
            },
            metrics={
                'class_output': 'accuracy',
                'domain_output': 'accuracy'
            }
        )

        # 4. è®­ç»ƒæ¨¡å‹ (ä½¿ç”¨è‡ªå®šä¹‰å¾ªç¯)
        print(f"      - æ­£åœ¨è®­ç»ƒç¬¬ {run_id + 1} ä¸ª DANN æ¨¡å‹...")
        history = train_dann(model, X_s_train, y_s_train_cat, X_t_train, X_s_val, y_s_val_cat, epochs, batch_size,
                             lambda_domain)
        print(f"      âœ… ç¬¬ {run_id + 1} ä¸ª DANN æ¨¡å‹è®­ç»ƒå®Œæˆã€‚")

        # 5. ä¿å­˜æ¨¡å‹
        model_save_path = os.path.join('..', 'data', 'processed', f'dann_model_run_{run_id + 1}_seed_{seed}.h5')
        model.save(model_save_path)
        print(f"      âœ… ç¬¬ {run_id + 1} ä¸ª DANN æ¨¡å‹å·²ä¿å­˜è‡³: {model_save_path}")

        # 6. æ„å»ºç”¨äºé¢„æµ‹çš„æ¨¡å‹ (ç§»é™¤åŸŸè¾“å‡º)
        final_model = Model(inputs=model.input, outputs=model.get_layer('class_output').output)

        # 7. é¢„æµ‹ç›®æ ‡åŸŸ
        print(f"      - æ­£åœ¨å¯¹ç›®æ ‡åŸŸæ•°æ®è¿›è¡Œç¬¬ {run_id + 1} æ¬¡é¢„æµ‹...")
        y_target_pred_proba = final_model.predict(X_target_scaled)  # å½¢çŠ¶: (N_samples, num_classes)
        y_target_pred_int = np.argmax(y_target_pred_proba, axis=1)  # å½¢çŠ¶: (N_samples,)

        # 8. å­˜å‚¨é¢„æµ‹ç»“æœ (æŒ‰æ–‡ä»¶ååˆ†ç»„)
        print(f"      - æ­£åœ¨å­˜å‚¨ç¬¬ {run_id + 1} æ¬¡é¢„æµ‹ç»“æœ...")
        for i, fname in enumerate(target_filenames):
            if run_id == 0:
                all_predictions_per_file_per_run[fname] = []
                all_probabilities_per_file_per_run[fname] = []
            all_predictions_per_file_per_run[fname].append(y_target_pred_int[i])
            all_probabilities_per_file_per_run[fname].append(y_target_pred_proba[i])  # å­˜å‚¨åŸå§‹æ¦‚ç‡

        print(f"      âœ… ç¬¬ {run_id + 1} æ¬¡é¢„æµ‹ä¸å­˜å‚¨å®Œæˆã€‚")

    # 9. æŠ•ç¥¨å†³ç­–
    print("\n  - æ­£åœ¨æ ¹æ®å¤šæ¬¡é¢„æµ‹ç»“æœè¿›è¡Œæ–‡ä»¶çº§æŠ•ç¥¨...")
    final_predictions = {}
    final_confidences = {}  # å­˜å‚¨æŠ•ç¥¨åçš„ç½®ä¿¡åº¦ (ä¾‹å¦‚ï¼Œè·èƒœç±»åˆ«çš„å¹³å‡æ¦‚ç‡)
    for fname in target_filenames:  # éå†æ‰€æœ‰æ–‡ä»¶å
        predictions_list = all_predictions_per_file_per_run.get(fname, [])
        probabilities_list = all_probabilities_per_file_per_run.get(fname, [])
        if predictions_list and probabilities_list:
            # ç®€å•å¤šæ•°æŠ•ç¥¨
            values, counts = np.unique(predictions_list, return_counts=True)
            final_label_int = values[np.argmax(counts)]
            final_label_str = le.inverse_transform([final_label_int])[0]
            final_predictions[fname] = final_label_str

            # è®¡ç®—ç½®ä¿¡åº¦ï¼šè·èƒœç±»åˆ«çš„å¹³å‡æ¦‚ç‡
            winning_class_probs = [probs[final_label_int] for probs in probabilities_list]
            final_confidences[fname] = np.mean(winning_class_probs)

        else:
            final_predictions[fname] = 'Unknown'
            final_confidences[fname] = 0.0

    print("  âœ… æ–‡ä»¶çº§æŠ•ç¥¨å®Œæˆã€‚")

    return final_predictions, final_confidences


def load_and_predict_with_voting(
        input_dim, num_classes, selected_feature_names,
        df_target_features, scaler, le,
        seeds=[42, 123, 456, 789, 999],  # æŒ‡å®šéšæœºç§å­
        lambda_grl=1.0, dropout_rate=0.5
):
    """
    åŠ è½½å·²è®­ç»ƒçš„æ¨¡å‹å¹¶è¿›è¡Œé¢„æµ‹ï¼Œç„¶åæŠ•ç¥¨ã€‚
    """
    print("  - å¼€å§‹åŠ è½½å·²è®­ç»ƒæ¨¡å‹å¹¶è¿›è¡Œé¢„æµ‹ä»¥æ”¯æŒæŠ•ç¥¨æœºåˆ¶...")
    all_predictions_per_file_per_run = {}  # {run_id: {filename: [pred_label_int, ...]}}
    all_probabilities_per_file_per_run = {}  # {run_id: {filename: [pred_proba_array, ...]}}

    # 1. å‡†å¤‡ç›®æ ‡åŸŸç‰¹å¾æ•°æ®
    target_filenames = df_target_features['source_file'].values  # å‡è®¾åˆ—åæ˜¯ source_file
    X_target_raw = df_target_features[selected_feature_names]
    X_target_scaled = scaler.transform(X_target_raw)

    for run_id, seed in enumerate(seeds):
        print(f"\n    --- åŠ è½½ç¬¬ {run_id + 1}/{len(seeds)} ä¸ªå·²è®­ç»ƒæ¨¡å‹ (ç§å­: {seed}) ---")

        # 1. åŠ è½½æ¨¡å‹
        model_load_path = os.path.join('..', 'data', 'processed', f'dann_model_run_{run_id + 1}_seed_{seed}.h5')
        try:
            model = tf.keras.models.load_model(model_load_path, custom_objects={
                'GradientReversalLayerCustom': GradientReversalLayerCustom})
            print(f"      âœ… æˆåŠŸåŠ è½½ç¬¬ {run_id + 1} ä¸ª DANN æ¨¡å‹: {model_load_path}")
        except Exception as e:
            print(f"      âŒ åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
            continue

        # 2. æ„å»ºç”¨äºé¢„æµ‹çš„æ¨¡å‹ (ç§»é™¤åŸŸè¾“å‡º)
        final_model = Model(inputs=model.input, outputs=model.get_layer('class_output').output)

        # 3. é¢„æµ‹ç›®æ ‡åŸŸ
        print(f"      - æ­£åœ¨å¯¹ç›®æ ‡åŸŸæ•°æ®è¿›è¡Œç¬¬ {run_id + 1} æ¬¡é¢„æµ‹...")
        y_target_pred_proba = final_model.predict(X_target_scaled)  # å½¢çŠ¶: (N_samples, num_classes)
        y_target_pred_int = np.argmax(y_target_pred_proba, axis=1)  # å½¢çŠ¶: (N_samples,)

        # 4. å­˜å‚¨é¢„æµ‹ç»“æœ (æŒ‰æ–‡ä»¶ååˆ†ç»„)
        print(f"      - æ­£åœ¨å­˜å‚¨ç¬¬ {run_id + 1} æ¬¡é¢„æµ‹ç»“æœ...")
        for i, fname in enumerate(target_filenames):
            if run_id == 0:
                all_predictions_per_file_per_run[fname] = []
                all_probabilities_per_file_per_run[fname] = []
            all_predictions_per_file_per_run[fname].append(y_target_pred_int[i])
            all_probabilities_per_file_per_run[fname].append(y_target_pred_proba[i])  # å­˜å‚¨åŸå§‹æ¦‚ç‡

        print(f"      âœ… ç¬¬ {run_id + 1} æ¬¡é¢„æµ‹ä¸å­˜å‚¨å®Œæˆã€‚")

    # 5. æŠ•ç¥¨å†³ç­–
    print("\n  - æ­£åœ¨æ ¹æ®å¤šæ¬¡é¢„æµ‹ç»“æœè¿›è¡Œæ–‡ä»¶çº§æŠ•ç¥¨...")
    final_predictions = {}
    final_confidences = {}  # å­˜å‚¨æŠ•ç¥¨åçš„ç½®ä¿¡åº¦ (ä¾‹å¦‚ï¼Œè·èƒœç±»åˆ«çš„å¹³å‡æ¦‚ç‡)
    for fname in target_filenames:  # éå†æ‰€æœ‰æ–‡ä»¶å
        predictions_list = all_predictions_per_file_per_run.get(fname, [])
        probabilities_list = all_probabilities_per_file_per_run.get(fname, [])
        if predictions_list and probabilities_list:
            # ç®€å•å¤šæ•°æŠ•ç¥¨
            values, counts = np.unique(predictions_list, return_counts=True)
            final_label_int = values[np.argmax(counts)]
            final_label_str = le.inverse_transform([final_label_int])[0]
            final_predictions[fname] = final_label_str

            # è®¡ç®—ç½®ä¿¡åº¦ï¼šè·èƒœç±»åˆ«çš„å¹³å‡æ¦‚ç‡
            winning_class_probs = [probs[final_label_int] for probs in probabilities_list]
            final_confidences[fname] = np.mean(winning_class_probs)

        else:
            final_predictions[fname] = 'Unknown'
            final_confidences[fname] = 0.0

    print("  âœ… æ–‡ä»¶çº§æŠ•ç¥¨å®Œæˆã€‚")

    return final_predictions, final_confidences


# --- æ–°å¢ç»“æŸ ---


# --- æ–°å¢ï¼št-SNE å¯è§†åŒ– (ä¿®æ­£ç‰ˆ) ---
def visualize_tsne(X_source, y_source, X_target, source_le, target_predictions_dict, output_dir, title_suffix=""):
    """
    ä½¿ç”¨ t-SNE å¯è§†åŒ–æºåŸŸå’Œç›®æ ‡åŸŸæ•°æ® (ä¿®æ­£ç‰ˆ)ã€‚
    """
    print(f"  - æ­£åœ¨å¯¹ç‰¹å¾è¿›è¡Œ t-SNE é™ç»´ ({title_suffix})...")
    try:
        # åˆå¹¶ç‰¹å¾
        all_features = np.vstack([X_source, X_target])
        # åˆ›å»ºåŸŸæ ‡ç­¾
        domain_labels = np.array(['Source'] * len(X_source) + ['Target'] * len(X_target))
        total_samples = len(all_features)

        # t-SNE é™ç»´
        tsne = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000)
        tsne_results = tsne.fit_transform(all_features)

        plt.figure(figsize=(12, 10))

        # --- ä¿®æ­£ï¼šç»˜åˆ¶æºåŸŸæ•°æ®ï¼ˆæŒ‰ç±»åˆ«ç€è‰²ï¼‰ ---
        unique_source_classes = np.unique(y_source)
        # ä½¿ç”¨ matplotlib.colormaps (æ¨èï¼Œé¿å…å¼ƒç”¨è­¦å‘Š)
        try:
            colors_classes = plt.colormaps['tab10']
        except AttributeError:
            # å…¼å®¹æ—§ç‰ˆæœ¬ matplotlib
            colors_classes = plt.cm.get_cmap('tab10')

        colors_classes = colors_classes(np.linspace(0, 1, len(unique_source_classes)))

        for i, cls_int in enumerate(unique_source_classes):
            cls_name = source_le.inverse_transform([cls_int])[0]
            # 1. æ‰¾åˆ°æ‰€æœ‰æºåŸŸæ ·æœ¬åœ¨ tsne_results ä¸­çš„ç´¢å¼•
            source_indices = np.where(domain_labels == 'Source')[0]  # å½¢çŠ¶ (len(X_source),)
            # 2. åœ¨æºåŸŸæ ·æœ¬ä¸­ï¼Œæ‰¾åˆ°å±äºå½“å‰ç±»åˆ«çš„æ ·æœ¬ç´¢å¼• (ç›¸å¯¹äº y_source)
            within_source_class_indices_bool = (y_source == cls_int)  # å½¢çŠ¶ (len(X_source),)
            # 3. å°† 2 ä¸­çš„å¸ƒå°”ç´¢å¼•åº”ç”¨åˆ° 1 ä¸­çš„ç´¢å¼•ä¸Šï¼Œå¾—åˆ°æœ€ç»ˆåœ¨ tsne_results ä¸­çš„ç´¢å¼•
            final_indices = source_indices[within_source_class_indices_bool]  # å½¢çŠ¶ (è¯¥ç±»åˆ«çš„æºåŸŸæ ·æœ¬æ•°,)

            if len(final_indices) > 0:
                plt.scatter(tsne_results[final_indices, 0], tsne_results[final_indices, 1],
                            c=[colors_classes[i]], label=f'Source-{cls_name}', alpha=0.6, s=20)
        # --- ä¿®æ­£ç»“æŸ ---

        # --- ä¿®æ­£ï¼šç»˜åˆ¶ç›®æ ‡åŸŸæ•°æ®ï¼ˆæŒ‰é¢„æµ‹ç±»åˆ«ç€è‰²ï¼‰---
        target_filenames_in_order = list(target_predictions_dict.keys())
        y_target_pred_str = [target_predictions_dict[fname] for fname in target_filenames_in_order]

        # å¤„ç†ç›®æ ‡åŸŸé¢„æµ‹æ ‡ç­¾ä¸åœ¨æºåŸŸæ ‡ç­¾ä¸­çš„æƒ…å†µ
        valid_target_mask = np.array([label in source_le.classes_ for label in y_target_pred_str])
        valid_target_filenames = np.array(target_filenames_in_order)[valid_target_mask]
        valid_y_target_pred_str = np.array(y_target_pred_str)[valid_target_mask]

        if len(valid_y_target_pred_str) > 0:
            try:
                y_target_pred_int_valid = source_le.transform(valid_y_target_pred_str)
                target_indices = np.where(domain_labels == 'Target')[0]  # å½¢çŠ¶ (len(X_target),)
                valid_target_indices = target_indices[valid_target_mask]  # å½¢çŠ¶ (len(valid_y_target_pred_str),)

                for i, cls_int in enumerate(unique_source_classes):
                    cls_name = source_le.inverse_transform([cls_int])[0]
                    # æ‰¾åˆ°ç›®æ ‡åŸŸä¸­é¢„æµ‹ä¸ºå½“å‰ç±»åˆ«çš„æ ·æœ¬ç´¢å¼•
                    within_target_class_indices_bool = (
                            y_target_pred_int_valid == cls_int)  # å½¢çŠ¶ (len(valid_y_target_pred_str),)
                    final_target_indices = valid_target_indices[within_target_class_indices_bool]  # å½¢çŠ¶ (è¯¥ç±»åˆ«é¢„æµ‹çš„ç›®æ ‡åŸŸæ ·æœ¬æ•°,)

                    if len(final_target_indices) > 0:
                        plt.scatter(tsne_results[final_target_indices, 0], tsne_results[final_target_indices, 1],
                                    c=[colors_classes[i]], label=f'Target-{cls_name}', alpha=0.5, s=20, marker='x')
            except ValueError as e:
                print(f"  âš ï¸  æ ‡ç­¾è½¬æ¢é”™è¯¯: {e}ã€‚å°†ä½¿ç”¨ç»Ÿä¸€é¢œè‰²ç»˜åˆ¶ç›®æ ‡åŸŸæ•°æ®ã€‚")
                target_indices = np.where(domain_labels == 'Target')[0]
                plt.scatter(tsne_results[target_indices, 0], tsne_results[target_indices, 1],
                            c='red', label='Target (Uncertain Labels)', alpha=0.5, s=20, marker='x')
        else:
            print("  âš ï¸  æ²¡æœ‰æœ‰æ•ˆçš„ç›®æ ‡åŸŸé¢„æµ‹æ ‡ç­¾ç”¨äºå¯è§†åŒ–ã€‚")
            target_indices = np.where(domain_labels == 'Target')[0]
            plt.scatter(tsne_results[target_indices, 0], tsne_results[target_indices, 1],
                        c='red', label='Target (No Valid Labels)', alpha=0.5, s=20, marker='x')
        # --- ä¿®æ­£ç»“æŸ ---

        plt.title(f't-SNE å¯è§†åŒ–: æºåŸŸä¸ç›®æ ‡åŸŸç‰¹å¾åˆ†å¸ƒ {title_suffix}', fontsize=16)
        plt.xlabel('t-SNE Component 1')
        plt.ylabel('t-SNE Component 2')
        # è°ƒæ•´å›¾ä¾‹ä½ç½®ï¼Œé¿å…è¶…å‡ºè¾¹ç•Œ
        plt.legend(loc='upper left', bbox_to_anchor=(1.05, 1), borderaxespad=0.)
        plt.grid(True, alpha=0.3)
        plt.tight_layout()

        save_path = os.path.join(output_dir, f'21_tsne_features_{title_suffix.replace(" ", "_")}.png')
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
        print(f"  - âœ… t-SNE å›¾å·²ä¿å­˜è‡³: {save_path}")
    except Exception as e:
        print(f"  - âš ï¸ t-SNE å¯è§†åŒ–å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


def visualize_tsne_before_after(X_source, y_source, X_target, source_le, target_predictions_dict,
                                trained_dann_model, output_dir):
    """
    å¯è§†åŒ–è¿ç§»å‰åçš„ç‰¹å¾åˆ†å¸ƒã€‚
    """
    print("  - å¼€å§‹è¿ç§»å‰å t-SNE å¯è§†åŒ–...")

    # 1. è¿ç§»å‰ï¼šä½¿ç”¨åŸå§‹ç‰¹å¾
    print("    - ç”Ÿæˆè¿ç§»å‰ t-SNE å¯è§†åŒ–...")
    visualize_tsne(X_source, y_source, X_target, source_le, target_predictions_dict, output_dir, "è¿ç§»å‰")

    # 2. è¿ç§»åï¼šä½¿ç”¨ DANN æ¨¡å‹æå–çš„ç‰¹å¾
    print("    - ç”Ÿæˆè¿ç§»å t-SNE å¯è§†åŒ–...")

    # æå–ç‰¹å¾è¡¨ç¤ºï¼ˆç§»é™¤åˆ†ç±»å¤´å’ŒåŸŸåˆ¤åˆ«å™¨ï¼‰
    feature_extractor = Model(inputs=trained_dann_model.input,
                              outputs=trained_dann_model.get_layer('feature_extractor_3').output)

    X_source_features = feature_extractor.predict(X_source)
    X_target_features = feature_extractor.predict(X_target)

    # é‡æ–°è¿›è¡Œ t-SNE é™ç»´
    all_features_extracted = np.vstack([X_source_features, X_target_features])
    domain_labels = np.array(['Source'] * len(X_source_features) + ['Target'] * len(X_target_features))

    tsne = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000)
    tsne_results = tsne.fit_transform(all_features_extracted)

    plt.figure(figsize=(12, 10))

    # ç»˜åˆ¶æºåŸŸæ•°æ®ï¼ˆæŒ‰ç±»åˆ«ç€è‰²ï¼‰
    unique_source_classes = np.unique(y_source)
    try:
        colors_classes = plt.colormaps['tab10']
    except AttributeError:
        colors_classes = plt.cm.get_cmap('tab10')

    colors_classes = colors_classes(np.linspace(0, 1, len(unique_source_classes)))

    for i, cls_int in enumerate(unique_source_classes):
        cls_name = source_le.inverse_transform([cls_int])[0]
        source_indices = np.where(domain_labels == 'Source')[0]
        within_source_class_indices_bool = (y_source == cls_int)
        final_indices = source_indices[within_source_class_indices_bool]

        if len(final_indices) > 0:
            plt.scatter(tsne_results[final_indices, 0], tsne_results[final_indices, 1],
                        c=[colors_classes[i]], label=f'Source-{cls_name}', alpha=0.6, s=20)

    # ç»˜åˆ¶ç›®æ ‡åŸŸæ•°æ®ï¼ˆæŒ‰é¢„æµ‹ç±»åˆ«ç€è‰²ï¼‰
    target_filenames_in_order = list(target_predictions_dict.keys())
    y_target_pred_str = [target_predictions_dict[fname] for fname in target_filenames_in_order]
    valid_target_mask = np.array([label in source_le.classes_ for label in y_target_pred_str])
    valid_y_target_pred_str = np.array(y_target_pred_str)[valid_target_mask]

    if len(valid_y_target_pred_str) > 0:
        try:
            y_target_pred_int_valid = source_le.transform(valid_y_target_pred_str)
            target_indices = np.where(domain_labels == 'Target')[0]
            valid_target_indices = target_indices[valid_target_mask]

            for i, cls_int in enumerate(unique_source_classes):
                cls_name = source_le.inverse_transform([cls_int])[0]
                within_target_class_indices_bool = (y_target_pred_int_valid == cls_int)
                final_target_indices = valid_target_indices[within_target_class_indices_bool]

                if len(final_target_indices) > 0:
                    plt.scatter(tsne_results[final_target_indices, 0], tsne_results[final_target_indices, 1],
                                c=[colors_classes[i]], label=f'Target-{cls_name}', alpha=0.5, s=20, marker='x')
        except ValueError as e:
            print(f"  âš ï¸  æ ‡ç­¾è½¬æ¢é”™è¯¯: {e}")
            target_indices = np.where(domain_labels == 'Target')[0]
            plt.scatter(tsne_results[target_indices, 0], tsne_results[target_indices, 1],
                        c='red', label='Target (Uncertain Labels)', alpha=0.5, s=20, marker='x')
    else:
        print("  âš ï¸  æ²¡æœ‰æœ‰æ•ˆçš„ç›®æ ‡åŸŸé¢„æµ‹æ ‡ç­¾ç”¨äºå¯è§†åŒ–ã€‚")
        target_indices = np.where(domain_labels == 'Target')[0]
        plt.scatter(tsne_results[target_indices, 0], tsne_results[target_indices, 1],
                    c='red', label='Target (No Valid Labels)', alpha=0.5, s=20, marker='x')

    plt.title('t-SNE å¯è§†åŒ–: æºåŸŸä¸ç›®æ ‡åŸŸç‰¹å¾åˆ†å¸ƒ (è¿ç§»å - DANNæå–ç‰¹å¾)', fontsize=16)
    plt.xlabel('t-SNE Component 1')
    plt.ylabel('t-SNE Component 2')
    plt.legend(loc='upper left', bbox_to_anchor=(1.05, 1), borderaxespad=0.)
    plt.grid(True, alpha=0.3)
    plt.tight_layout()

    save_path = os.path.join(output_dir, '21_tsne_features_è¿ç§»å.png')
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"  - âœ… è¿ç§»å t-SNE å›¾å·²ä¿å­˜è‡³: {save_path}")


# --- æ–°å¢ç»“æŸ ---


# ä¸»ç¨‹åº
if __name__ == "__main__":
    set_chinese_font()
    output_dir = create_output_dir()

    print("ğŸš€ ä»»åŠ¡ä¸‰ï¼šè¿ç§»è¯Šæ–­ (åŸºäº DANN å’Œé¢†åŸŸè‡ªé€‚åº”) å¼€å§‹æ‰§è¡Œ...")

    # --- é˜¶æ®µä¸€ï¼šåŠ è½½æºåŸŸæ¨¡å‹ä¸é¢„å¤„ç†å™¨ ---
    print("\n--- é˜¶æ®µä¸€ï¼šåŠ è½½æºåŸŸæ¨¡å‹ä¸é¢„å¤„ç†å™¨ ---")

    PROCESSED_DIR = os.path.join('..', 'data', 'processed')
    TASK2_OUTPUTS_DIR = os.path.join(PROCESSED_DIR, 'task2_outputs_final')  # å‡è®¾æ¨¡å‹ä¿å­˜åœ¨æ­¤ç›®å½•

    SCALER_PATH = os.path.join(TASK2_OUTPUTS_DIR, 'final_scaler.joblib')
    LE_PATH = os.path.join(TASK2_OUTPUTS_DIR, 'final_label_encoder.joblib')
    # æ³¨æ„ï¼šDANN æ¨¡å‹æƒé‡å°†åœ¨æ­¤è„šæœ¬ä¸­è®­ç»ƒå¹¶ä¿å­˜

    try:
        scaler = joblib.load(SCALER_PATH)
        le = joblib.load(LE_PATH)
        print("âœ… æˆåŠŸåŠ è½½ StandardScaler å’Œ LabelEncoderã€‚")
    except FileNotFoundError as e:
        print(f"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ–‡ä»¶ {e.filename}ã€‚è¯·ç¡®ä¿ä»»åŠ¡äºŒå·²æˆåŠŸè¿è¡Œå¹¶ç”Ÿæˆäº†è¾“å‡ºæ–‡ä»¶ã€‚")
        exit(1)
    except Exception as e:
        print(f"âŒ åŠ è½½é¢„å¤„ç†å™¨æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        import traceback

        traceback.print_exc()  # æ‰“å°å®Œæ•´çš„é”™è¯¯å †æ ˆ
        exit(1)

    # --- é˜¶æ®µäºŒï¼šåŠ è½½å¹¶é¢„å¤„ç†æºåŸŸå’Œç›®æ ‡åŸŸæ•°æ® ---
    print("\n--- é˜¶æ®µäºŒï¼šåŠ è½½å¹¶é¢„å¤„ç†æºåŸŸå’Œç›®æ ‡åŸŸæ•°æ® ---")

    # 1. åŠ è½½æºåŸŸæ•°æ® (ç”¨äºè®­ç»ƒè¿ç§»æ¨¡å‹)
    SOURCE_FEATURES_PATH = os.path.join(PROCESSED_DIR, 'source_features_selected.csv')
    TARGET_FEATURES_PATH = os.path.join(PROCESSED_DIR, 'target_features.csv')  # å‡è®¾è¿™æ˜¯06è„šæœ¬å¤„ç†åçš„ç‰¹å¾

    try:
        df_source_features = pd.read_csv(SOURCE_FEATURES_PATH)
        df_target_features = pd.read_csv(TARGET_FEATURES_PATH)
        print(f"âœ… æˆåŠŸåŠ è½½æºåŸŸç‰¹å¾æ•°æ®: {df_source_features.shape}")
        print(f"âœ… æˆåŠŸåŠ è½½ç›®æ ‡åŸŸç‰¹å¾æ•°æ®: {df_target_features.shape}")

        # 2. åˆ†ç¦»ç‰¹å¾å’Œæ ‡ç­¾ (æºåŸŸ)
        selected_feature_names = df_source_features.drop(columns=['label', 'rpm', 'filename']).columns.tolist()
        X_source_raw = df_source_features[selected_feature_names]
        y_source_str = df_source_features['label']
        y_source = le.transform(y_source_str)  # è½¬æ¢ä¸ºæ•´æ•°æ ‡ç­¾
        num_classes = len(le.classes_)
        y_source_cat = to_categorical(y_source, num_classes=num_classes)  # è½¬æ¢ä¸º one-hot

        # 3. åˆ†ç¦»æ–‡ä»¶åå’Œç‰¹å¾ (ç›®æ ‡åŸŸ)
        # ç¡®ä¿ df_target_features åŒ…å« 'source_file' åˆ—ï¼Œæ ‡è¯†æ¯ä¸ªæ ·æœ¬æ¥è‡ªå“ªä¸ª .mat æ–‡ä»¶
        # è¿™éœ€è¦åœ¨ 06 è„šæœ¬ä¸­å®ç°æ»‘åŠ¨çª—å£å¹¶è®°å½•æ¥æº
        assert 'source_file' in df_target_features.columns, "ç›®æ ‡åŸŸç‰¹å¾æ•°æ®ç¼ºå°‘ 'source_file' åˆ—!"
        target_filenames = df_target_features['source_file']
        X_target_raw = df_target_features[selected_feature_names]  # ç¡®ä¿ä½¿ç”¨ç›¸åŒçš„ç‰¹å¾

        # 4. æ ‡å‡†åŒ–ç‰¹å¾
        X_source_scaled = scaler.transform(X_source_raw)
        X_target_scaled = scaler.transform(X_target_raw)

        input_dim = X_source_scaled.shape[1]
        print(f"âœ… æ•°æ®é¢„å¤„ç†å®Œæˆã€‚è¾“å…¥ç»´åº¦: {input_dim}")

    except FileNotFoundError as e:
        print(f"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ•°æ®æ–‡ä»¶ {e.filename}ã€‚")
        exit(1)
    except Exception as e:
        print(f"âŒ å¤„ç†æ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        import traceback

        traceback.print_exc()
        exit(1)

    # --- é˜¶æ®µä¸‰ï¼šæ·±å…¥åˆ†ææºåŸŸä¸ç›®æ ‡åŸŸçš„å…±æ€§ä¸å·®å¼‚ (å¯è§†åŒ–) ---
    print("\n--- é˜¶æ®µä¸‰ï¼šæ·±å…¥åˆ†ææºåŸŸä¸ç›®æ ‡åŸŸçš„å…±æ€§ä¸å·®å¼‚ ---")
    # (æ­¤éƒ¨åˆ†å¯ä»¥ä¿æŒä¸å˜æˆ–æ ¹æ®æ–°ç‰¹å¾è°ƒæ•´ï¼Œæš‚æ—¶çœç•¥ä»¥èšç„¦æ ¸å¿ƒè®­ç»ƒ)

    # --- é˜¶æ®µå››ï¼šè®¾è®¡å¹¶è®­ç»ƒè¿ç§»æ¨¡å‹ (é¢†åŸŸè‡ªé€‚åº”) ---
    print("\n--- é˜¶æ®µå››ï¼šè®¾è®¡å¹¶è®­ç»ƒè¿ç§»æ¨¡å‹ (é¢†åŸŸè‡ªé€‚åº”) ---")
    try:
        print("  - æ„å»ºåŒ…å«é¢†åŸŸè‡ªé€‚åº”çš„ DANN æ¨¡å‹...")
        lambda_grl = 1.0  # GRL çš„ lambda å‚æ•°
        dropout_rate = 0.5
        # dann_model = create_dann_model(input_dim, num_classes, lambda_grl=lambda_grl, dropout_rate=dropout_rate)
        # dann_model.summary()
        # print("âœ… DANN æ¨¡å‹æ„å»ºå®Œæˆã€‚")

        # --- å…³é”®ä¿®æ”¹ï¼šåˆ’åˆ†æºåŸŸè®­ç»ƒ/éªŒè¯é›† ---
        print("  - åˆ’åˆ†æºåŸŸè®­ç»ƒé›†å’ŒéªŒè¯é›†...")
        from sklearn.model_selection import train_test_split

        X_s_train, X_s_val, y_s_train, y_s_val, y_s_train_cat, y_s_val_cat = train_test_split(
            X_source_scaled, y_source, y_source_cat, test_size=0.2, random_state=42, stratify=y_source
        )
        X_t_train = X_target_scaled  # ç›®æ ‡åŸŸæ•°æ®å…¨éƒ¨ç”¨äºè®­ç»ƒ
        print(f"    - æºåŸŸè®­ç»ƒé›†: {X_s_train.shape}, éªŒè¯é›†: {X_s_val.shape}")
        print(f"    - ç›®æ ‡åŸŸè®­ç»ƒé›†: {X_t_train.shape}")

        # --- å…³é”®ä¿®æ”¹ï¼šä½¿ç”¨å¤šæ¬¡è®­ç»ƒå’ŒæŠ•ç¥¨ ---
        print("\n--- å¯åŠ¨å¤šæ¬¡è®­ç»ƒä¸æŠ•ç¥¨é¢„æµ‹æµç¨‹ ---")
        final_predictions, final_confidences = train_and_predict_with_voting(
            input_dim=input_dim,
            num_classes=num_classes,
            selected_feature_names=selected_feature_names,
            X_s_train=X_s_train, y_s_train_cat=y_s_train_cat,
            X_t_train=X_t_train,
            X_s_val=X_s_val, y_s_val_cat=y_s_val_cat,
            df_target_features=df_target_features,
            scaler=scaler, le=le,
            seeds=[42, 123, 456, 789, 999],  # ä½¿ç”¨æŒ‡å®šç§å­
            epochs=20,  # å¯æ ¹æ®éœ€è¦è°ƒæ•´
            batch_size=64,
            lambda_domain=1.0,
            lambda_grl=lambda_grl,
            dropout_rate=dropout_rate
        )
        print("âœ… å¤šæ¬¡è®­ç»ƒä¸æŠ•ç¥¨é¢„æµ‹æµç¨‹å®Œæˆã€‚")

        # --- é˜¶æ®µäº”ï¼šç›®æ ‡åŸŸé¢„æµ‹ä¸æ ‡å®š ---
        print("\n--- é˜¶æ®µäº”ï¼šç›®æ ‡åŸŸé¢„æµ‹ä¸æ ‡å®š ---")
        # final_model = Model(inputs=dann_model.input, outputs=dann_model.get_layer('class_output').output)
        # y_target_pred_proba = final_model.predict(X_target_scaled)
        # y_target_pred_int = np.argmax(y_target_pred_proba, axis=1)
        # y_target_pred_labels = le.inverse_transform(y_target_pred_int)
        # print("âœ… ç›®æ ‡åŸŸæ•°æ®é¢„æµ‹å®Œæˆã€‚")

        # --- ä½¿ç”¨æŠ•ç¥¨ç»“æœ ---
        # final_predictions å’Œ final_confidences å·²ç»æ˜¯æ–‡ä»¶çº§çš„æœ€ç»ˆé¢„æµ‹ç»“æœå’Œç½®ä¿¡åº¦
        print("âœ… ç›®æ ‡åŸŸæ•°æ®é¢„æµ‹ä¸æ ‡å®šå®Œæˆ (åŸºäºæŠ•ç¥¨)ã€‚")

        # --- é˜¶æ®µå…­ï¼šè¿ç§»ç»“æœå¯è§†åŒ–å±•ç¤ºä¸åˆ†æ ---
        print("\n--- é˜¶æ®µå…­ï¼šè¿ç§»ç»“æœå¯è§†åŒ–å±•ç¤ºä¸åˆ†æ ---")
        # 1. é¢„æµ‹æ ‡ç­¾åˆ†å¸ƒ
        # unique_labels, counts = np.unique(y_target_pred_labels, return_counts=True)
        unique_labels, counts = np.unique(list(final_predictions.values()), return_counts=True)
        plt.figure(figsize=(8, 6))
        colors = sns.color_palette("husl", len(unique_labels))
        plt.pie(counts, labels=unique_labels, autopct='%1.1f%%', startangle=140, colors=colors)
        # plt.title('ç›®æ ‡åŸŸé¢„æµ‹ç»“æœç±»åˆ«åˆ†å¸ƒ (DANN)', fontsize=14, weight='bold')
        plt.title('ç›®æ ‡åŸŸé¢„æµ‹ç»“æœç±»åˆ«åˆ†å¸ƒ (DANN + æŠ•ç¥¨)', fontsize=14, weight='bold')  # æ›´æ–°æ ‡é¢˜
        plt.axis('equal')
        # save_path_pie = os.path.join(output_dir, '21_target_prediction_distribution_dann.png')
        save_path_pie = os.path.join(output_dir, '21_target_prediction_distribution_dann_voted.png')  # æ›´æ–°æ–‡ä»¶å
        plt.savefig(save_path_pie, dpi=300, bbox_inches='tight')
        plt.close()
        print(f"âœ… é¢„æµ‹æ ‡ç­¾åˆ†å¸ƒé¥¼å›¾å·²ä¿å­˜è‡³: {save_path_pie}")

        # 2. é¢„æµ‹ç½®ä¿¡åº¦åˆ†æ
        # max_probs = np.max(y_target_pred_proba, axis=1)
        max_probs = list(final_confidences.values())  # ä½¿ç”¨æŠ•ç¥¨åçš„ç½®ä¿¡åº¦
        plt.figure(figsize=(10, 6))
        plt.hist(max_probs, bins=30, edgecolor='black', alpha=0.7, color='skyblue')
        plt.xlabel('é¢„æµ‹ç½®ä¿¡åº¦ (æŠ•ç¥¨åè·èƒœç±»å¹³å‡æ¦‚ç‡)', fontsize=12)  # æ›´æ–°xlabel
        plt.ylabel('æ–‡ä»¶æ•°é‡', fontsize=12)  # æ›´æ–°ylabel
        # plt.title('ç›®æ ‡åŸŸé¢„æµ‹ç½®ä¿¡åº¦åˆ†å¸ƒ (DANN)', fontsize=14, weight='bold')
        plt.title('ç›®æ ‡åŸŸé¢„æµ‹ç½®ä¿¡åº¦åˆ†å¸ƒ (DANN + æŠ•ç¥¨)', fontsize=14, weight='bold')  # æ›´æ–°æ ‡é¢˜
        plt.grid(True, alpha=0.3)
        mean_conf = np.mean(max_probs)
        median_conf = np.median(max_probs)
        stats_text = f'å‡å€¼: {mean_conf:.3f}\nä¸­ä½æ•°: {median_conf:.3f}'
        plt.text(0.05, 0.95, stats_text, transform=plt.gca().transAxes, fontsize=10,
                 verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
        # save_path_hist = os.path.join(output_dir, '21_prediction_confidence_histogram_dann.png')
        save_path_hist = os.path.join(output_dir, '21_prediction_confidence_histogram_dann_voted.png')  # æ›´æ–°æ–‡ä»¶å
        plt.savefig(save_path_hist, dpi=300, bbox_inches='tight')
        plt.close()
        print(f"âœ… é¢„æµ‹ç½®ä¿¡åº¦ç›´æ–¹å›¾å·²ä¿å­˜è‡³: {save_path_hist}")
        print(f"ğŸ“Š é¢„æµ‹ç½®ä¿¡åº¦ç»Ÿè®¡ - å‡å€¼: {mean_conf:.4f}, ä¸­ä½æ•°: {median_conf:.4f}")

        # 3. ä¿å­˜é¢„æµ‹ç»“æœ
        # results_df = pd.DataFrame({
        #     'filename': target_filenames,
        #     'predicted_label': y_target_pred_labels,
        #     'confidence': max_probs
        # })
        # results_df = results_df.sort_values(by='filename').reset_index(drop=True)
        # RESULTS_CSV_PATH = os.path.join(output_dir, '21_target_domain_predictions_dann.csv')
        results_df = pd.DataFrame({
            'filename': list(final_predictions.keys()),
            'predicted_label': list(final_predictions.values()),
            'confidence': [final_confidences[fname] for fname in final_predictions.keys()]  # æ·»åŠ ç½®ä¿¡åº¦
        })
        results_df = results_df.sort_values(by='filename').reset_index(drop=True)
        RESULTS_CSV_PATH = os.path.join(output_dir, '21_target_domain_predictions_dann_voted.csv')  # æ›´æ–°æ–‡ä»¶å
        results_df.to_csv(RESULTS_CSV_PATH, index=False)
        print(f"âœ… ç›®æ ‡åŸŸé¢„æµ‹ç»“æœå·²ä¿å­˜è‡³: {RESULTS_CSV_PATH}")

        # 4. (å¯é€‰) t-SNE å¯è§†åŒ– (è¿ç§»å‰åå¯¹æ¯”)
        # åŠ è½½æœ€åä¸€ä¸ªè®­ç»ƒçš„æ¨¡å‹ç”¨äºå¯è§†åŒ–
        last_model_path = os.path.join('..', 'data', 'processed', f'dann_model_run_5_seed_999.h5')
        try:
            trained_dann_model = tf.keras.models.load_model(last_model_path, custom_objects={
                'GradientReversalLayerCustom': GradientReversalLayerCustom})
            visualize_tsne_before_after(X_source_scaled, y_source, X_target_scaled, le,
                                        final_predictions, trained_dann_model, output_dir)
        except Exception as e:
            print(f"âš ï¸  åŠ è½½æ¨¡å‹è¿›è¡Œt-SNEå¯è§†åŒ–å¤±è´¥: {e}")
            # é€€å›åˆ°åŸå§‹å¯è§†åŒ–
            visualize_tsne(X_source_scaled, y_source, X_target_scaled, le,
                           final_predictions,  # ä½¿ç”¨æŠ•ç¥¨åçš„é¢„æµ‹ç»“æœ
                           output_dir, title_suffix="è¿ç§»å (æŠ•ç¥¨)")

        print(f"\nğŸ† ä»»åŠ¡ä¸‰è¿ç§»è¯Šæ–­å®Œæˆ (DANN + é¢†åŸŸè‡ªé€‚åº” + æŠ•ç¥¨)!")  # æ›´æ–°æœ€ç»ˆæ‰“å°ä¿¡æ¯
        print(f"   - ä½¿ç”¨çš„åŸºç¡€æ¨¡å‹: DANN (è‡ªå®šä¹‰è®­ç»ƒå¾ªç¯)")
        print(f"   - è¿ç§»ç­–ç•¥: é¢†åŸŸè‡ªé€‚åº” (Domain Adaptation with GRL)")
        print(f"   - é¢„æµ‹ç­–ç•¥: å¤šæ¬¡è®­ç»ƒ (5 seeds) + æŠ•ç¥¨å†³ç­–")  # æ›´æ–°ä¿¡æ¯
        print(f"   - é¢„æµ‹ç»“æœå·²ä¿å­˜åœ¨: {RESULTS_CSV_PATH}")
        print(f"   - å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜åœ¨: {os.path.abspath(output_dir)}")

    except Exception as e:
        print(f"âŒ åœ¨æ¨¡å‹æ„å»ºã€è®­ç»ƒæˆ–é¢„æµ‹é˜¶æ®µå‘ç”Ÿé”™è¯¯: {e}")
        import traceback

        traceback.print_exc()
        exit(1)